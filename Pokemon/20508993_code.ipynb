{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1 part: Adding a Win Rate column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "battles = pd.read_csv('./battles.csv', sep =',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pokemon = pd.read_csv('./pokemon.csv', sep =',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "pokemon['Involved'] = 0\n",
    "pokemon['Wins'] = 0\n",
    "count = 0\n",
    "win = 0\n",
    "for id in pokemon['#']:\n",
    "    for id_battle in battles['First_pokemon']:\n",
    "        if id == id_battle:\n",
    "            count += 1\n",
    "    for id_battle in battles['Second_pokemon']:\n",
    "        if id == id_battle:\n",
    "            count += 1\n",
    "    for id_battle in battles['Winner']:\n",
    "        if id == id_battle:\n",
    "            win += 1\n",
    "    if count > 0:\n",
    "        pokemon.loc[id-1,'Involved'] = count\n",
    "    else:\n",
    "        pokemon.loc[id-1,'Involved'] = np.nan\n",
    "        pokemon.loc[id-1,'Wins'] = np.nan\n",
    "    pokemon.loc[id-1,'Wins'] = win\n",
    "    count = 0\n",
    "    win = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def f(involved, wins):\n",
    "        return wins/involved\n",
    "    \n",
    "pokemon['Win Rate'] = f(pokemon['Involved'], pokemon['Wins'])\n",
    "pokemon['Win Rate'] = pokemon['Win Rate'].replace([np.inf, -np.inf], np.nan)\n",
    "pokemon['Win Rate'] = pokemon['Win Rate'].fillna(0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#</th>\n",
       "      <th>Name</th>\n",
       "      <th>Type 1</th>\n",
       "      <th>Type 2</th>\n",
       "      <th>HP</th>\n",
       "      <th>Attack</th>\n",
       "      <th>Defense</th>\n",
       "      <th>Sp. Atk</th>\n",
       "      <th>Sp. Def</th>\n",
       "      <th>Speed</th>\n",
       "      <th>Generation</th>\n",
       "      <th>Has Gender</th>\n",
       "      <th>Legendary</th>\n",
       "      <th>Win Rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Bulbasaur</td>\n",
       "      <td>Grass</td>\n",
       "      <td>Poison</td>\n",
       "      <td>45</td>\n",
       "      <td>49</td>\n",
       "      <td>49</td>\n",
       "      <td>65</td>\n",
       "      <td>65</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.254902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Ivysaur</td>\n",
       "      <td>Grass</td>\n",
       "      <td>Poison</td>\n",
       "      <td>60</td>\n",
       "      <td>62</td>\n",
       "      <td>63</td>\n",
       "      <td>80</td>\n",
       "      <td>80</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.395833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Venusaur</td>\n",
       "      <td>Grass</td>\n",
       "      <td>Poison</td>\n",
       "      <td>80</td>\n",
       "      <td>82</td>\n",
       "      <td>83</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.631068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Mega Venusaur</td>\n",
       "      <td>Grass</td>\n",
       "      <td>Poison</td>\n",
       "      <td>80</td>\n",
       "      <td>100</td>\n",
       "      <td>123</td>\n",
       "      <td>122</td>\n",
       "      <td>120</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.578947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Charmander</td>\n",
       "      <td>Fire</td>\n",
       "      <td>NaN</td>\n",
       "      <td>39</td>\n",
       "      <td>52</td>\n",
       "      <td>43</td>\n",
       "      <td>60</td>\n",
       "      <td>50</td>\n",
       "      <td>65</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.445652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>795</th>\n",
       "      <td>796</td>\n",
       "      <td>Diancie</td>\n",
       "      <td>Rock</td>\n",
       "      <td>Fairy</td>\n",
       "      <td>50</td>\n",
       "      <td>100</td>\n",
       "      <td>150</td>\n",
       "      <td>100</td>\n",
       "      <td>150</td>\n",
       "      <td>50</td>\n",
       "      <td>6</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.386364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>796</th>\n",
       "      <td>797</td>\n",
       "      <td>Mega Diancie</td>\n",
       "      <td>Rock</td>\n",
       "      <td>Fairy</td>\n",
       "      <td>50</td>\n",
       "      <td>160</td>\n",
       "      <td>110</td>\n",
       "      <td>160</td>\n",
       "      <td>110</td>\n",
       "      <td>110</td>\n",
       "      <td>6</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.911765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>797</th>\n",
       "      <td>798</td>\n",
       "      <td>Hoopa Confined</td>\n",
       "      <td>Psychic</td>\n",
       "      <td>Ghost</td>\n",
       "      <td>80</td>\n",
       "      <td>110</td>\n",
       "      <td>60</td>\n",
       "      <td>150</td>\n",
       "      <td>130</td>\n",
       "      <td>70</td>\n",
       "      <td>6</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.528090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>798</th>\n",
       "      <td>799</td>\n",
       "      <td>Hoopa Unbound</td>\n",
       "      <td>Psychic</td>\n",
       "      <td>Dark</td>\n",
       "      <td>80</td>\n",
       "      <td>160</td>\n",
       "      <td>60</td>\n",
       "      <td>170</td>\n",
       "      <td>130</td>\n",
       "      <td>80</td>\n",
       "      <td>6</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.617886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799</th>\n",
       "      <td>800</td>\n",
       "      <td>Volcanion</td>\n",
       "      <td>Fire</td>\n",
       "      <td>Water</td>\n",
       "      <td>80</td>\n",
       "      <td>110</td>\n",
       "      <td>120</td>\n",
       "      <td>130</td>\n",
       "      <td>90</td>\n",
       "      <td>70</td>\n",
       "      <td>6</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.602151</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>800 rows Ã— 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       #            Name   Type 1  Type 2  HP  Attack  Defense  Sp. Atk  \\\n",
       "0      1       Bulbasaur    Grass  Poison  45      49       49       65   \n",
       "1      2         Ivysaur    Grass  Poison  60      62       63       80   \n",
       "2      3        Venusaur    Grass  Poison  80      82       83      100   \n",
       "3      4   Mega Venusaur    Grass  Poison  80     100      123      122   \n",
       "4      5      Charmander     Fire     NaN  39      52       43       60   \n",
       "..   ...             ...      ...     ...  ..     ...      ...      ...   \n",
       "795  796         Diancie     Rock   Fairy  50     100      150      100   \n",
       "796  797    Mega Diancie     Rock   Fairy  50     160      110      160   \n",
       "797  798  Hoopa Confined  Psychic   Ghost  80     110       60      150   \n",
       "798  799   Hoopa Unbound  Psychic    Dark  80     160       60      170   \n",
       "799  800       Volcanion     Fire   Water  80     110      120      130   \n",
       "\n",
       "     Sp. Def  Speed  Generation  Has Gender  Legendary  Win Rate  \n",
       "0         65     45           1        True      False  0.254902  \n",
       "1         80     60           1        True      False  0.395833  \n",
       "2        100     80           1        True      False  0.631068  \n",
       "3        120     80           1        True      False  0.578947  \n",
       "4         50     65           1        True      False  0.445652  \n",
       "..       ...    ...         ...         ...        ...       ...  \n",
       "795      150     50           6       False       True  0.386364  \n",
       "796      110    110           6       False       True  0.911765  \n",
       "797      130     70           6       False       True  0.528090  \n",
       "798      130     80           6       False       True  0.617886  \n",
       "799       90     70           6       False       True  0.602151  \n",
       "\n",
       "[800 rows x 14 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pokemon.drop(['Involved'], axis=1, inplace=True)\n",
    "pokemon.drop(['Wins'], axis=1, inplace=True)\n",
    "pokemon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2 part: Linear Regression for 6 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "model_HP = LinearRegression()\n",
    "\n",
    "X = pokemon\n",
    "y = pokemon['Win Rate']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=4211)\n",
    "#model for HP\n",
    "X_train_HP = X_train['HP'].values.reshape(-1, 1)\n",
    "X_test_HP = X_test['HP'].values.reshape(-1, 1)\n",
    "model_HP.fit(X_train_HP, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 score: 0.11\n"
     ]
    }
   ],
   "source": [
    "#graph for HP\n",
    "xfit = np.linspace(0, 140, 1000)\n",
    "yfit = model_HP.predict(xfit[:, np.newaxis])\n",
    "y_pred = model_HP.predict(X_test_HP)\n",
    "plt.scatter(X_test_HP, y_test)\n",
    "plt.plot(xfit, yfit, color = 'red')\n",
    "plt.suptitle(\"Win Rate vs HP\")\n",
    "plt.title(\"R^2 score :  %.2f\" % r2_score(y_test, y_pred))\n",
    "plt.show()\n",
    "\n",
    "print('R^2 score: %.2f' % r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_Attack = LinearRegression()\n",
    "#model for attack\n",
    "X_train_Attack = X_train['Attack'].values.reshape(-1, 1)\n",
    "X_test_Attack = X_test['Attack'].values.reshape(-1, 1)\n",
    "model_Attack.fit(X_train_Attack, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 score: 0.22\n"
     ]
    }
   ],
   "source": [
    "#graph for attack\n",
    "xfit = np.linspace(0, 180, 1000)\n",
    "yfit = model_Attack.predict(xfit[:, np.newaxis])\n",
    "y_pred = model_Attack.predict(X_test_Attack)\n",
    "plt.scatter(X_test_Attack, y_test)\n",
    "plt.plot(xfit, yfit, color = 'red')\n",
    "plt.suptitle(\"Win Rate vs Attack\")\n",
    "plt.title(\"R^2 score :  %.2f\" % r2_score(y_test, y_pred))\n",
    "plt.show()\n",
    "\n",
    "print('R^2 score: %.2f' % r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_Defense = LinearRegression()\n",
    "#model for defense\n",
    "X_train_Defense = X_train['Defense'].values.reshape(-1, 1)\n",
    "X_test_Defense = X_test['Defense'].values.reshape(-1, 1)\n",
    "model_Defense.fit(X_train_Defense, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 score: -0.01\n"
     ]
    }
   ],
   "source": [
    "#graph for defense\n",
    "xfit = np.linspace(0, 240, 1000)\n",
    "yfit = model_Defense.predict(xfit[:, np.newaxis])\n",
    "y_pred = model_Defense.predict(X_test_Defense)\n",
    "plt.scatter(X_test_Defense, y_test)\n",
    "plt.plot(xfit, yfit, color = 'red')\n",
    "plt.suptitle(\"Win Rate vs Defense\")\n",
    "plt.title(\"R^2 score :  %.2f\" % r2_score(y_test, y_pred))\n",
    "plt.show()\n",
    "\n",
    "print('R^2 score: %.2f' % r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_SpAtk = LinearRegression()\n",
    "#model for spatk\n",
    "X_train_SpAtk = X_train['Sp. Atk'].values.reshape(-1, 1)\n",
    "X_test_SpAtk = X_test['Sp. Atk'].values.reshape(-1, 1)\n",
    "model_SpAtk.fit(X_train_SpAtk, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 score: 0.24\n"
     ]
    }
   ],
   "source": [
    "#graph for spatk\n",
    "xfit = np.linspace(0, 200, 1000)\n",
    "yfit = model_SpAtk.predict(xfit[:, np.newaxis])\n",
    "y_pred = model_SpAtk.predict(X_test_SpAtk)\n",
    "plt.scatter(X_test_SpAtk, y_test)\n",
    "plt.plot(xfit, yfit, color = 'red')\n",
    "plt.suptitle(\"Win Rate vs Sp. Atk\")\n",
    "plt.title(\"R^2 score :  %.2f\" % r2_score(y_test, y_pred))\n",
    "plt.show()\n",
    "\n",
    "print('R^2 score: %.2f' % r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_SpDef = LinearRegression()\n",
    "#model for spdef\n",
    "X_train_SpDef = X_train['Sp. Def'].values.reshape(-1, 1)\n",
    "X_test_SpDef = X_test['Sp. Def'].values.reshape(-1, 1)\n",
    "model_SpDef.fit(X_train_SpDef, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 score: 0.23\n"
     ]
    }
   ],
   "source": [
    "#graph for spdef\n",
    "xfit = np.linspace(0, 240, 1000)\n",
    "yfit = model_SpDef.predict(xfit[:, np.newaxis])\n",
    "y_pred = model_SpDef.predict(X_test_SpAtk)\n",
    "plt.scatter(X_test_SpDef, y_test)\n",
    "plt.plot(xfit, yfit, color = 'red')\n",
    "plt.suptitle(\"Win Rate vs Sp. Def\")\n",
    "plt.title(\"R^2 score :  %.2f\" % r2_score(y_test, y_pred))\n",
    "plt.show()\n",
    "\n",
    "print('R^2 score: %.2f' % r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_Speed = LinearRegression()\n",
    "#model for speed\n",
    "X_train_Speed = X_train['Speed'].values.reshape(-1, 1)\n",
    "X_test_Speed = X_test['Speed'].values.reshape(-1, 1)\n",
    "model_Speed.fit(X_train_Speed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEVCAYAAADwyx6sAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3deZxN9RvA8c9jDEZiFBUjobRYipqinzZtlhbak1aV+pX2FC2i9Et7KZGkRaUUTcrWQiohJMlWQpghwigZzPL8/jjnjjN37p17hztzl3ner9e83HvO957zvWfGc7/3Od9FVBVjjDHxr1K0K2CMMSYyLKAbY0yCsIBujDEJwgK6McYkCAvoxhiTICygG2NMgrCAbiJORLaJSJNo16OiE5FVInJmtOthyo8FdFMiEekrIhP9tv0WZNvlAKpaQ1VX7MG5GomIuh8I29yA1KcUr79WRL4r7XkjTUS6iMhPIvK3iPwlIl+JSKNo18skvsrRroCJed8AfUQkSVXzReQgIBk41m/bYW7ZSEhV1TwRSQemi8g8Vf0iQscuUyJyGPA2cCEwFagBnA0URLNepmKwFroJZQ5OAG/lPj8FmAYs89v2u6pmAbit7MPcx2+KyBARmSAi/4jIbBE5NJwTq+pcYJHnPIhIHxH53T3WYhG5wN1+FDAMONFt3We726uKyDMislpE/hSRYSKS4n8ut1y2iLTwbKsrIjkicoCI1BGRz9wym0XkWxEJ9P+nFbBSVb9Sxz+qOlZVV7vH7C8iH4nIB+57+FFEjvGcs76IjBWRjSKyUkRu9+yr5Hn/m0RkjIjs59l/lYj84e57MJxrbBKLBXRTIlXdBczGCdq4/34LfOe3raTWeTdgAFAbWA48Hs65RaQt0MJ9jc/vwMlALfeY74hIPVVdAtwMzHRTPqlu+SeBw3EC7WFAGtAvwPvcCYxz6+pzKTBdVTcA9wBrgbrAgcADQKB5M34EjhSR50WkvYjUCFCmC/AhsB/wHpAhIsnuB8SnwAK3nmcAd4pIB/d1twNdgVOB+sAWYIh7rZoBQ4Gr3H37Aw0CnNskMlW1H/sp8QfoD3zsPl4ANAU6+m27xlNegcPcx28CIzz7OgNLg5ynkfvabCDHffwMICXU7Segi/v4WuA7zz4B/gUO9Ww7EacFHehYZwIrPM9nAFe7jx8FPvG9rxDXqy0wBtgI7HCvQQ3PtZzlKVsJWIfzIdUGWO13rL7AG+7jJcAZnn31gFyc1Gk/4H3Pvn2AXcCZ0f77sZ/y+7EWugnHN8BJIlIbqKuqvwHfA/9xt7Wg5Bb6es/j7Th55ZLUccvcC5yGk/IBQESudm84ZrtplRZu+UDqAtWBeZ7yk93tgUwFUkSkjYgcgtOq/9jd9zTON4XPRWRFSTdrVXWWql6qqnVxAvUpgDcFssZTtgCn5V8fOASo76urW98HcL4R4O7/2LNvCZDv7q/vd9x/gU3B6mgSk90UNeGYiZPi6InTakVV/xaRLHdblqqujOQJVTUfeNbNkd8CvOAG2ddwUhEz1bkh+xNOSxyKp0D+wmnpN1fVzDDOWSAiY3DSLn8Cn6nqP+6+f3DSLveISHNgmojMUdWvQhxzjoiMw/ng8TnY98BNszQAsoA8nG8PTYMcbg3QQ1Vn+O8QkXXAUZ7n1XHSLqYCsRa6CUlVc4C5wN04+XOf79xtkerdEsgg4D4RqYaTRlCcVAYich1FA+WfQAMRqeLWuwDnA+B5ETnAfU2aJycdyHvAZUB39zHu684VkcNERIC/cVrG+f4vFpGTRORGz/mOBM4HZnmKHSciF4pIZeBOYKe7/wfgbxG5X0RSRCRJRFqIyPHu64YBj7sfbL6btl3cfR8B57rnr4KTIrL/3xWM/cJNuKYDB+AEcZ9v3W1lGdAn4Nz8u1FVFwPP4nxj+BNoifuNwTUVp1fMehH5y912P06qZJaI/A18CRwR7GSqOhsn714fmOTZ1dR97Tb3/K+o6tcBDpGNE8AXisg2nBTPx8BTnjKf4HxobMG5iXmhqua630rOw+0pg/MNYwTOtyOAF4HxOGmff3A+BNq49V4E3IrzIbTOPfbaYO/TJCZRtQUujCkvItIf58bqldGui0k81kI3xpgEYQHdGGMShKVcTLkTkTOA/wAv+HqRGGP2nrXQTUSIM5FWjjvsfr075L9Yf3MRORlnRGZnnD7VVfz29xaRX9xh8StFpHc5vYWoEGdCsmkisl1ElkoJsyOKyH7ulAF/uT/vikhNd19D2T2pme9HReSe8ns3JtosoJtIOk9Va+D00miNM8qxkIgcjTOC8gqcwTZbgVF+c6IIcDXONAEdgV7izuJY3txuhWVtNDAfp8/4g8BHIhJs4NNAnOvSBDgUZ0BRfwBVXa3OlAc13N9BS5wJwcaWbfVNLLGAbiJOVdcDUyg6qVYjnOBypapOUNVcnK57eTjd8XyvfUpVf1TVPFVdhtPFr12g84hINRF5x52MKltE5ojIge6+/UTkDRHJEpEtIpLhed2NIrJcnEm2xotIfc8+FZFbReQ34Dd325Ei8oVbfpmIXBqJ6yQihwPHAo+oao6qjgUWAhcFeUljIENV/1bVrTjdIZsHKXs18I2qropEXU18sIBuIk5EGgCd8EyqpaqrVLWpd2SlG7S7q+ptQY4jOEPnFwU51TU4fbQPxmnh3owzMhRgFM6w/+Y4feWfd495OvAEzsRb9YA/gPf9jtsVp393MxHZB/gCp3/3ATijSF9xR4sGqnMfEfksSH39NceZO8Z7H2EBwYP0EJzBQ7XFmXLhIor2lfe6GngrzHqYBGFD/00kZYiI4szDMhV4ZC+P1x+n0fFGkP25OIH8MFX9GZgHICL1cD5Q9lfVLW7Z6e6/3YGRqvqjW7YvsEVEGnlas0+o6mZ3/2XAKlX11eFHERkLXEyADxpVHVSK91cDJ+3ktRVnpsVAfgSqsHuOlq+AV/wLufcpDsQZPWoqEGuhm0jqqqr74kyodSTBJ80KSUR64bQyz1FnattARuGkdt53UytPiUgyTot9syeYe9XHaZUDoKrbcAKkN4iu8Tw+BGgjRSfM6g4ctKfvzWMbUNNvW00gWM+fD4FfgX3dcr8D7wQodw0w1n1vpgKxgG4iTlWn40wZ+8yevF5EegB9cKaKDTp83R0uP0BVm+F0gzwX50NgDbCfiKQGeFkWTpD2nWsfnFa+d/Iub1/eNThzoqd6fmqo6n/35L35WQQ0EZF9PduOIXiK6RjgVVX91w3Ww3B6CxUSZ/GOS7B0S4VkAd2UlReAs0SkVciSHiLSHfgfcJaGWJdUnAUkWopIEs6EWblAvqquw8ktv+Lmm5NFxLcYx3vAdSLSSkSquueaXcLNw8+Aw8VZDSjZ/TlenBWS9oqq/oozn/sj7g3eC4CjCd4zZQ5wgztxVwrOTJcL/MpcgDOfzLS9rZ+JPxbQTZlQ1Y04a2s+XMqXDsRpMc/x9KceFqTsQTh54r9x5gafzu4UxFU4AX4psAFnVkPcm7IP4wTNdTjd/4J2i3RvWJ7tlsnCmdv9SaBqoPIi8oCIBLtRGcjlQDrOZFqDgIvda4eIdBcRb2u9B84iIGtxvlE0wVnUw+sa4G21EYMVko0UNcaYBGEtdGOMSRAW0I0xJkFYQDfGmARhAd0YYxJE1EaK1qlTRxs1ahSt0xtjTFyaN2/eX6oacAK3qAX0Ro0aMXfu3Gid3hhj4pKI/BFsn6VcjDEmQVhAN8aYBGEB3RhjEoQFdGOMSRAW0I0xJkGEDOgiMlJENojILyHKHS8i+SJyceSqZ4wxJlzhtNDfxFmsNyh3+tIncRYbMMYYEwUhA7qqfgNsDlHsNpzpSDdEolLGGJOQcnNh0CCYM6dMDr/XOXQRScOZVD/YnNXGGGPmz4c2baBvXxgbbA2TvROJm6IvAPeran6ogiLSU0TmisjcjRs3RuDUxhgT43bsgAcfhOOPh6ws+Ogjp5VeBiIx9D8dZ5FecBYF7iwieaqa4V9QVYcDwwHS09NtZQ1jTGKbMQOuvx6WLYPrroNnn4XatcvsdHsd0FW1se+xiLwJfBYomBtjTIXxzz/wwAMwZAg0bAhTpsDZZ5f5aUMGdBEZDZwG1BGRtcAjQDKAqlre3BhjvKZMgZ49Yc0auO02ePxxqFGjXE4dMqCrardwD6aq1+5VbYwxJl5t3gx33w1vvQVHHgnffgvt2pVrFWykqDHG7K2xY6FZM3jnHecG6Pz55R7MIYrzoRtjTNxbtw569YJx4+DYY2HyZGjVKmrVsRa6McaUliq88YbTKp8wwemGOHt2VIM5WAvdGGNKZ9Uq56bnF1/AySfDiBFw+OHRrhVgLXRjjAlPfj4MHgwtWsDMmU6XxK+/jplgDtZCN8aY0JYsgRtugO+/h44d4dVXnf7lMcZa6MYYE0xurtOPvFUrWLoURo2CiRNjMpiDtdCNMSawefOcYfsLFsCll8JLL8EBB0S7ViWyFroxxnjl5ECfPs7MiBs2wMcfwwcfxHwwB2uhG2PMbt984+TKf/vNaZ0/8wykpka7VmGzFroxxvz9N9x6K5x6KuTlwZdfOt0R4yiYgwV0Y0xFN2mS0xVx6FC4805YuBDOOCPatdojlnIxxlRMmzbBXXc5PVeaNXO6JLZtG+1a7RVroRtjKhZVGDMGjjoKRo+Gfv3gxx/jPpiDtdCNMRVJVhbccgt88gmkpzu58qOPjnatIsZa6MaYxKcKr7/upFamTIGnn3aG7ydQMAdroRtjEt2KFXDjjTB1qtOLZcQIOOywaNeqTFgL3RiTmPLz4YUXoGVLmDMHhg1zgnqCBnMII6CLyEgR2SAivwTZ311EfnZ/vheRYyJfTWOMKYVFi5wVg+66C9q3h8WL4aaboFJit2HDeXdvAh1L2L8SOFVVjwYeA4ZHoF7GGFN6u3bBo49C69bw++/w3nvw6afQoEG0a1Yuwlkk+hsRaVTC/u89T2cBFePKGWNiy5w5znD9hQuhWzd48UWoWzfatSpXkf7+cT0wKcLHNMaY4LZvh969nX7kmzfD+PFOy7yCBXOIYC8XEWmPE9BPKqFMT6AnQMMYnU/YGBNHvv7a6cGyfLmzLNxTT0GtWtGuVdREpIUuIkcDI4AuqropWDlVHa6q6aqaXrcCfnoaYyJk61a4+Wbnhqeq03vl1VcrdDCHCAR0EWkIjAOuUtVf975KxhhTgs8+g+bN4bXX4N574eefncBuQqdcRGQ0cBpQR0TWAo8AyQCqOgzoB+wPvCIiAHmqml5WFTbGVFAbN8Iddzjzr7RoAePGwQknRLtWMSWcXi7dQuy/AbghYjUyxhgvVXj/fbj9difVMmCAs6JQlSrRrlnMsaH/xpjYtXYt/Pe/TprlhBOc+VhatIh2rWJWYg+bMsbEp4ICGD7cyZV/9RU895wzX7kF8xJZC90YE1uWL3e6In79NZx+uhPYDz002rWKC9ZCN8bEhrw8Z1Hmli2dBSdee82Zr9yCedishW6Mib6FC51h+3PmwPnnwyuvQFpatGsVd6yFboyJnp074ZFH4NhjYdUqpzdLRoYF8z1kLXRjTHTMnu20yhctgiuvhOefhzp1ol2ruGYtdGNM+fr3X7j7bjjxRKdf+WefwahRFswjwFroxpjyM3Wq04NlxQqnf/mgQVCzZrRrlTCshW6MKXvZ2U4gP+MMSEqC6dOdG58WzCPKAroxpmx98gk0awYjR8J998GCBXDKKdGuVUKygG6MKRsbNsDll0PXrs5iE7Nnw5NPQkpKtGuWsCygG2MiSxXeeQeOOgo+/hgeewzmzoV0m4S1rNlNUWNM5KxZ4yw8MXGisyTc66876RZTLqyFbozZewUFMHSoE7y//tpZoPm77yyYlzNroRtj9s6vv8INN8C338KZZzqTaTVuHO1aVUjWQjfG7Jm8PGdR5mOOceZiGTkSPv/cgnkUWQvdGFN6CxZAjx7OrIgXXABDhkC9etGuVYVnLXRjTPh27oSHH3Z6rKxdCx9+CGPHWjCPESEDuoiMFJENIvJLkP0iIoNFZLmI/Cwix0a+msaYqPv+e2jdGgYOhO7dYfFiuPhicBaHNzEgnBb6m0DHEvZ3Apq6Pz2BoXtfLWNMzNi2De64A046yZlYa/JkePNN2H//aNfM+AkZ0FX1G2BzCUW6AG+rYxaQKiL2/cuYRPDFF84KQoMHw623wi+/QIcO0a6VCSISOfQ0YI3n+Vp3WzEi0lNE5orI3I0bN0bg1MaYMrFli3PT8+yzoWpVp0viSy/BvvtGu2amBJHo5RIogaaBCqrqcGA4QHp6esAyxpjIyZifydNTlpGVnUP91BR6dziCrq3TSt738cdwyy2wcSP07Qv9+kG1alF+JyYckQjoa4GDPc8bAFkROK4xZi9kzM+k77iF5OTmA5CZncOdH/xE//GLOPeYeoydl1lk37NvT+f4+94j7csJ0KoVTJjgLA1n4kYkAvp4oJeIvA+0Abaq6roIHNcYsxeenrKsMGB7Zefk8u6s1bu/Rqty0S9TeXjqa6Tk7YT//Q/uvReSk8u1vmbvhQzoIjIaOA2oIyJrgUeAZABVHQZMBDoDy4HtwHVlVVljTPiysnOC7vMF87StG/jflJc5deWPzElrRp9Ot/FV35vLp4Im4kIGdFXtFmK/ArdGrEbGmIion5pCZpCgLlrAVT9O4P7pbwHQ78ybGHXsOdSvvU95VtFEmA39NyZB9e5wRJEcuk+TTWt5ctJgjs9czPTGx/JAh15k1jqAlOQkenc4Ikq1NZFgAd2YclZSz5NI8h1zwKeL2LI9l8r5efT8YRx3zBhNTnJVxt72GM/VO5GsrTtIK8N67K3yul6JQJyMSflLT0/XuXPnRuXcxkSLf88TgJTkJJ64sGWZBqlpo6eQdk8vDl+3nKktTmHncy/Q6azWZXa+SInW9YplIjJPVQMu/2STcxlTBjLmZ9Ju0FQa95lAu0FTyZifCQTueZKTm8/TU5aVTUV27IC+fWl/1Tkcrttg7FhOXzg9LoI5ROF6xTlLuRgTYYH6f/cdt7DwcSCZ2TlkzM+MbKvzu+/g+uudBSiuuw6efRZq147c8ctBsJ46JfXgqcgsoBsTYSW1KpNEyA+S5vQF/ZKCelj55H/+cUZ4DhkCjRo5i06cddZevadoCdZTp35qShRqE/ss5WJMhJXUqgwWzCF0KsHX8s/MzkHZ3fL3pXMy5mdyV48nyWxwKAWvvMLv3Xo4KwnFaTAHp6dOSnJSkW3WGyc4C+jGRFiw1mP91BTSQrQsM7NziuTcvUpq+U/4+he45hqef6MPOZWrcvEVT3HuoZeQ8dvWPX8jMaBr6zSeuLAlaakpCJCWmlKhb4iGYr1cjImwknpmAAH7hgfjS9GkBRskpEqnZTMY+OUwaub8w7A2F/Pyfy5jZ+UqgBMAZ/Q5fe/flIkZJfVysRy6MWWgWnKlwqCdmpJM//ObF2lV+vqGh+JL0WRm5yAUnca07rbNPPbFUDr+OpOFBx7KVZc8yuIDmxR5vd08rFgsoBsTQYFa5zvzCoqU6do6ja6t0wpvcAbr+eJPceaqVlUuWfglD00dQdX8XJ447VpGHH8B+ZWSir3Gbh5WLBbQjYmgkvLcweYhL4207PU8MfllTv7jJ2Y3aE6fTrezcr/A+WS7eVjxWEA3JoJC9ZsO1Ec9HJUK8rn6xwnc/83bqAgPnX0L77bqiMrufg21qydTvUplGyJfgVlANyaCaqUkk51TPDdeK8WZWzzYHOUlOeyv1Tw5aTDHZS1lfbv2LHhgEGNnbkb9bro+cl5zC+AVnAV0Y0oh1MAeCbQgo2d7STcp01JTyMrOIbV6MqqwbVsOt8wey63fjyananXmPvYi6Q/exkEiPFHPJqwyxVlANyZMJQ3p9wXTYD1XfNuDjXz0dS/0fWDst3Qhz00ZTNP1K+Cyy6g6eDDpBxxQWN53Y9UYLxtYZEyYQk0UFWgwkI+vhV7SyMeM+Zn0/2AuV2W8Qsbbd1NzWza3XtKPjPufBU8wNyYYa6EbE6ZQE2v5WuuB+Mbv+VrVgdIlt9z0AuM+fJomW7IYffTZPNG+B39Xq8GM8YusNW7CElZAF5GOwItAEjBCVQf57W8IvAWkumX6qOrECNfVmKgKNrFWkkipbnYWS5f8/TfccguvDB/K6loHcsVlA/m+UavC3dk5uZGfidEkpHAWiU4ChgBnAWuBOSIyXlUXe4o9BIxR1aEi0gxn4ehGZVBfY6Im2MRa+aohux/Wrp5c+Nh7Y/Wi9T/z6OSXqL5hPe+3u4gBJ3Qjp0q1Yq/39mM3JphwWugnAMtVdQWAiLwPdAG8AV2Bmu7jWkBWJCtpTCwINp9KWmoK67fuCBrwKwk8cl5zYPeN1WpbN/PcV69xweKvWV6nIZlvZFDt6OPI+eCngMewIfwmHOHcFE0D1nier3W3efUHrhSRtTit89sCHUhEeorIXBGZu3Hjxj2orjHRU9INzZKmxU2qtLsv49OTl3LGz9P44vVbOHfpt7zQrhudr3mRB9bVoGvrtCIteS8bwm/CEU5AD9Sz1v+vtxvwpqo2ADoDo0Sk2LFVdbiqpqtqet26dUtfW2MiLNhScYF0bZ3GRcelkeR2WUkS4aLjnHx4SdPi5uar0xMmK4v+Ix/k5fFPkVnzAM699kVeOKk7uyonF7bAHzmvuc3/bfZYOAF9LXCw53kDiqdUrgfGAKjqTKAaUCcSFTSmrIRaMCJQ+bHzMgtb4/mqjJ2XScb8zICt90KqnDQ9g38OPZyTV81nYPseXHjVMyyr26iwiK8FbvN/m70RTg59DtBURBoDmcDlwBV+ZVYDZwBvishROAHdciompoUzkVa45X1zjt8zZkGR9EvDLet4YspLtPvjZ2Yd3IIHz7mD1bXrk1+wu4x/C9wGDZk9FTKgq2qeiPQCpuB0SRypqotE5FFgrqqOB+4BXhORu3DSMddqtFbOMCZMwW40Zmbn0KjPBJJE6NbmYAZ2bVlied92XxDuO24hO3fu4rq547n323fIq1SJvh168f4xZ6NSidrVKtskWqZMhNUP3e1TPtFvWz/P48VAu8hWzZiyFWwYvk++Ku/MWs3KjdtYtSmn2I0j73F8urZOY9/lSzno7rtovnYpXx56PA+dfSvra+7OQGZvz2V+v7Mj9TaMKWRD/02FVWLe22PG75uDBv4i6ZJdu2DAAM7o3onmO/7ikcse5IaL+hUJ5mA9VkzZsYBuKiz/G5ClVeSG5Zw5cNxx0L8/XHIJLF5M6/v/S0qVol+CrceKKUs2l4up0Lw3IBv1mRD26wScG6Hbt8O998Lzz0O9ejB+PJx3nnNst2euTXNryosFdJMwHspYyOjZa8hXLXZDE0LPZV4a9VNTYNo0uPFG+P13uOkmePJJqFWrSDnrsWLKkwV0kxAeyljIO7NWFz733dAEGNi1pdNX/KMF5OY7tzYzs3Po/dECgFIH3Jo7/+XW91+GvpPZ1uAQakydCu3bR+idGLPnLIduEsLo2WtK3D7g00WFwdwnN18Z8OmisI7vGwl65vLZfD7iv1z28+e8esKFnNz9BTJSD9+LmhsTORbQTUIoaSZECL2SEMCVbRsGLHNl24bMuL4lwyc/x4ixj7ElpSYXXPUMT7TvwRaSCxe4MCbaLOVi4kKo/HdJc5WHa2DXlqzcuI0Zv28u3NauSW0Gbl8IR7XntC1bee6k7gxtezG5Sbsn0bKZEE2ssIBuYl44a3m2bVK7SCD2adukNgDJlSC3oPixkz3fUTPmZ/LDyi2Fzw/6+y+uf/pRWP4DtGlDjxN78l3VA4sdw/qVm1hhKRcT80Kt5QmwalPgVrJve36QYZ7e7f3HLyK3QBEt4IqfJvHF6//lxFULeKbDTTBjBo1OPT7gMdofaTOHmthgLXQT80LNoRJOmYIgAd27PTsnl0abMxk0+SXarvmFGYccTZ+Ot7Mm9SDuTUpi2tLA880F225MebOAbmJesDlXvKmOUGWE4pP4+7YDkJfHjbPHcc9377ArKZn7Ot7OmKPPAk8OPpwPFmOiyVIuJuaVtFKQt4x3ZSBwVgrylQk2sZYC/PwznHgiD349km8aH8uZ17/CmGPOLgzmvlWEguXKLYduYoUFdBPzwln0Ye4fm4vMMQ6QX6DM/aP4jVKfKnm53PXtu84cLH/8wQ+DhnLrxQ+xYd/9C8skJ0nheqDhfLAYE00W0E1CeNczSjSc7a0zl/LZm3dwx/ej4fLLYckSTrj/Zp6+pFWRD46nLz6m8IPDVhMysc5y6CbmhdNtscSUikfKrh3c++0orps7nvX77s+1Fz/Cm6P6F+4PNfeKzc1iYpkFdBMTSho4VNql4gJJS03hkJ9mMmjySzTc+idvtz6Hp069hloH7h/6xcbECQvoJupCtcDD6V2yT5Uk/t2VH7DchG8W8+JXL5P+5cesqF2fS68YxA8Ht3COb33ITQIJK4cuIh1FZJmILBeRPkHKXCoii0VkkYi8F9lqmkQWauBQOL1LHr+gZcAyZ/02izbnnkKrqZ8wtM3FdLrupcJgDtaH3CSWkAFdRJKAIUAnoBnQTUSa+ZVpCvQF2qlqc+DOMqirSVDBlnfzbQ82EtO73T/1UuffLbz8yZO8Nm4gG1JqcsFVz/HkadeyM7lqkXLWh9wkknBSLicAy1V1BYCIvA90ARZ7ytwIDFHVLQCquiHSFTWJK9TEWuGO0ExLTSFzy3YuWDSNfl+9RvXcHJ4++So+7XAl+UmVIcTgJGPiXTgBPQ3wTja9FmjjV+ZwABGZASQB/VV1sv+BRKQn0BOgYcPAU5WaiifU1LfhjtDsd3QNUm6/n1N+n8u8+kdyX6c7yKrXiCc6O/3IvXl6sD7kJvGEE9ADzT/q/z+wMtAUOA1oAHwrIi1UNbvIi1SHA8MB0tPTg/U0M6aIkEP/Cwpg2DA63H8/eXn5PH/urbx81NkctF8NnvCbZtfW9zSJLJyAvhY42PO8AZAVoMwsVc0FVorIMpwAPycitTQVWvsj6xZZXs67nV9/hRtugG+/hbPOovLw4dzVqBF3RaGexkRbOAF9DtBURBoDmcDlwBV+ZTKAbsCbIlIHJwWzIpIVNfFrbxenCJRDTyrIp/6rg2HaKO1RaMwAABoVSURBVEhJgTfegGuuKTKZln8dQg1OMibehezloqp5QC9gCrAEGKOqi0TkURE53y02BdgkIouBaUBvVd1UVpU28cMXSDOzc1B2B9KM+ZmFZbq1OTjga33b/XPlzf5cQcbbd3PL5BHQuTMsXgzXXhs0mEN4c6obE+/CGlikqhOBiX7b+nkeK3C3+2NMoXBGeQZc+u3Q/RjY1elb7suhV83bxW3fv8/Nsz5iS/Wa3HvZQ8xs1Z6sF38MmRO3qW9NRWCTc5kyFU4gzZifyQ+rthTZ/8OqLYWt+PZH1uXYtUuY8Mbt9Jo5hozm7Tnz+qGMa9y2xJa/l019ayoCG/pvylSwHiq1UpJpN2gqWdk5iBRfUSg3Xxnw6SK6Nq3FMU89wqMzM8iqWYerLxnAN02Ocwr5vaak+V16dzjCui2ahGctdFOmAs0hnlxJ+HdXXmHrOtjycC0WzYYWLbhoZgZvH3sOHXoM2R3Mgwj2jcCmvjUVgbXQTZnyzpjo6+WyfVceW7bnBn1NzR3bePirEVzyy5f80+hQ7rvpeSalHhbW+UpKodjUtybRWUA3Zc4/kDbuMyFo2Q7LvuexL4ay3/atDGl7CR917sEd57bka790SXIlAXFSMz6WQjEVnQX0BBeqD3gkjlPacwTKq9fdtoUBXwyl86/fs+iAJlx3SX8WHXgo8m9+wFa+L3An4sjPSP3OTMVjAT2BRWowTUnHAej90YLClnJmdg69P1pQ5Bz+Aar9kXUZOy/TOZ4qF//yFQ9NHUFK7k6ePPUaXjv+AvKSnD9NXwolWLok0QKdDYAye8MCegKLxEo/oY6zfVdekbQHeHqotE4LGKDGzsvk2Ia1WPPjUgZOfolTVs3nhwbN6NPxdlbs36DIsSpaCiVSvzNTMVlAT2CRGkxT0nGCzbDmu+k54NNFxQLUjl25NB3zFsOnv4WK8PBZN/NO686oFO10JVS8VqkNgDJ7wwJ6HAuVaw05S2GYSjpOsMUpfPXz781y6KY1DJr0EsdnLmZ642N5oEMvMmsdEPD13dtWvCmWI/U7MxWT9UOPU+HMkRKoD/ie9ATZ0+N450mpnJ/HLTPHMPGN22i6aTV3n3MX11wyIGAwTxLhyrYNC4f+VySR+p2Zisla6HEqnFxrsN4hpU1jlHScAZ8uCtinvHb15MI0QfP1y3lq0mCab1jBhCPa8chZN/PXPrWLvSYtNYUZfU4vVd0STaR+Z6ZiEg2yWkxZS09P17lz50bl3ImgcZ8JAfPXAqwcdE651eOhjIUB5yq/sm1DZixcyyUTR9Jz9jg2V6/Fw2f/lymH/4fkSlA5Kalov/IkYZ8qldmak2tBzJgSiMg8VU0PtM9a6HEqVnKtwdb73DR5KuMnD2bfP1bwQcuzePz06/m7Wg1npwgXHZfGtKUbycrOIbV6Mtt25JGd47T0raueMXvGcuhxKlZyrf69L/bZuZ0BXwxl6Kt3sq8UcPPVT3B/5zt2B3Ocbo3Tlm5kRp/TWTnoHKpXqUyu34QuNle5MaVnLfQyUtaj/WIl1+r9pnDa73N5fMoQ6v3zF2P+cyGXTnmLKQOnB3yd94PAuuoZExkW0MtAeY32i4XJphrtn8K/6/7k4a9e46JF0/ht/4O5+MqnSDmlHZfWqEGtlOTCVIpXrZTkwsexkj4yJt5ZQC8D8TTaL9xvEgHLtapP7Ynj+eLzYaTu+IfBJ17Gy/+5nF2Vk0la4SxYEWxVOO/2QHOVC84HYbtBU+0GqTFhCiuHLiIdRWSZiCwXkT4llLtYRFREAt6BrSjiJYUQTl/2YOWee2s6WWd04uWMQayrWYfzr3me5065il2VnZa3b9HnYNPkerd75yoHJ5j7MuqhViIyxuwWMqCLSBIwBOgENAO6iUizAOX2BW4HZke6kvEmXpY7C3fh5CLlVLnk58/5bFhP9v92GoPa9+CCq55lyQFNirwmyW2CJwVpovtv79o6jRl9TictNaVYd0y7QWpMeMJpoZ8ALFfVFaq6C3gf6BKg3GPAU8COCNYvLsVKD5RQwv0m4XveIHs9oz54mKcnDWbJAY3peN1LDDvhQvIrJRU7RtsmzsCh/CDjHIJtj5dvN8bEonBy6GnAGs/ztUAbbwERaQ0crKqfici9wQ4kIj2BngANGybuPB2x0gMllFA3I315cynI59ofP6P3N2+TL5V48OxbeK9Vx2KTaXmt2uQcN0kkYPAO1nK3G6TG7LlwAnqg/3mF/0NFpBLwPHBtqAOp6nBgODgjRcOrYuwJ50ZiLPRACaV3hyOKzGUOzojN3h2OKMybp61byUeTXuTYrGVMa3IcD3ToxbqadUMe29eiLm0L3RZzNmbPhRPQ1wIHe543ALI8z/cFWgBfi9PqOggYLyLnq2rCje1PuAUI/OOq+/z5Cb9ww9fv0GvmB/xbpTp3nHsPnzQ7LXi3FT++bolpQVrcaUFa3PHy7caYWBROQJ8DNBWRxkAmcDlwhW+nqm4F6viei8jXwL2JGMwhvrokhvL0lGXFRmjmFiifvj6eYWOe4qiNqxh/1CkMOKMnm/ZJLdWxfXF/T1rc8fDtxphYFDKgq2qeiPQCpgBJwEhVXSQijwJzVXV8WVcylsTbTbuS0kP+da6au5O7vnuXG+dksHnf2txw4cN82bRNoMOGlO3pllgtuVJhQE9NSab/+c0tYBtTBsIaWKSqE4GJftv6BSl72t5XK3bF0027UOkh73tps3ohgyYPpvGWdXxy/DlUfuYpvpy4co/PXT81pdj5AXbmFezFOyp7tkCziWc2OVcpxUuXRAjdz7z9kXWpsXM7A6cM4YPRfamkSrfLH2fOA09wzinNSEne8z+P9kfWDbufe6wId6CVMbHKAnopeUc1Cs7NvScubBmTrbhQ6aGtH2Xw+eu30G3BFF47visdr3uZmYccw4Sf1wFQLbl4/3J/wW6R+qbGLU29oi3ePoCM8WdzueyBeLlpFyw9dFTyLrjySl56912W1WnILV378lP93d8wfMPys4MM2/cK1vfUl7KIl/QUxN/9EWP8WQs9gRVLD6ly3uLpjHr2WnLf/4AX2nXj3GtfLBLMvYIF3nA6Lvryz/GSnoL4mbLBmGCshZ7AvH2689asYeDnQzlr+Wx+qteU+zvdwbK6jYK+tt2gqTTaP3ALO9SIMF/Qjrc+5TaoycQ7W1M0joXVI0MVRoxg2213kZSfx7Mnd2dkehcKAsy/4s8762E4BGI+aIdivVxMrLM1RRNQWCNWf/8dbrwRpk1jYcOW9Ol4G3/Urh/2OUoTzNNSU5jR5/RSvCI2xcv9EWMCsRx6nCqxR0Z+Pjz3HLRsCfPmwauv0vum50oVzEvin0O3tIQxscECepwK1vOixm9L4D//gXvugTPOgEWLoGdP7u10VLEblOEIFLy7t20YF902jaloLOUSZ3w5Xv90SHJ+LrfM/JBes8ZA7VQYPRouu6xwUpVANyiz3AE0waQkJ3HRcWmFfcotp2xMbLOAHkNC3ZALNJQe4JisZTw5aTBH/vUHC07pzDFj34I6dfwPXyw/3KjPhKB1SbPgbUzcsYAeJf7Bu/2RdRk7L7PEm5z+efNquTu459t36DF3PBv2qU2Pi/qxLP1UZniC+UMZCxk9ew35qiSJ0K3NwQzs2hIoeWrbRLjBaUxFYwE9CgL1UHl31uqAa2kO+HRRYeD37j/xj58ZNHkwh2Sv591WHRl02nX8U3UfxBOgH8pYyDuzVhc+z1flnVmrWblxG6s25QQM5pXAbnAaE6csoEdBoB4qwXLZW7bnFg7FB9h357/0nTaSKxZMYVVqPS7v9j9mNTy6cL93VOPo2WsIZMbvm4PWrQCY+8dmS7UYE4csoEfBns4Ncsby2Tw+ZQh1/81m2AkX8sJJV7AjuVrhfv/ug8GWeQtl9Ow1hWkZY0z8sIAeBcEmrQo2MnO/7Vvp/+WrnL/kG5bUbcSNFz7ML/WaUi25EpXyCihQZ9Hli44retMz2ALNoezpB4ExJrqsH3oU9O5wBMlJRXt4JydJsf7dqdUq02XRNL4c8V86LvueZ0/qzvnXPM/mo47m+ctaAYJvBbl8VcbOyywyd3e3NgezJ5LCXDfUGBNbrIUeJfl+a3nmFyjph+y3O9WxZg3rr7iOg777ivn1juC+TrfzW91DCtMq4axt6juWt5dL2ya1+XH11mKv9drTDwJjTHSFNTmXiHQEXsRZU3SEqg7y2383cAOQB2wEeqjqHyUdsyJPztX60c+L3Oj0qV09mfkPnQnDh8N995GXm8fzp17N0KM7U1ApidrVk3nkPGc9zsZ9JgRMzwiwctA5JZ7f22WyWnIldnrSNt5ujcaY2LNXk3OJSBIwBDgLWAvMEZHxqrrYU2w+kK6q20Xkv8BTwGV7X/XIi4XZ9AIFc4Baa1fB6afD9OlsOOEkuh/fg99qHFC4f0fu7vU4U6snBzxOavXkkOdP1AmoYuF3a0w0hZNyOQFYrqorAETkfaALUBjQVXWap/ws4MpIVjJSwpqhMILnCje4JBXkc/2cDO7+7l3YJwVef50L/jyEzK07ipTLyc2n/3inX3qwD4WKej+zPH+3xsSqcG6KpgHeDs1r3W3BXA9M2ptKlZXyWjMy1GLDqSm7W9FHbVjBx6Pu4YGv32DmocfB4sXQowdZfsHcJzsnN2APGZ+tOaGXjUtEth6oMeEF9EBdHgK2A0XkSiAdeDrI/p4iMldE5m7cuDH8WkZIea0ZGSq49D+/OfsU5HH3N6MY/9Zd1Pv7L27r2oet742B+s4Ut7VSQqdOAqmoy6XZeqDGhJdyWQt4uz00ALL8C4nImcCDwKmqujPQgVR1ODAcnJuipa7tXiqvRYtDBZeuO1Zz+ke9qbnyN8Y1b89rF9zGTReeUCQ1sCc9ByvyvOTxtiC1MWUhnBb6HKCpiDQWkSrA5cB4bwERaQ28CpyvqhsiX83IKK9Fi4MFkSbVgTvvhHbtqJm3AyZO5MJfpjLpsQuK5Xmzg+TIg6no85LH24LUxpSFkC10Vc0TkV7AFJxuiyNVdZGIPArMVdXxOCmWGsCH4jQtV6vq+WVY7z1SXosWB1ps+PQ1P/PS10Mhaw3ccgs88QTUrBn0GOGOJk1JTqrQgdwn3hakNqYs2CLRZcTXy+Wf9Rt5fMabnDd3MjRtCq+/DiefvEdzn9uCE8YYWyQ6Crq2TqPrH3Ng8F2wYQP06QP9+kFKChnzM7l7zE+Fw/Yzs3O4e8xPha/z/mstTmNMuCygl4U//4TbboMPP4RjjoFPP4Xjjivc/cC4n/Eb+U+BOttLE7BtII0xxssCeoRkzM/k6clLaTtjAo9MHcE+eTtJevxx6N0bkot2QdzuGfEZaHvG/EwGfLqoyOChzOwcen+0AHBa7zaQxhjjz2ZbjICM+ZkMfuMrHn/tfp6d8Dy/1U7j/OtfIqPTNcWCeTjH6jtuYcCRoLn5yoBPFwE2kMYYU5y10PdWQQG/93+S8ZNHIKo8cuZNjGrtTKblnfkwXIECtZcv0NtAGmOMPwvoe2PZMrjhBu757ju+adSaBzr2Ym2tAwt3Bwuu+1RJ4t9dxYP2PlWSwg7ItVKSyQ4wzH9PR5gaY+KfBfQ9kZsLzz4L/ftDSgqPX9yb15qcUmx4Z7ABRtsDBHPf9mD9z31888AEG0lqa1MYU3FZDr205s+HNm2gb1845xxYsoTmD9xBSpWin43JScK/O/No3GcC7QZNLbKSUEpy4Mueklwp4IjHwmNWEvqf3xwIPpK0tCNMjTGJwwJ6uHbsgAcfhOOPZ8fqtTzY/REaN+1BuzedWYSfuLBl4fJxtasngzozIwaabTEnL3Avl5y8Arq2Tis8FuxeDi4tNYWnLzmmMCcfrPVvc5cYU3FZQA/HjBnQqhX873+s7nwhp107hHcbHF8kWAPM6HM6KwedQ/Uqlcn162ju7YESbHCub3vX1mnM6HM6L1zWioNqVQs43aXNXWKM8WcBvSTbtsHtt8PJJzst9MmT6fafm1lfuXqRYv7dBUP1QAm2CLN3e6g51b0ted+i0janizEVm90UDWbKFLjpJli9Gnr1gv/9D2rUIGvahIDFvUE81FSu3doczDuzVhfb712cOZxFoBN1KTljzJ6xFrq/zZvh2muhY0eoVg2+/RYGD4YaNYDwcteh0iEDu7bkyrYNC1vkSSJc2bZhkcWZrZ+5Maa0rIXuNXYs3Hor/PUXPPAAPPywE9Q9Ak2N65+7DmdirYFdWxYJ4P5swQZjTGlZQAdYt85Jq4wbB61bw+TJzk3QAMKdBXFv0yHhfHAYY4xXxQ7oqvDWW3DXXZCTA4MGwd13l3r+lbJg0+caY0qr4gb0VaugZ0/44gs46SQYMQKOCN36Lc9ZDu2mpzGmNCreTdH8fHjpJWjRAmbOhCFDYPr0sII52CyHxpjYVbFa6EuWwA03wPffO71Yhg2DQw4p1SGs94kxJlaF1UIXkY4iskxElotInwD7q4rIB+7+2SLSKNIVDVfG/EzaDZpadA6V3Fx4/HHnRufSpfD22zBxYqmDOdiQe2NM7AoZ0EUkCRgCdAKaAd1EpJlfseuBLap6GPA88GSkKxqOQKMrRw35mK0tWsFDD0GXLrB4MVx11R5PS2hD7o0xsSqclMsJwHJVXQEgIu8DXYDFnjJdgP7u44+Al0VEVIPNWlI2vPntqrk7uXPGaG78YRzZNVKdLokXXLDX57DeJ8aYWBVOQE8D1nierwXaBCujqnkishXYH/jLW0hEegI9ARo2bLiHVQ7Ol8c+fs0vDJr8EoduzuT9o8/mifY9WBCBYO5jvU+MMbEonIAeKDfh3/IOpwyqOhwYDpCenh7x1vth1Qq4avyrXD1/AmtqHUj3ywYyo1GrwqlojTEmkYUT0NcCB3ueNwCygpRZKyKVgVrA5ojUMFyTJvHJqzdTbf06Xk/vwjMnX0VOlWqW3zbGVBjh9HKZAzQVkcYiUgW4HBjvV2Y8cI37+GJgarnlzzdtgquvhs6dqV67Ft++kcHIi25nR5VqNqWsMaZCCdlCd3PivYApQBIwUlUXicijwFxVHQ+8DowSkeU4LfPLy7LSbsXgww+dOVi2bHEm0nrwQU6tWpUZZX5yY4yJPWENLFLVicBEv239PI93AJdEtmolyMpyZkXMyIDjjoMvv4Sjjy630xtjTCyKv5GiEyfCFVfAzp3w1FPOxFqV4+9tGGNMpMVfJDz8cDjxRGfRiaZNo10bY4yJGfEX0A87DCZNinYtjDEm5lS82RaNMSZBWUA3xpgEYQHdGGMShAV0Y4xJEBbQjTEmQVhAN8aYBGEB3RhjEoQFdGOMSRBSzosK7T6xyEbgjz18eR38Fs+IYfFSV6tn5MVLXa2ekVXW9TxEVesG2hG1gL43RGSuqqZHux7hiJe6Wj0jL17qavWMrGjW01IuxhiTICygG2NMgojXgD482hUohXipq9Uz8uKlrlbPyIpaPeMyh26MMaa4eG2hG2OM8RN3AV1EOorIMhFZLiJ9ol0fHxE5WESmicgSEVkkIne42/uLSKaI/OT+dI6Buq4SkYVufea62/YTkS9E5Df339oxUM8jPNftJxH5W0TujIVrKiIjRWSDiPzi2RbwGopjsPs3+7OIHBvlej4tIkvdunwsIqnu9kYikuO5rsPKq54l1DXo71pE+rrXdJmIdIhyPT/w1HGViPzkbi/fa6qqcfODs0j170AToAqwAGgW7Xq5dasHHOs+3hf4FWgG9AfujXb9/Oq6Cqjjt+0poI/7uA/wZLTrGeB3vx44JBauKXAKcCzwS6hrCHQGJgECtAVmR7meZwOV3cdPeurZyFsuRq5pwN+1+39rAVAVaOzGhaRo1dNv/7NAv2hc03hroZ8ALFfVFaq6C3gf6BLlOgGgqutU9Uf38T/AEiAturUqlS7AW+7jt4CuUaxLIGcAv6vqng5GiyhV/QbY7Lc52DXsArytjllAqojUi1Y9VfVzVc1zn84CGpRHXUIJck2D6QK8r6o7VXUlsBwnPpS5kuopIgJcCowuj7r4i7eAngas8TxfSwwGTRFpBLQGZruberlfb0fGQioDUOBzEZknIj3dbQeq6jpwPpyAA6JWu8Aup+h/kli7phD8Gsby320PnG8PPo1FZL6ITBeRk6NVKT+Bftexek1PBv5U1d8828rtmsZbQJcA22Kqm46I1ADGAneq6t/AUOBQoBWwDufrWLS1U9VjgU7ArSJySrQrVBIRqQKcD3zoborFa1qSmPy7FZEHgTzgXXfTOqChqrYG7gbeE5Ga0aqfK9jvOiavKdCNog2Pcr2m8RbQ1wIHe543ALKiVJdiRCQZJ5i/q6rjAFT1T1XNV9UC4DXK6WthSVQ1y/13A/AxTp3+9KUB3H83RK+GxXQCflTVPyE2r6kr2DWMub9bEbkGOBform6y101fbHIfz8PJSx8evVqW+LuOxWtaGbgQ+MC3rbyvabwF9DlAUxFp7LbaLgfGR7lOQGHu7HVgiao+59nuzZVeAPzi/9ryJCL7iMi+vsc4N8h+wbmO17jFrgE+iU4NAyrS6om1a+oR7BqOB652e7u0Bbb6UjPRICIdgfuB81V1u2d7XRFJch83AZoCK6JTy8I6BftdjwcuF5GqItIYp64/lHf9/JwJLFXVtb4N5X5Ny+vua6R+cHoM/IrzSfdgtOvjqddJOF/5fgZ+cn86A6OAhe728UC9KNezCU7vgAXAIt81BPYHvgJ+c//dL9rX1K1XdWATUMuzLerXFOcDZh2Qi9NavD7YNcRJDwxx/2YXAulRrudynPyz7+90mFv2IvdvYgHwI3BeDFzToL9r4EH3mi4DOkWznu72N4Gb/cqW6zW1kaLGGJMg4i3lYowxJggL6MYYkyAsoBtjTIKwgG6MMQnCAroxxiQIC+jGGJMgLKAbY0yCsIBujDEJ4v9ZqziLBKjw2gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 score: 0.87\n"
     ]
    }
   ],
   "source": [
    "#graph for speed\n",
    "xfit = np.linspace(0, 180, 1000)\n",
    "yfit = model_Speed.predict(xfit[:, np.newaxis])\n",
    "y_pred = model_Speed.predict(X_test_Speed)\n",
    "plt.scatter(X_test_Speed, y_test)\n",
    "plt.plot(xfit, yfit, color = 'red')\n",
    "plt.suptitle(\"Win Rate vs Speed\")\n",
    "plt.title(\"R^2 score :  %.2f\" % r2_score(y_test, y_pred))\n",
    "plt.show()\n",
    "\n",
    "print('R^2 score: %.2f' % r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3.1 part: Logistic Regression for legendary pokemon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.special\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, plot_confusion_matrix\n",
    "from time import time\n",
    "one_hot = pd.get_dummies(pokemon[['Type 1', 'Type 2']])\n",
    "pokemon = pd.concat([pokemon, one_hot], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.concat([pokemon.iloc[:,4:12],pokemon.iloc[:,14:49]], axis=1).values\n",
    "X = features\n",
    "y = pokemon['Legendary']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=4211)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**setting 1(not stable convergence) :  Î·  = 0.7, adaptive learning rate (random_state 0, 1, 2 for each the three repetition)**\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 274.38, NNZs: 41, Bias: -15.050000, T: 640, Avg. loss: 2824.867408\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 328.88, NNZs: 42, Bias: -29.750000, T: 1280, Avg. loss: 2577.347575\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 367.22, NNZs: 42, Bias: -44.450000, T: 1920, Avg. loss: 2761.741746\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 295.12, NNZs: 42, Bias: -57.050000, T: 2560, Avg. loss: 2958.537670\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 327.78, NNZs: 42, Bias: -73.150000, T: 3200, Avg. loss: 2760.729828\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 476.29, NNZs: 42, Bias: -85.050000, T: 3840, Avg. loss: 2671.789584\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 489.34, NNZs: 42, Bias: -99.750000, T: 4480, Avg. loss: 2772.123224\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 368.26, NNZs: 42, Bias: -101.710000, T: 5120, Avg. loss: 1056.839050\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 323.98, NNZs: 43, Bias: -104.229999, T: 5760, Avg. loss: 711.355479\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 317.14, NNZs: 43, Bias: -106.775134, T: 6400, Avg. loss: 545.206944\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 317.95, NNZs: 43, Bias: -109.435136, T: 7040, Avg. loss: 518.175787\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 318.62, NNZs: 43, Bias: -111.565073, T: 7680, Avg. loss: 523.162302\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 321.39, NNZs: 43, Bias: -114.353420, T: 8320, Avg. loss: 495.753158\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 324.05, NNZs: 43, Bias: -116.593420, T: 8960, Avg. loss: 513.007395\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 332.49, NNZs: 43, Bias: -119.393249, T: 9600, Avg. loss: 564.734607\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 336.87, NNZs: 43, Bias: -121.773249, T: 10240, Avg. loss: 512.888452\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 343.58, NNZs: 43, Bias: -124.713377, T: 10880, Avg. loss: 500.820684\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 350.64, NNZs: 43, Bias: -127.235091, T: 11520, Avg. loss: 515.355874\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 344.84, NNZs: 43, Bias: -127.511252, T: 12160, Avg. loss: 191.323835\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 341.54, NNZs: 43, Bias: -127.792088, T: 12800, Avg. loss: 152.070134\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 340.29, NNZs: 43, Bias: -128.125780, T: 13440, Avg. loss: 120.381279\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 339.87, NNZs: 43, Bias: -128.490108, T: 14080, Avg. loss: 100.193575\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 339.60, NNZs: 43, Bias: -128.742108, T: 14720, Avg. loss: 98.274105\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 339.98, NNZs: 43, Bias: -129.192764, T: 15360, Avg. loss: 89.734730\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 340.19, NNZs: 43, Bias: -129.596178, T: 16000, Avg. loss: 85.361009\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 339.78, NNZs: 43, Bias: -129.904217, T: 16640, Avg. loss: 89.802047\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 339.67, NNZs: 43, Bias: -130.206168, T: 17280, Avg. loss: 89.969877\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 339.52, NNZs: 43, Bias: -130.514177, T: 17920, Avg. loss: 87.375044\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 339.62, NNZs: 43, Bias: -130.931409, T: 18560, Avg. loss: 89.239584\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 339.77, NNZs: 43, Bias: -131.271812, T: 19200, Avg. loss: 86.771371\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 339.49, NNZs: 43, Bias: -131.294299, T: 19840, Avg. loss: 33.351985\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 339.24, NNZs: 43, Bias: -131.324985, T: 20480, Avg. loss: 27.812194\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 339.02, NNZs: 43, Bias: -131.352960, T: 21120, Avg. loss: 23.896170\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 338.82, NNZs: 43, Bias: -131.392790, T: 21760, Avg. loss: 23.048860\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 338.61, NNZs: 43, Bias: -131.414198, T: 22400, Avg. loss: 25.538202\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 338.39, NNZs: 43, Bias: -131.442196, T: 23040, Avg. loss: 23.851253\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 338.21, NNZs: 43, Bias: -131.481268, T: 23680, Avg. loss: 21.724273\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 338.04, NNZs: 43, Bias: -131.521647, T: 24320, Avg. loss: 21.015900\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 39\n",
      "Norm: 337.86, NNZs: 43, Bias: -131.560379, T: 24960, Avg. loss: 20.244817\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 40\n",
      "Norm: 337.64, NNZs: 43, Bias: -131.590292, T: 25600, Avg. loss: 24.361822\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 41\n",
      "Norm: 337.44, NNZs: 43, Bias: -131.616575, T: 26240, Avg. loss: 22.252668\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 42\n",
      "Norm: 337.27, NNZs: 43, Bias: -131.654052, T: 26880, Avg. loss: 20.092132\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 43\n",
      "Norm: 337.09, NNZs: 43, Bias: -131.686947, T: 27520, Avg. loss: 22.445856\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 44\n",
      "Norm: 336.90, NNZs: 43, Bias: -131.721061, T: 28160, Avg. loss: 22.738266\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 45\n",
      "Norm: 336.72, NNZs: 43, Bias: -131.757343, T: 28800, Avg. loss: 21.261960\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 46\n",
      "Norm: 336.53, NNZs: 43, Bias: -131.799613, T: 29440, Avg. loss: 24.745298\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 47\n",
      "Norm: 336.36, NNZs: 43, Bias: -131.839034, T: 30080, Avg. loss: 20.184076\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 48\n",
      "Norm: 336.31, NNZs: 43, Bias: -131.836800, T: 30720, Avg. loss: 14.990179\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 49\n",
      "Norm: 336.26, NNZs: 43, Bias: -131.841747, T: 31360, Avg. loss: 12.820874\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 50\n",
      "Norm: 336.21, NNZs: 43, Bias: -131.847341, T: 32000, Avg. loss: 13.500626\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 51\n",
      "Norm: 336.17, NNZs: 43, Bias: -131.846815, T: 32640, Avg. loss: 12.532206\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 52\n",
      "Norm: 336.12, NNZs: 43, Bias: -131.852342, T: 33280, Avg. loss: 12.591435\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 53\n",
      "Norm: 336.07, NNZs: 43, Bias: -131.858627, T: 33920, Avg. loss: 13.454568\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 54\n",
      "Norm: 336.03, NNZs: 43, Bias: -131.864220, T: 34560, Avg. loss: 13.221861\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 55\n",
      "Norm: 335.98, NNZs: 43, Bias: -131.865219, T: 35200, Avg. loss: 13.243642\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 56\n",
      "Norm: 335.93, NNZs: 43, Bias: -131.873739, T: 35840, Avg. loss: 13.117553\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 57\n",
      "Norm: 335.92, NNZs: 43, Bias: -131.872541, T: 36480, Avg. loss: 11.905579\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 58\n",
      "Norm: 335.91, NNZs: 43, Bias: -131.873841, T: 37120, Avg. loss: 11.797927\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 59\n",
      "Norm: 335.90, NNZs: 43, Bias: -131.874986, T: 37760, Avg. loss: 11.684422\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 60\n",
      "Norm: 335.89, NNZs: 43, Bias: -131.874893, T: 38400, Avg. loss: 11.775890\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 61\n",
      "Norm: 335.88, NNZs: 43, Bias: -131.875755, T: 39040, Avg. loss: 11.686129\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 62\n",
      "Norm: 335.87, NNZs: 43, Bias: -131.875486, T: 39680, Avg. loss: 11.685314\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 63\n",
      "Norm: 335.86, NNZs: 43, Bias: -131.877278, T: 40320, Avg. loss: 11.996528\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 64\n",
      "Norm: 335.85, NNZs: 43, Bias: -131.877311, T: 40960, Avg. loss: 11.730077\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 65\n",
      "Norm: 335.85, NNZs: 43, Bias: -131.877712, T: 41600, Avg. loss: 11.451748\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 66\n",
      "Norm: 335.85, NNZs: 43, Bias: -131.877878, T: 42240, Avg. loss: 11.390129\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 67\n",
      "Norm: 335.85, NNZs: 43, Bias: -131.878158, T: 42880, Avg. loss: 11.390984\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 68\n",
      "Norm: 335.85, NNZs: 43, Bias: -131.878228, T: 43520, Avg. loss: 11.392776\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 69\n",
      "Norm: 335.84, NNZs: 43, Bias: -131.878396, T: 44160, Avg. loss: 11.396691\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 70\n",
      "Norm: 335.84, NNZs: 43, Bias: -131.878549, T: 44800, Avg. loss: 11.397243\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 71\n",
      "Norm: 335.84, NNZs: 43, Bias: -131.878672, T: 45440, Avg. loss: 11.390679\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 72\n",
      "Norm: 335.84, NNZs: 43, Bias: -131.878721, T: 46080, Avg. loss: 11.298661\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 73\n",
      "Norm: 335.84, NNZs: 43, Bias: -131.878769, T: 46720, Avg. loss: 11.295511\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 74\n",
      "Norm: 335.84, NNZs: 43, Bias: -131.878817, T: 47360, Avg. loss: 11.292744\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 75\n",
      "Norm: 335.84, NNZs: 43, Bias: -131.878844, T: 48000, Avg. loss: 11.297099\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 76\n",
      "Norm: 335.84, NNZs: 43, Bias: -131.878878, T: 48640, Avg. loss: 11.297134\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 77\n",
      "Norm: 335.84, NNZs: 43, Bias: -131.878903, T: 49280, Avg. loss: 11.296116\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 78\n",
      "Norm: 335.84, NNZs: 43, Bias: -131.878926, T: 49920, Avg. loss: 11.294806\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 79\n",
      "Norm: 335.84, NNZs: 43, Bias: -131.878969, T: 50560, Avg. loss: 11.293408\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 80\n",
      "Norm: 335.84, NNZs: 43, Bias: -131.878975, T: 51200, Avg. loss: 11.277126\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 81\n",
      "Norm: 335.84, NNZs: 43, Bias: -131.878981, T: 51840, Avg. loss: 11.277084\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 82\n",
      "Norm: 335.84, NNZs: 43, Bias: -131.878988, T: 52480, Avg. loss: 11.277038\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 83\n",
      "Norm: 335.84, NNZs: 43, Bias: -131.878994, T: 53120, Avg. loss: 11.276899\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 84\n",
      "Norm: 335.84, NNZs: 43, Bias: -131.879000, T: 53760, Avg. loss: 11.276933\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 85\n",
      "Norm: 335.84, NNZs: 43, Bias: -131.879007, T: 54400, Avg. loss: 11.276888\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 86\n",
      "Norm: 335.84, NNZs: 43, Bias: -131.879008, T: 55040, Avg. loss: 11.273003\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 87\n",
      "Norm: 335.84, NNZs: 43, Bias: -131.879009, T: 55680, Avg. loss: 11.272998\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 88\n",
      "Norm: 335.84, NNZs: 43, Bias: -131.879011, T: 56320, Avg. loss: 11.272990\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 89\n",
      "Norm: 335.84, NNZs: 43, Bias: -131.879012, T: 56960, Avg. loss: 11.272980\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 90\n",
      "Norm: 335.84, NNZs: 43, Bias: -131.879013, T: 57600, Avg. loss: 11.272977\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 91\n",
      "Norm: 335.84, NNZs: 43, Bias: -131.879014, T: 58240, Avg. loss: 11.272970\n",
      "Total training time: 0.05 seconds.\n",
      "Convergence after 91 epochs took 0.05 seconds\n"
     ]
    }
   ],
   "source": [
    "clf_sgd = SGDClassifier(loss='log', max_iter=100, random_state=0, verbose=1, learning_rate ='adaptive', eta0 = 0.7)\n",
    "start = time()\n",
    "clf_sgd.fit(X_train, y_train)\n",
    "log_time = time()-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training time of the model: 0.05 .\n",
      "accuracy: 0.88125\n",
      "f1: 0.42424242424242425\n"
     ]
    }
   ],
   "source": [
    "\n",
    "y_hat = clf_sgd.predict(X_test)\n",
    "print(\"The training time of the model: %.2f .\"\n",
    "      % (log_time))\n",
    "print('accuracy:', accuracy_score(y_test, y_hat))\n",
    "print('f1:', f1_score(y_test, y_hat))\n",
    "setting1_time1 = log_time\n",
    "setting1_acc1 = accuracy_score(y_test, y_hat)\n",
    "setting1_f1 = f1_score(y_test, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 350.48, NNZs: 40, Bias: -17.150000, T: 640, Avg. loss: 2760.089069\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 341.33, NNZs: 41, Bias: -31.150000, T: 1280, Avg. loss: 2808.190502\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 443.45, NNZs: 42, Bias: -45.150000, T: 1920, Avg. loss: 2787.950682\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 319.06, NNZs: 43, Bias: -58.450000, T: 2560, Avg. loss: 2575.494753\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 458.62, NNZs: 43, Bias: -75.250000, T: 3200, Avg. loss: 2895.756405\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 478.53, NNZs: 43, Bias: -86.450000, T: 3840, Avg. loss: 2534.598689\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 598.38, NNZs: 43, Bias: -99.750000, T: 4480, Avg. loss: 2775.263996\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 426.80, NNZs: 43, Bias: -110.950000, T: 5120, Avg. loss: 2848.468669\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 433.48, NNZs: 43, Bias: -125.650000, T: 5760, Avg. loss: 2493.974559\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 570.88, NNZs: 43, Bias: -139.650000, T: 6400, Avg. loss: 2622.120358\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 586.87, NNZs: 43, Bias: -152.950000, T: 7040, Avg. loss: 2597.034475\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 529.76, NNZs: 43, Bias: -167.650000, T: 7680, Avg. loss: 2681.919568\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 667.42, NNZs: 43, Bias: -182.350054, T: 8320, Avg. loss: 2813.228413\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 644.05, NNZs: 43, Bias: -196.350054, T: 8960, Avg. loss: 2793.973038\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 556.89, NNZs: 43, Bias: -198.170054, T: 9600, Avg. loss: 1031.970818\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 519.45, NNZs: 43, Bias: -200.270054, T: 10240, Avg. loss: 700.126213\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 507.78, NNZs: 43, Bias: -201.813802, T: 10880, Avg. loss: 627.143523\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 503.30, NNZs: 43, Bias: -203.913802, T: 11520, Avg. loss: 503.351254\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 502.59, NNZs: 43, Bias: -206.153802, T: 12160, Avg. loss: 481.759520\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 506.89, NNZs: 43, Bias: -208.253789, T: 12800, Avg. loss: 517.181752\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 504.97, NNZs: 43, Bias: -210.493372, T: 13440, Avg. loss: 534.917446\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 511.73, NNZs: 43, Bias: -213.573372, T: 14080, Avg. loss: 531.694327\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 516.74, NNZs: 43, Bias: -216.093372, T: 14720, Avg. loss: 519.355698\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 514.68, NNZs: 43, Bias: -218.193360, T: 15360, Avg. loss: 496.304167\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 511.83, NNZs: 43, Bias: -218.406426, T: 16000, Avg. loss: 141.089586\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 510.42, NNZs: 43, Bias: -218.574280, T: 16640, Avg. loss: 101.489484\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 509.59, NNZs: 43, Bias: -218.882280, T: 17280, Avg. loss: 95.985573\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 508.90, NNZs: 43, Bias: -219.134280, T: 17920, Avg. loss: 73.795988\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 508.13, NNZs: 43, Bias: -219.330320, T: 18560, Avg. loss: 81.852843\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 507.36, NNZs: 43, Bias: -219.629090, T: 19200, Avg. loss: 92.070840\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 506.73, NNZs: 43, Bias: -219.873721, T: 19840, Avg. loss: 86.042587\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 506.01, NNZs: 43, Bias: -220.139051, T: 20480, Avg. loss: 80.839640\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 505.56, NNZs: 43, Bias: -220.418942, T: 21120, Avg. loss: 82.467273\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 505.19, NNZs: 43, Bias: -220.454809, T: 21760, Avg. loss: 39.657235\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 504.87, NNZs: 43, Bias: -220.484558, T: 22400, Avg. loss: 33.896149\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 504.58, NNZs: 43, Bias: -220.499852, T: 23040, Avg. loss: 31.240499\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 504.28, NNZs: 43, Bias: -220.527867, T: 23680, Avg. loss: 31.895691\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 503.98, NNZs: 43, Bias: -220.566739, T: 24320, Avg. loss: 33.329245\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 39\n",
      "Norm: 503.68, NNZs: 43, Bias: -220.597178, T: 24960, Avg. loss: 32.236977\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 40\n",
      "Norm: 503.39, NNZs: 43, Bias: -220.631157, T: 25600, Avg. loss: 29.290366\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 41\n",
      "Norm: 503.11, NNZs: 43, Bias: -220.660331, T: 26240, Avg. loss: 30.526851\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 42\n",
      "Norm: 502.84, NNZs: 43, Bias: -220.675416, T: 26880, Avg. loss: 28.283555\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 43\n",
      "Norm: 502.55, NNZs: 43, Bias: -220.707778, T: 27520, Avg. loss: 32.693004\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 44\n",
      "Norm: 502.26, NNZs: 43, Bias: -220.735759, T: 28160, Avg. loss: 31.273542\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 45\n",
      "Norm: 501.99, NNZs: 43, Bias: -220.769309, T: 28800, Avg. loss: 26.613639\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 46\n",
      "Norm: 501.71, NNZs: 43, Bias: -220.797528, T: 29440, Avg. loss: 29.983378\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 47\n",
      "Norm: 501.43, NNZs: 43, Bias: -220.831128, T: 30080, Avg. loss: 28.429239\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 48\n",
      "Norm: 501.15, NNZs: 43, Bias: -220.852237, T: 30720, Avg. loss: 29.805457\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 49\n",
      "Norm: 500.86, NNZs: 43, Bias: -220.880747, T: 31360, Avg. loss: 31.542746\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 50\n",
      "Norm: 500.57, NNZs: 43, Bias: -220.908678, T: 32000, Avg. loss: 30.467898\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 51\n",
      "Norm: 500.51, NNZs: 43, Bias: -220.910868, T: 32640, Avg. loss: 21.417460\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 52\n",
      "Norm: 500.44, NNZs: 43, Bias: -220.916370, T: 33280, Avg. loss: 22.117227\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 53\n",
      "Norm: 500.38, NNZs: 43, Bias: -220.921965, T: 33920, Avg. loss: 21.975702\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 54\n",
      "Norm: 500.31, NNZs: 43, Bias: -220.924123, T: 34560, Avg. loss: 21.714614\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 55\n",
      "Norm: 500.25, NNZs: 43, Bias: -220.928614, T: 35200, Avg. loss: 21.609281\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 56\n",
      "Norm: 500.18, NNZs: 43, Bias: -220.935029, T: 35840, Avg. loss: 21.696382\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 57\n",
      "Norm: 500.17, NNZs: 43, Bias: -220.935047, T: 36480, Avg. loss: 20.179825\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 58\n",
      "Norm: 500.16, NNZs: 43, Bias: -220.935665, T: 37120, Avg. loss: 20.208461\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 59\n",
      "Norm: 500.15, NNZs: 43, Bias: -220.936736, T: 37760, Avg. loss: 20.055851\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 60\n",
      "Norm: 500.13, NNZs: 43, Bias: -220.938067, T: 38400, Avg. loss: 19.899148\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 61\n",
      "Norm: 500.12, NNZs: 43, Bias: -220.938518, T: 39040, Avg. loss: 20.165574\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 62\n",
      "Norm: 500.11, NNZs: 43, Bias: -220.939194, T: 39680, Avg. loss: 20.149018\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 63\n",
      "Norm: 500.09, NNZs: 43, Bias: -220.940291, T: 40320, Avg. loss: 20.132702\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 64\n",
      "Norm: 500.08, NNZs: 43, Bias: -220.940512, T: 40960, Avg. loss: 20.110031\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 65\n",
      "Norm: 500.07, NNZs: 43, Bias: -220.941632, T: 41600, Avg. loss: 19.955161\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 66\n",
      "Norm: 500.06, NNZs: 43, Bias: -220.941692, T: 42240, Avg. loss: 19.741167\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 67\n",
      "Norm: 500.06, NNZs: 43, Bias: -220.941724, T: 42880, Avg. loss: 19.723761\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 68\n",
      "Norm: 500.06, NNZs: 43, Bias: -220.941814, T: 43520, Avg. loss: 19.703713\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 69\n",
      "Norm: 500.06, NNZs: 43, Bias: -220.941939, T: 44160, Avg. loss: 19.678649\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 70\n",
      "Norm: 500.05, NNZs: 43, Bias: -220.941985, T: 44800, Avg. loss: 19.687897\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 71\n",
      "Norm: 500.05, NNZs: 43, Bias: -220.942141, T: 45440, Avg. loss: 19.673726\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 72\n",
      "Norm: 500.05, NNZs: 43, Bias: -220.942237, T: 46080, Avg. loss: 19.678700\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 73\n",
      "Norm: 500.05, NNZs: 43, Bias: -220.942369, T: 46720, Avg. loss: 19.676855\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 74\n",
      "Norm: 500.04, NNZs: 43, Bias: -220.942457, T: 47360, Avg. loss: 19.668210\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 75\n",
      "Norm: 500.04, NNZs: 43, Bias: -220.942745, T: 48000, Avg. loss: 19.646130\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 76\n",
      "Norm: 500.04, NNZs: 43, Bias: -220.942877, T: 48640, Avg. loss: 19.660458\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 77\n",
      "Norm: 500.04, NNZs: 43, Bias: -220.942965, T: 49280, Avg. loss: 19.661319\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 78\n",
      "Norm: 500.03, NNZs: 43, Bias: -220.943049, T: 49920, Avg. loss: 19.664501\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 79\n",
      "Norm: 500.03, NNZs: 43, Bias: -220.943301, T: 50560, Avg. loss: 19.652302\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 80\n",
      "Norm: 500.03, NNZs: 43, Bias: -220.943384, T: 51200, Avg. loss: 19.663122\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 81\n",
      "Norm: 500.03, NNZs: 43, Bias: -220.943419, T: 51840, Avg. loss: 19.561604\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 82\n",
      "Norm: 500.03, NNZs: 43, Bias: -220.943440, T: 52480, Avg. loss: 19.560528\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 83\n",
      "Norm: 500.03, NNZs: 43, Bias: -220.943483, T: 53120, Avg. loss: 19.559209\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 84\n",
      "Norm: 500.03, NNZs: 43, Bias: -220.943515, T: 53760, Avg. loss: 19.561423\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 85\n",
      "Norm: 500.03, NNZs: 43, Bias: -220.943534, T: 54400, Avg. loss: 19.560151\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 86\n",
      "Norm: 500.02, NNZs: 43, Bias: -220.943563, T: 55040, Avg. loss: 19.560845\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 87\n",
      "Norm: 500.02, NNZs: 43, Bias: -220.943587, T: 55680, Avg. loss: 19.560095\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 88\n",
      "Norm: 500.02, NNZs: 43, Bias: -220.943627, T: 56320, Avg. loss: 19.558857\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 89\n",
      "Norm: 500.02, NNZs: 43, Bias: -220.943633, T: 56960, Avg. loss: 19.540044\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 90\n",
      "Norm: 500.02, NNZs: 43, Bias: -220.943639, T: 57600, Avg. loss: 19.540072\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 91\n",
      "Norm: 500.02, NNZs: 43, Bias: -220.943645, T: 58240, Avg. loss: 19.539951\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 92\n",
      "Norm: 500.02, NNZs: 43, Bias: -220.943651, T: 58880, Avg. loss: 19.539822\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 93\n",
      "Norm: 500.02, NNZs: 43, Bias: -220.943657, T: 59520, Avg. loss: 19.539915\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 94\n",
      "Norm: 500.02, NNZs: 43, Bias: -220.943662, T: 60160, Avg. loss: 19.539870\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 95\n",
      "Norm: 500.02, NNZs: 43, Bias: -220.943664, T: 60800, Avg. loss: 19.535778\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 96\n",
      "Norm: 500.02, NNZs: 43, Bias: -220.943665, T: 61440, Avg. loss: 19.535772\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 97\n",
      "Norm: 500.02, NNZs: 43, Bias: -220.943666, T: 62080, Avg. loss: 19.535761\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 98\n",
      "Norm: 500.02, NNZs: 43, Bias: -220.943667, T: 62720, Avg. loss: 19.535751\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 99\n",
      "Norm: 500.02, NNZs: 43, Bias: -220.943668, T: 63360, Avg. loss: 19.535738\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 100\n",
      "Norm: 500.02, NNZs: 43, Bias: -220.943669, T: 64000, Avg. loss: 19.535729\n",
      "Total training time: 0.06 seconds.\n",
      "Convergence after 100 epochs took 0.06 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "clf_sgd = SGDClassifier(loss='log', max_iter=100, random_state=1, verbose=1, learning_rate ='adaptive', eta0 = 0.7)\n",
    "start = time()\n",
    "clf_sgd.fit(X_train, y_train)\n",
    "log_time = time()-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training time of the model: 0.06 .\n",
      "accuracy: 0.875\n",
      "f1: 0.4117647058823529\n"
     ]
    }
   ],
   "source": [
    "y_hat = clf_sgd.predict(X_test)\n",
    "print(\"The training time of the model: %.2f .\"\n",
    "      % (log_time))\n",
    "print('accuracy:', accuracy_score(y_test, y_hat))\n",
    "print('f1:', f1_score(y_test, y_hat))\n",
    "setting1_time2 = log_time\n",
    "setting1_acc2 = accuracy_score(y_test, y_hat)\n",
    "setting1_f12 = f1_score(y_test, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 356.82, NNZs: 40, Bias: -16.450000, T: 640, Avg. loss: 2681.264171\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 482.28, NNZs: 41, Bias: -31.150000, T: 1280, Avg. loss: 2690.412104\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 480.28, NNZs: 41, Bias: -46.550000, T: 1920, Avg. loss: 2700.718148\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 375.13, NNZs: 41, Bias: -61.250000, T: 2560, Avg. loss: 3123.947489\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 462.16, NNZs: 42, Bias: -75.950000, T: 3200, Avg. loss: 2671.302308\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 493.55, NNZs: 42, Bias: -89.250000, T: 3840, Avg. loss: 2954.444103\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 424.58, NNZs: 42, Bias: -101.850000, T: 4480, Avg. loss: 2766.919238\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 471.24, NNZs: 42, Bias: -115.150000, T: 5120, Avg. loss: 2732.415074\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 478.60, NNZs: 42, Bias: -131.250000, T: 5760, Avg. loss: 2797.125059\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 522.27, NNZs: 43, Bias: -144.550000, T: 6400, Avg. loss: 2923.044995\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 426.78, NNZs: 43, Bias: -145.530000, T: 7040, Avg. loss: 936.340742\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 396.90, NNZs: 43, Bias: -147.496618, T: 7680, Avg. loss: 630.202076\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 390.91, NNZs: 43, Bias: -149.876598, T: 8320, Avg. loss: 514.875894\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 394.33, NNZs: 43, Bias: -152.256598, T: 8960, Avg. loss: 520.435755\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 400.91, NNZs: 43, Bias: -154.916610, T: 9600, Avg. loss: 518.620044\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 403.07, NNZs: 43, Bias: -157.436610, T: 10240, Avg. loss: 472.848969\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 405.01, NNZs: 43, Bias: -160.096573, T: 10880, Avg. loss: 528.947887\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 408.38, NNZs: 43, Bias: -162.756573, T: 11520, Avg. loss: 501.958584\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 409.52, NNZs: 43, Bias: -165.136572, T: 12160, Avg. loss: 489.069592\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 415.66, NNZs: 43, Bias: -167.625678, T: 12800, Avg. loss: 457.430287\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 422.73, NNZs: 43, Bias: -169.585678, T: 13440, Avg. loss: 500.686040\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 421.45, NNZs: 43, Bias: -171.965281, T: 14080, Avg. loss: 506.546382\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 424.40, NNZs: 43, Bias: -173.924647, T: 14720, Avg. loss: 470.426079\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 426.62, NNZs: 43, Bias: -176.444647, T: 15360, Avg. loss: 562.465925\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 433.28, NNZs: 43, Bias: -179.244662, T: 16000, Avg. loss: 535.408898\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 429.59, NNZs: 43, Bias: -179.496662, T: 16640, Avg. loss: 145.012855\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 427.46, NNZs: 43, Bias: -179.693086, T: 17280, Avg. loss: 115.620536\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 426.82, NNZs: 43, Bias: -180.021721, T: 17920, Avg. loss: 85.217836\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 426.25, NNZs: 43, Bias: -180.357694, T: 18560, Avg. loss: 99.578360\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 425.81, NNZs: 43, Bias: -180.597243, T: 19200, Avg. loss: 90.124226\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 425.32, NNZs: 43, Bias: -180.875852, T: 19840, Avg. loss: 94.121194\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 425.28, NNZs: 43, Bias: -181.211908, T: 20480, Avg. loss: 74.070662\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 424.98, NNZs: 43, Bias: -181.519909, T: 21120, Avg. loss: 88.040955\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 424.33, NNZs: 43, Bias: -181.760852, T: 21760, Avg. loss: 88.798311\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 423.96, NNZs: 43, Bias: -181.985129, T: 22400, Avg. loss: 71.423057\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 423.61, NNZs: 43, Bias: -182.268404, T: 23040, Avg. loss: 91.583336\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 423.22, NNZs: 43, Bias: -182.560091, T: 23680, Avg. loss: 94.047549\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 422.93, NNZs: 43, Bias: -182.872630, T: 24320, Avg. loss: 87.089012\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 39\n",
      "Norm: 423.12, NNZs: 43, Bias: -183.264630, T: 24960, Avg. loss: 88.134167\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 40\n",
      "Norm: 422.89, NNZs: 43, Bias: -183.600543, T: 25600, Avg. loss: 88.165033\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 41\n",
      "Norm: 422.64, NNZs: 43, Bias: -183.617624, T: 26240, Avg. loss: 26.186087\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 42\n",
      "Norm: 422.40, NNZs: 43, Bias: -183.638624, T: 26880, Avg. loss: 24.667383\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 43\n",
      "Norm: 422.17, NNZs: 43, Bias: -183.666621, T: 27520, Avg. loss: 24.371340\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 44\n",
      "Norm: 421.95, NNZs: 43, Bias: -183.689125, T: 28160, Avg. loss: 23.619956\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 45\n",
      "Norm: 421.73, NNZs: 43, Bias: -183.705765, T: 28800, Avg. loss: 21.405945\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 46\n",
      "Norm: 421.50, NNZs: 43, Bias: -183.743528, T: 29440, Avg. loss: 25.147537\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 47\n",
      "Norm: 421.30, NNZs: 43, Bias: -183.774100, T: 30080, Avg. loss: 21.057759\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 48\n",
      "Norm: 421.09, NNZs: 43, Bias: -183.792285, T: 30720, Avg. loss: 21.454446\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 49\n",
      "Norm: 420.87, NNZs: 43, Bias: -183.814667, T: 31360, Avg. loss: 21.788534\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 50\n",
      "Norm: 420.65, NNZs: 43, Bias: -183.842888, T: 32000, Avg. loss: 20.679551\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 51\n",
      "Norm: 420.43, NNZs: 43, Bias: -183.867403, T: 32640, Avg. loss: 23.327992\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 52\n",
      "Norm: 420.22, NNZs: 43, Bias: -183.895195, T: 33280, Avg. loss: 19.964483\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 53\n",
      "Norm: 420.00, NNZs: 43, Bias: -183.923608, T: 33920, Avg. loss: 20.611253\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 54\n",
      "Norm: 419.80, NNZs: 43, Bias: -183.961227, T: 34560, Avg. loss: 19.802963\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 55\n",
      "Norm: 419.60, NNZs: 43, Bias: -183.991547, T: 35200, Avg. loss: 18.336177\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 56\n",
      "Norm: 419.40, NNZs: 43, Bias: -184.007499, T: 35840, Avg. loss: 19.985241\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 57\n",
      "Norm: 419.18, NNZs: 43, Bias: -184.031822, T: 36480, Avg. loss: 21.094302\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 58\n",
      "Norm: 418.97, NNZs: 43, Bias: -184.059963, T: 37120, Avg. loss: 21.005273\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 59\n",
      "Norm: 418.78, NNZs: 43, Bias: -184.093095, T: 37760, Avg. loss: 18.768088\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 60\n",
      "Norm: 418.57, NNZs: 43, Bias: -184.110562, T: 38400, Avg. loss: 19.417741\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 61\n",
      "Norm: 418.52, NNZs: 43, Bias: -184.110510, T: 39040, Avg. loss: 14.480284\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 62\n",
      "Norm: 418.47, NNZs: 43, Bias: -184.111416, T: 39680, Avg. loss: 13.647870\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 63\n",
      "Norm: 418.42, NNZs: 43, Bias: -184.113170, T: 40320, Avg. loss: 12.639025\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 64\n",
      "Norm: 418.37, NNZs: 43, Bias: -184.120992, T: 40960, Avg. loss: 13.137173\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 65\n",
      "Norm: 418.33, NNZs: 43, Bias: -184.124247, T: 41600, Avg. loss: 12.256202\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 66\n",
      "Norm: 418.28, NNZs: 43, Bias: -184.128024, T: 42240, Avg. loss: 13.050091\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 67\n",
      "Norm: 418.23, NNZs: 43, Bias: -184.131822, T: 42880, Avg. loss: 12.997502\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 68\n",
      "Norm: 418.18, NNZs: 43, Bias: -184.133979, T: 43520, Avg. loss: 12.953445\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 69\n",
      "Norm: 418.13, NNZs: 43, Bias: -184.138570, T: 44160, Avg. loss: 13.228375\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 70\n",
      "Norm: 418.08, NNZs: 43, Bias: -184.140848, T: 44800, Avg. loss: 12.377402\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 71\n",
      "Norm: 418.07, NNZs: 43, Bias: -184.141516, T: 45440, Avg. loss: 11.810176\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 72\n",
      "Norm: 418.06, NNZs: 43, Bias: -184.141306, T: 46080, Avg. loss: 11.918023\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 73\n",
      "Norm: 418.05, NNZs: 43, Bias: -184.141580, T: 46720, Avg. loss: 11.450876\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 74\n",
      "Norm: 418.04, NNZs: 43, Bias: -184.143711, T: 47360, Avg. loss: 11.407780\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 75\n",
      "Norm: 418.03, NNZs: 43, Bias: -184.143546, T: 48000, Avg. loss: 11.894215\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 76\n",
      "Norm: 418.02, NNZs: 43, Bias: -184.143980, T: 48640, Avg. loss: 11.725244\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 77\n",
      "Norm: 418.01, NNZs: 43, Bias: -184.143703, T: 49280, Avg. loss: 11.556577\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 78\n",
      "Norm: 418.00, NNZs: 43, Bias: -184.145300, T: 49920, Avg. loss: 11.730513\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 79\n",
      "Norm: 417.99, NNZs: 43, Bias: -184.145932, T: 50560, Avg. loss: 11.676610\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 80\n",
      "Norm: 417.99, NNZs: 43, Bias: -184.145726, T: 51200, Avg. loss: 11.475131\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 81\n",
      "Norm: 417.99, NNZs: 43, Bias: -184.145789, T: 51840, Avg. loss: 11.439857\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 82\n",
      "Norm: 417.98, NNZs: 43, Bias: -184.145914, T: 52480, Avg. loss: 11.411690\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 83\n",
      "Norm: 417.98, NNZs: 43, Bias: -184.145965, T: 53120, Avg. loss: 11.423851\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 84\n",
      "Norm: 417.98, NNZs: 43, Bias: -184.145923, T: 53760, Avg. loss: 11.393119\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 85\n",
      "Norm: 417.98, NNZs: 43, Bias: -184.146061, T: 54400, Avg. loss: 11.415182\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 86\n",
      "Norm: 417.98, NNZs: 43, Bias: -184.146157, T: 55040, Avg. loss: 11.418471\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 87\n",
      "Norm: 417.97, NNZs: 43, Bias: -184.146244, T: 55680, Avg. loss: 11.418508\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 88\n",
      "Norm: 417.97, NNZs: 43, Bias: -184.146475, T: 56320, Avg. loss: 11.398389\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 89\n",
      "Norm: 417.97, NNZs: 43, Bias: -184.146534, T: 56960, Avg. loss: 11.419060\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 90\n",
      "Norm: 417.97, NNZs: 43, Bias: -184.146543, T: 57600, Avg. loss: 11.332142\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 91\n",
      "Norm: 417.97, NNZs: 43, Bias: -184.146561, T: 58240, Avg. loss: 11.330088\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 92\n",
      "Norm: 417.97, NNZs: 43, Bias: -184.146578, T: 58880, Avg. loss: 11.329989\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 93\n",
      "Norm: 417.97, NNZs: 43, Bias: -184.146593, T: 59520, Avg. loss: 11.329965\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 94\n",
      "Norm: 417.97, NNZs: 43, Bias: -184.146611, T: 60160, Avg. loss: 11.328552\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 95\n",
      "Norm: 417.97, NNZs: 43, Bias: -184.146636, T: 60800, Avg. loss: 11.326597\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 96\n",
      "Norm: 417.97, NNZs: 43, Bias: -184.146654, T: 61440, Avg. loss: 11.329091\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 97\n",
      "Norm: 417.97, NNZs: 43, Bias: -184.146665, T: 62080, Avg. loss: 11.328504\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 98\n",
      "Norm: 417.97, NNZs: 43, Bias: -184.146686, T: 62720, Avg. loss: 11.327339\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 99\n",
      "Norm: 417.97, NNZs: 43, Bias: -184.146706, T: 63360, Avg. loss: 11.327023\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 100\n",
      "Norm: 417.97, NNZs: 43, Bias: -184.146721, T: 64000, Avg. loss: 11.327600\n",
      "Total training time: 0.06 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "clf_sgd = SGDClassifier(loss='log', max_iter=100, random_state=2, verbose=1, learning_rate ='adaptive', eta0 = 0.7)\n",
    "start = time()\n",
    "clf_sgd.fit(X_train, y_train)\n",
    "log_time = time()-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training time of the model: 0.06 .\n",
      "accuracy: 0.9\n",
      "f1: 0.5\n",
      "The mean training time of the model: 0.05750465393066406 (std: 0.00497832148495019) .\n",
      "The mean accuracy time of the model: 0.88541666666666663 (std: 0.01062295731998498) .\n",
      "The mean F1 score time of the model: 0.44533571004159239 (std: 0.03898770585163963) .\n"
     ]
    }
   ],
   "source": [
    "y_hat = clf_sgd.predict(X_test)\n",
    "print(\"The training time of the model: %.2f .\"\n",
    "      % (log_time))\n",
    "print('accuracy:', accuracy_score(y_test, y_hat))\n",
    "print('f1:', f1_score(y_test, y_hat))\n",
    "setting1_time3 = log_time\n",
    "setting1_acc3 = accuracy_score(y_test, y_hat)\n",
    "setting1_f13 = f1_score(y_test, y_hat)\n",
    "set1_time = np.array([setting1_time1, setting1_time2, setting1_time3])\n",
    "mean_time = np.mean(set1_time)\n",
    "std_time = np.std(set1_time)\n",
    "set1_acc = np.array([setting1_acc1, setting1_acc2, setting1_acc3])\n",
    "mean_acc = np.mean(set1_acc)\n",
    "std_acc = np.std(set1_acc)\n",
    "set1_f1 = np.array([setting1_f1, setting1_f12, setting1_f13])\n",
    "mean_f1 = np.mean(set1_f1)\n",
    "std_f1 = np.std(set1_f1)\n",
    "print(\"The mean training time of the model: %.17f (std: %.17f) .\"\n",
    "      % (mean_time, std_time))\n",
    "print(\"The mean accuracy time of the model: %.17f (std: %.17f) .\"\n",
    "      % (mean_acc, std_acc))\n",
    "print(\"The mean F1 score time of the model: %.17f (std: %.17f) .\"\n",
    "      % (mean_f1, std_f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**setting 2(not stable convergence):  Î·  = 0.5, adaptive learning rate (random_state 0, 1, 2 for each the three repetition)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 197.34, NNZs: 41, Bias: -10.750000, T: 640, Avg. loss: 2017.856022\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 283.86, NNZs: 42, Bias: -20.750000, T: 1280, Avg. loss: 1775.396118\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 271.12, NNZs: 42, Bias: -30.250000, T: 1920, Avg. loss: 2031.454361\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 224.12, NNZs: 42, Bias: -39.750000, T: 2560, Avg. loss: 2208.122617\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 258.72, NNZs: 42, Bias: -50.249938, T: 3200, Avg. loss: 1790.468525\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 405.50, NNZs: 42, Bias: -59.774004, T: 3840, Avg. loss: 1847.084664\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 337.85, NNZs: 42, Bias: -70.274004, T: 4480, Avg. loss: 2075.753100\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 259.53, NNZs: 42, Bias: -71.774004, T: 5120, Avg. loss: 702.994651\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 234.44, NNZs: 42, Bias: -73.473814, T: 5760, Avg. loss: 471.887251\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 232.53, NNZs: 43, Bias: -75.171984, T: 6400, Avg. loss: 400.483135\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 237.08, NNZs: 43, Bias: -76.933092, T: 7040, Avg. loss: 360.863726\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 240.85, NNZs: 43, Bias: -78.732712, T: 7680, Avg. loss: 387.562014\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 238.73, NNZs: 43, Bias: -80.633619, T: 8320, Avg. loss: 358.320066\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 244.16, NNZs: 43, Bias: -82.333619, T: 8960, Avg. loss: 395.593899\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 253.22, NNZs: 43, Bias: -84.533616, T: 9600, Avg. loss: 410.665198\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 253.92, NNZs: 43, Bias: -86.228855, T: 10240, Avg. loss: 392.738535\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 259.39, NNZs: 43, Bias: -88.128855, T: 10880, Avg. loss: 318.090583\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 262.34, NNZs: 43, Bias: -90.128855, T: 11520, Avg. loss: 343.351549\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 265.60, NNZs: 43, Bias: -92.090726, T: 12160, Avg. loss: 373.796266\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 271.17, NNZs: 43, Bias: -93.990729, T: 12800, Avg. loss: 420.356469\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 277.39, NNZs: 43, Bias: -95.890729, T: 13440, Avg. loss: 350.496517\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 280.11, NNZs: 43, Bias: -97.790726, T: 14080, Avg. loss: 346.753400\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 277.41, NNZs: 43, Bias: -97.990711, T: 14720, Avg. loss: 122.330928\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 276.47, NNZs: 43, Bias: -98.250622, T: 15360, Avg. loss: 80.774710\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 276.65, NNZs: 43, Bias: -98.570429, T: 16000, Avg. loss: 65.169057\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 276.44, NNZs: 43, Bias: -98.808457, T: 16640, Avg. loss: 70.308149\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 276.15, NNZs: 43, Bias: -98.988457, T: 17280, Avg. loss: 63.729704\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 276.04, NNZs: 43, Bias: -99.188448, T: 17920, Avg. loss: 59.686461\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 275.91, NNZs: 43, Bias: -99.424113, T: 18560, Avg. loss: 64.268574\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 275.86, NNZs: 43, Bias: -99.652970, T: 19200, Avg. loss: 61.537758\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 275.79, NNZs: 43, Bias: -99.873126, T: 19840, Avg. loss: 60.764912\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 275.89, NNZs: 43, Bias: -100.093144, T: 20480, Avg. loss: 69.438591\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 275.76, NNZs: 43, Bias: -100.272155, T: 21120, Avg. loss: 55.510533\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 275.73, NNZs: 43, Bias: -100.552141, T: 21760, Avg. loss: 61.667991\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 275.96, NNZs: 43, Bias: -100.806681, T: 22400, Avg. loss: 63.093331\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 275.84, NNZs: 43, Bias: -100.986681, T: 23040, Avg. loss: 65.538194\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 275.96, NNZs: 43, Bias: -101.246649, T: 23680, Avg. loss: 65.173885\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 275.93, NNZs: 43, Bias: -101.481723, T: 24320, Avg. loss: 64.952777\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 39\n",
      "Norm: 275.71, NNZs: 43, Bias: -101.489722, T: 24960, Avg. loss: 26.443870\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 40\n",
      "Norm: 275.54, NNZs: 43, Bias: -101.504786, T: 25600, Avg. loss: 20.927453\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 41\n",
      "Norm: 275.40, NNZs: 43, Bias: -101.523771, T: 26240, Avg. loss: 18.530258\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 42\n",
      "Norm: 275.27, NNZs: 43, Bias: -101.539041, T: 26880, Avg. loss: 16.049741\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 43\n",
      "Norm: 275.14, NNZs: 43, Bias: -101.561399, T: 27520, Avg. loss: 17.578957\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 44\n",
      "Norm: 275.01, NNZs: 43, Bias: -101.583044, T: 28160, Avg. loss: 17.194903\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 45\n",
      "Norm: 274.88, NNZs: 43, Bias: -101.608335, T: 28800, Avg. loss: 16.334106\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 46\n",
      "Norm: 274.75, NNZs: 43, Bias: -101.631245, T: 29440, Avg. loss: 18.310053\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 47\n",
      "Norm: 274.63, NNZs: 43, Bias: -101.658223, T: 30080, Avg. loss: 16.095142\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 48\n",
      "Norm: 274.60, NNZs: 43, Bias: -101.659365, T: 30720, Avg. loss: 11.423258\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 49\n",
      "Norm: 274.57, NNZs: 43, Bias: -101.662240, T: 31360, Avg. loss: 10.532908\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 50\n",
      "Norm: 274.54, NNZs: 43, Bias: -101.666402, T: 32000, Avg. loss: 11.140929\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 51\n",
      "Norm: 274.51, NNZs: 43, Bias: -101.665338, T: 32640, Avg. loss: 10.396705\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 52\n",
      "Norm: 274.48, NNZs: 43, Bias: -101.670075, T: 33280, Avg. loss: 10.434133\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 53\n",
      "Norm: 274.45, NNZs: 43, Bias: -101.674946, T: 33920, Avg. loss: 11.292615\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 54\n",
      "Norm: 274.41, NNZs: 43, Bias: -101.678323, T: 34560, Avg. loss: 11.317760\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 55\n",
      "Norm: 274.38, NNZs: 43, Bias: -101.679092, T: 35200, Avg. loss: 10.979658\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 56\n",
      "Norm: 274.35, NNZs: 43, Bias: -101.684581, T: 35840, Avg. loss: 11.154994\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 57\n",
      "Norm: 274.34, NNZs: 43, Bias: -101.684241, T: 36480, Avg. loss: 10.045778\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 58\n",
      "Norm: 274.34, NNZs: 43, Bias: -101.684802, T: 37120, Avg. loss: 9.983383\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 59\n",
      "Norm: 274.33, NNZs: 43, Bias: -101.685650, T: 37760, Avg. loss: 9.991056\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 60\n",
      "Norm: 274.32, NNZs: 43, Bias: -101.685823, T: 38400, Avg. loss: 9.941785\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 61\n",
      "Norm: 274.32, NNZs: 43, Bias: -101.686311, T: 39040, Avg. loss: 9.967267\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 62\n",
      "Norm: 274.31, NNZs: 43, Bias: -101.686122, T: 39680, Avg. loss: 9.847830\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 63\n",
      "Norm: 274.31, NNZs: 43, Bias: -101.687403, T: 40320, Avg. loss: 10.089262\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 64\n",
      "Norm: 274.30, NNZs: 43, Bias: -101.687626, T: 40960, Avg. loss: 9.887029\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 65\n",
      "Norm: 274.29, NNZs: 43, Bias: -101.688326, T: 41600, Avg. loss: 9.952055\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 66\n",
      "Norm: 274.29, NNZs: 43, Bias: -101.688411, T: 42240, Avg. loss: 9.875131\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 67\n",
      "Norm: 274.28, NNZs: 43, Bias: -101.689783, T: 42880, Avg. loss: 9.967241\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 68\n",
      "Norm: 274.28, NNZs: 43, Bias: -101.689621, T: 43520, Avg. loss: 9.723246\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 69\n",
      "Norm: 274.28, NNZs: 43, Bias: -101.689639, T: 44160, Avg. loss: 9.689074\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 70\n",
      "Norm: 274.27, NNZs: 43, Bias: -101.689690, T: 44800, Avg. loss: 9.675129\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 71\n",
      "Norm: 274.27, NNZs: 43, Bias: -101.689785, T: 45440, Avg. loss: 9.669542\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 72\n",
      "Norm: 274.27, NNZs: 43, Bias: -101.690015, T: 46080, Avg. loss: 9.637531\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 73\n",
      "Norm: 274.27, NNZs: 43, Bias: -101.690182, T: 46720, Avg. loss: 9.635826\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 74\n",
      "Norm: 274.27, NNZs: 43, Bias: -101.690318, T: 47360, Avg. loss: 9.651241\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 75\n",
      "Norm: 274.27, NNZs: 43, Bias: -101.690303, T: 48000, Avg. loss: 9.678688\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 76\n",
      "Norm: 274.27, NNZs: 43, Bias: -101.690421, T: 48640, Avg. loss: 9.663057\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 77\n",
      "Norm: 274.27, NNZs: 43, Bias: -101.690474, T: 49280, Avg. loss: 9.668648\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 78\n",
      "Norm: 274.26, NNZs: 43, Bias: -101.690525, T: 49920, Avg. loss: 9.649171\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 79\n",
      "Norm: 274.26, NNZs: 43, Bias: -101.690559, T: 50560, Avg. loss: 9.593290\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 80\n",
      "Norm: 274.26, NNZs: 43, Bias: -101.690589, T: 51200, Avg. loss: 9.593030\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 81\n",
      "Norm: 274.26, NNZs: 43, Bias: -101.690614, T: 51840, Avg. loss: 9.592751\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 82\n",
      "Norm: 274.26, NNZs: 43, Bias: -101.690639, T: 52480, Avg. loss: 9.592597\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 83\n",
      "Norm: 274.26, NNZs: 43, Bias: -101.690671, T: 53120, Avg. loss: 9.590003\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 84\n",
      "Norm: 274.26, NNZs: 43, Bias: -101.690694, T: 53760, Avg. loss: 9.591734\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 85\n",
      "Norm: 274.26, NNZs: 43, Bias: -101.690725, T: 54400, Avg. loss: 9.590551\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 86\n",
      "Norm: 274.26, NNZs: 43, Bias: -101.690739, T: 55040, Avg. loss: 9.590872\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 87\n",
      "Norm: 274.26, NNZs: 43, Bias: -101.690759, T: 55680, Avg. loss: 9.591819\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 88\n",
      "Norm: 274.26, NNZs: 43, Bias: -101.690789, T: 56320, Avg. loss: 9.590986\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 89\n",
      "Norm: 274.26, NNZs: 43, Bias: -101.690794, T: 56960, Avg. loss: 9.577720\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 90\n",
      "Norm: 274.26, NNZs: 43, Bias: -101.690798, T: 57600, Avg. loss: 9.577750\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 91\n",
      "Norm: 274.26, NNZs: 43, Bias: -101.690803, T: 58240, Avg. loss: 9.577721\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 92\n",
      "Norm: 274.26, NNZs: 43, Bias: -101.690807, T: 58880, Avg. loss: 9.577652\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 93\n",
      "Norm: 274.26, NNZs: 43, Bias: -101.690812, T: 59520, Avg. loss: 9.577653\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 94\n",
      "Norm: 274.26, NNZs: 43, Bias: -101.690816, T: 60160, Avg. loss: 9.577617\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 95\n",
      "Norm: 274.26, NNZs: 43, Bias: -101.690817, T: 60800, Avg. loss: 9.574804\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 96\n",
      "Norm: 274.26, NNZs: 43, Bias: -101.690818, T: 61440, Avg. loss: 9.574799\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 97\n",
      "Norm: 274.26, NNZs: 43, Bias: -101.690819, T: 62080, Avg. loss: 9.574793\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 98\n",
      "Norm: 274.26, NNZs: 43, Bias: -101.690820, T: 62720, Avg. loss: 9.574787\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 99\n",
      "Norm: 274.26, NNZs: 43, Bias: -101.690821, T: 63360, Avg. loss: 9.574783\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 100\n",
      "Norm: 274.26, NNZs: 43, Bias: -101.690822, T: 64000, Avg. loss: 9.574776\n",
      "Total training time: 0.07 seconds.\n",
      "Convergence after 100 epochs took 0.07 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "clf_sgd_2 = SGDClassifier(loss='log', max_iter=100, random_state=0, verbose=1, learning_rate ='adaptive', eta0 = 0.5)\n",
    "start = time()\n",
    "clf_sgd_2.fit(X_train, y_train)\n",
    "log_time = time()-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training time of the model: 0.07 .\n",
      "accuracy: 0.8875\n",
      "f1: 0.43749999999999994\n"
     ]
    }
   ],
   "source": [
    "y_hat = clf_sgd_2.predict(X_test)\n",
    "print(\"The training time of the model: %.2f .\"\n",
    "      % (log_time))\n",
    "print('accuracy:', accuracy_score(y_test, y_hat))\n",
    "print('f1:', f1_score(y_test, y_hat))\n",
    "setting2_time1 = log_time\n",
    "setting2_acc1 = accuracy_score(y_test, y_hat)\n",
    "setting2_f1 = f1_score(y_test, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 252.02, NNZs: 40, Bias: -12.250000, T: 640, Avg. loss: 1971.366275\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 245.65, NNZs: 41, Bias: -22.750000, T: 1280, Avg. loss: 1872.313614\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 293.74, NNZs: 41, Bias: -31.750000, T: 1920, Avg. loss: 1838.710446\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 216.66, NNZs: 42, Bias: -41.250000, T: 2560, Avg. loss: 1728.955743\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 307.68, NNZs: 42, Bias: -51.750000, T: 3200, Avg. loss: 2108.917122\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 323.71, NNZs: 42, Bias: -60.250000, T: 3840, Avg. loss: 1802.609722\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 391.47, NNZs: 42, Bias: -69.749822, T: 4480, Avg. loss: 1968.683052\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 402.30, NNZs: 42, Bias: -81.249822, T: 5120, Avg. loss: 1930.829507\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 356.74, NNZs: 43, Bias: -91.249822, T: 5760, Avg. loss: 1936.423539\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 310.45, NNZs: 43, Bias: -92.449814, T: 6400, Avg. loss: 588.983607\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 298.95, NNZs: 43, Bias: -94.139298, T: 7040, Avg. loss: 415.459669\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 291.68, NNZs: 43, Bias: -95.632099, T: 7680, Avg. loss: 380.313027\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 303.22, NNZs: 43, Bias: -97.660083, T: 8320, Avg. loss: 351.038439\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 301.54, NNZs: 43, Bias: -99.260076, T: 8960, Avg. loss: 351.724266\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 302.01, NNZs: 43, Bias: -100.856702, T: 9600, Avg. loss: 345.661807\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 303.16, NNZs: 43, Bias: -102.548190, T: 10240, Avg. loss: 365.071629\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 305.69, NNZs: 43, Bias: -103.948190, T: 10880, Avg. loss: 395.237134\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 310.36, NNZs: 43, Bias: -105.949278, T: 11520, Avg. loss: 357.618457\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 312.43, NNZs: 43, Bias: -107.405034, T: 12160, Avg. loss: 306.525593\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 314.54, NNZs: 43, Bias: -108.705034, T: 12800, Avg. loss: 382.545213\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 316.61, NNZs: 43, Bias: -110.475656, T: 13440, Avg. loss: 383.806077\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 319.40, NNZs: 43, Bias: -112.574757, T: 14080, Avg. loss: 377.739299\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 326.57, NNZs: 43, Bias: -114.674758, T: 14720, Avg. loss: 385.868613\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 330.20, NNZs: 43, Bias: -116.576873, T: 15360, Avg. loss: 382.103297\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 327.29, NNZs: 43, Bias: -116.796872, T: 16000, Avg. loss: 133.948667\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 326.20, NNZs: 43, Bias: -116.976872, T: 16640, Avg. loss: 87.396733\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 325.43, NNZs: 43, Bias: -117.153319, T: 17280, Avg. loss: 75.897167\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 325.14, NNZs: 43, Bias: -117.352319, T: 17920, Avg. loss: 56.848080\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 324.76, NNZs: 43, Bias: -117.559831, T: 18560, Avg. loss: 58.852623\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 324.45, NNZs: 43, Bias: -117.837227, T: 19200, Avg. loss: 74.530398\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 324.22, NNZs: 43, Bias: -118.039194, T: 19840, Avg. loss: 62.558809\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 324.06, NNZs: 43, Bias: -118.259194, T: 20480, Avg. loss: 61.014629\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 323.78, NNZs: 43, Bias: -118.455432, T: 21120, Avg. loss: 57.617470\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 323.57, NNZs: 43, Bias: -118.475582, T: 21760, Avg. loss: 26.704436\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 323.39, NNZs: 43, Bias: -118.497681, T: 22400, Avg. loss: 23.948887\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 323.22, NNZs: 43, Bias: -118.521410, T: 23040, Avg. loss: 23.886356\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 323.04, NNZs: 43, Bias: -118.549696, T: 23680, Avg. loss: 23.767475\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 322.87, NNZs: 43, Bias: -118.577787, T: 24320, Avg. loss: 23.326563\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 39\n",
      "Norm: 322.69, NNZs: 43, Bias: -118.593899, T: 24960, Avg. loss: 23.452133\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 40\n",
      "Norm: 322.52, NNZs: 43, Bias: -118.622854, T: 25600, Avg. loss: 21.225892\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 41\n",
      "Norm: 322.35, NNZs: 43, Bias: -118.645405, T: 26240, Avg. loss: 22.645991\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 42\n",
      "Norm: 322.20, NNZs: 43, Bias: -118.661398, T: 26880, Avg. loss: 20.676438\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 43\n",
      "Norm: 322.04, NNZs: 43, Bias: -118.685354, T: 27520, Avg. loss: 23.043498\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 44\n",
      "Norm: 321.87, NNZs: 43, Bias: -118.709045, T: 28160, Avg. loss: 22.955431\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 45\n",
      "Norm: 321.72, NNZs: 43, Bias: -118.731027, T: 28800, Avg. loss: 19.389271\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 46\n",
      "Norm: 321.56, NNZs: 43, Bias: -118.754724, T: 29440, Avg. loss: 21.775572\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 47\n",
      "Norm: 321.39, NNZs: 43, Bias: -118.785981, T: 30080, Avg. loss: 20.369354\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 48\n",
      "Norm: 321.23, NNZs: 43, Bias: -118.804575, T: 30720, Avg. loss: 20.545981\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 49\n",
      "Norm: 321.06, NNZs: 43, Bias: -118.831471, T: 31360, Avg. loss: 23.610356\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 50\n",
      "Norm: 320.90, NNZs: 43, Bias: -118.851616, T: 32000, Avg. loss: 21.692561\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 51\n",
      "Norm: 320.86, NNZs: 43, Bias: -118.855043, T: 32640, Avg. loss: 15.169277\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 52\n",
      "Norm: 320.82, NNZs: 43, Bias: -118.860026, T: 33280, Avg. loss: 15.785156\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 53\n",
      "Norm: 320.78, NNZs: 43, Bias: -118.864814, T: 33920, Avg. loss: 15.565183\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 54\n",
      "Norm: 320.75, NNZs: 43, Bias: -118.865929, T: 34560, Avg. loss: 15.572455\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 55\n",
      "Norm: 320.71, NNZs: 43, Bias: -118.869406, T: 35200, Avg. loss: 15.541910\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 56\n",
      "Norm: 320.67, NNZs: 43, Bias: -118.874406, T: 35840, Avg. loss: 15.912494\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 57\n",
      "Norm: 320.66, NNZs: 43, Bias: -118.874789, T: 36480, Avg. loss: 14.373291\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 58\n",
      "Norm: 320.65, NNZs: 43, Bias: -118.875133, T: 37120, Avg. loss: 14.427994\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 59\n",
      "Norm: 320.64, NNZs: 43, Bias: -118.876220, T: 37760, Avg. loss: 14.218163\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 60\n",
      "Norm: 320.64, NNZs: 43, Bias: -118.877172, T: 38400, Avg. loss: 14.166235\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 61\n",
      "Norm: 320.63, NNZs: 43, Bias: -118.877458, T: 39040, Avg. loss: 14.438082\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 62\n",
      "Norm: 320.62, NNZs: 43, Bias: -118.878225, T: 39680, Avg. loss: 14.408897\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 63\n",
      "Norm: 320.61, NNZs: 43, Bias: -118.879163, T: 40320, Avg. loss: 14.358546\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 64\n",
      "Norm: 320.60, NNZs: 43, Bias: -118.879697, T: 40960, Avg. loss: 14.406039\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 65\n",
      "Norm: 320.60, NNZs: 43, Bias: -118.880449, T: 41600, Avg. loss: 14.266855\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 66\n",
      "Norm: 320.59, NNZs: 43, Bias: -118.880503, T: 42240, Avg. loss: 14.078907\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 67\n",
      "Norm: 320.59, NNZs: 43, Bias: -118.880538, T: 42880, Avg. loss: 14.063820\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 68\n",
      "Norm: 320.59, NNZs: 43, Bias: -118.880656, T: 43520, Avg. loss: 14.054525\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 69\n",
      "Norm: 320.59, NNZs: 43, Bias: -118.880763, T: 44160, Avg. loss: 14.054574\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 70\n",
      "Norm: 320.59, NNZs: 43, Bias: -118.880805, T: 44800, Avg. loss: 14.047748\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 71\n",
      "Norm: 320.59, NNZs: 43, Bias: -118.880901, T: 45440, Avg. loss: 14.028847\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 72\n",
      "Norm: 320.58, NNZs: 43, Bias: -118.881039, T: 46080, Avg. loss: 14.050928\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 73\n",
      "Norm: 320.58, NNZs: 43, Bias: -118.881179, T: 46720, Avg. loss: 14.048628\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 74\n",
      "Norm: 320.58, NNZs: 43, Bias: -118.881275, T: 47360, Avg. loss: 14.042911\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 75\n",
      "Norm: 320.58, NNZs: 43, Bias: -118.881452, T: 48000, Avg. loss: 14.039525\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 76\n",
      "Norm: 320.58, NNZs: 43, Bias: -118.881569, T: 48640, Avg. loss: 14.034541\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 77\n",
      "Norm: 320.58, NNZs: 43, Bias: -118.881587, T: 49280, Avg. loss: 13.968482\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 78\n",
      "Norm: 320.58, NNZs: 43, Bias: -118.881605, T: 49920, Avg. loss: 13.967949\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 79\n",
      "Norm: 320.58, NNZs: 43, Bias: -118.881628, T: 50560, Avg. loss: 13.967519\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 80\n",
      "Norm: 320.58, NNZs: 43, Bias: -118.881649, T: 51200, Avg. loss: 13.967353\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 81\n",
      "Norm: 320.58, NNZs: 43, Bias: -118.881670, T: 51840, Avg. loss: 13.966743\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 82\n",
      "Norm: 320.58, NNZs: 43, Bias: -118.881690, T: 52480, Avg. loss: 13.966427\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 83\n",
      "Norm: 320.58, NNZs: 43, Bias: -118.881695, T: 53120, Avg. loss: 13.951578\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 84\n",
      "Norm: 320.58, NNZs: 43, Bias: -118.881699, T: 53760, Avg. loss: 13.951554\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 85\n",
      "Norm: 320.58, NNZs: 43, Bias: -118.881703, T: 54400, Avg. loss: 13.951462\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 86\n",
      "Norm: 320.58, NNZs: 43, Bias: -118.881708, T: 55040, Avg. loss: 13.951432\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 87\n",
      "Norm: 320.58, NNZs: 43, Bias: -118.881712, T: 55680, Avg. loss: 13.951350\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 88\n",
      "Norm: 320.58, NNZs: 43, Bias: -118.881717, T: 56320, Avg. loss: 13.951290\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 89\n",
      "Norm: 320.58, NNZs: 43, Bias: -118.881718, T: 56960, Avg. loss: 13.948283\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 90\n",
      "Norm: 320.58, NNZs: 43, Bias: -118.881719, T: 57600, Avg. loss: 13.948270\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 91\n",
      "Norm: 320.58, NNZs: 43, Bias: -118.881720, T: 58240, Avg. loss: 13.948258\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 92\n",
      "Norm: 320.58, NNZs: 43, Bias: -118.881720, T: 58880, Avg. loss: 13.948245\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 93\n",
      "Norm: 320.58, NNZs: 43, Bias: -118.881721, T: 59520, Avg. loss: 13.948235\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 94\n",
      "Norm: 320.58, NNZs: 43, Bias: -118.881722, T: 60160, Avg. loss: 13.948223\n",
      "Total training time: 0.06 seconds.\n",
      "Convergence after 94 epochs took 0.06 seconds\n"
     ]
    }
   ],
   "source": [
    "clf_sgd_2 = SGDClassifier(loss='log', max_iter=100, random_state=1, verbose=1, learning_rate ='adaptive', eta0 = 0.5)\n",
    "start = time()\n",
    "clf_sgd_2.fit(X_train, y_train)\n",
    "log_time = time()-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training time of the model: 0.07 .\n",
      "accuracy: 0.86875\n",
      "f1: 0.39999999999999997\n"
     ]
    }
   ],
   "source": [
    "y_hat = clf_sgd_2.predict(X_test)\n",
    "print(\"The training time of the model: %.2f .\"\n",
    "      % (log_time))\n",
    "print('accuracy:', accuracy_score(y_test, y_hat))\n",
    "print('f1:', f1_score(y_test, y_hat))\n",
    "setting2_time2 = log_time\n",
    "setting2_acc2 = accuracy_score(y_test, y_hat)\n",
    "setting2_f12 = f1_score(y_test, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 253.64, NNZs: 40, Bias: -11.250000, T: 640, Avg. loss: 1869.658693\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 347.24, NNZs: 41, Bias: -22.250000, T: 1280, Avg. loss: 1968.482560\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 337.05, NNZs: 41, Bias: -32.750000, T: 1920, Avg. loss: 1992.683296\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 269.44, NNZs: 41, Bias: -43.250000, T: 2560, Avg. loss: 2202.215062\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 413.12, NNZs: 42, Bias: -52.250001, T: 3200, Avg. loss: 1848.165968\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 396.64, NNZs: 43, Bias: -62.249560, T: 3840, Avg. loss: 2017.654898\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 330.88, NNZs: 43, Bias: -72.249560, T: 4480, Avg. loss: 2011.989096\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 382.22, NNZs: 43, Bias: -80.749560, T: 5120, Avg. loss: 1750.017162\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 363.89, NNZs: 43, Bias: -90.246665, T: 5760, Avg. loss: 1931.355303\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 403.37, NNZs: 43, Bias: -99.746665, T: 6400, Avg. loss: 2099.906354\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 437.83, NNZs: 43, Bias: -108.246665, T: 7040, Avg. loss: 1846.461609\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 377.16, NNZs: 43, Bias: -118.746665, T: 7680, Avg. loss: 2113.328318\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 406.20, NNZs: 43, Bias: -129.246665, T: 8320, Avg. loss: 1958.945491\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 366.00, NNZs: 43, Bias: -130.346665, T: 8960, Avg. loss: 606.965367\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 361.81, NNZs: 43, Bias: -132.046665, T: 9600, Avg. loss: 387.783193\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 360.24, NNZs: 43, Bias: -133.646665, T: 10240, Avg. loss: 347.451167\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 360.50, NNZs: 43, Bias: -135.443666, T: 10880, Avg. loss: 414.812077\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 366.42, NNZs: 43, Bias: -137.403531, T: 11520, Avg. loss: 318.071943\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 366.20, NNZs: 43, Bias: -138.903531, T: 12160, Avg. loss: 343.015057\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 370.43, NNZs: 43, Bias: -140.714129, T: 12800, Avg. loss: 327.835406\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 371.46, NNZs: 43, Bias: -141.814129, T: 13440, Avg. loss: 333.920066\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 373.35, NNZs: 43, Bias: -143.376167, T: 14080, Avg. loss: 348.434150\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 376.81, NNZs: 43, Bias: -145.076167, T: 14720, Avg. loss: 357.179531\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 373.76, NNZs: 43, Bias: -145.317339, T: 15360, Avg. loss: 144.661419\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 372.65, NNZs: 43, Bias: -145.477380, T: 16000, Avg. loss: 82.585177\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 372.13, NNZs: 43, Bias: -145.637202, T: 16640, Avg. loss: 53.532310\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 371.74, NNZs: 43, Bias: -145.834655, T: 17280, Avg. loss: 71.327110\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 371.46, NNZs: 43, Bias: -146.035562, T: 17920, Avg. loss: 53.689724\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 371.01, NNZs: 43, Bias: -146.199691, T: 18560, Avg. loss: 66.072233\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 370.71, NNZs: 43, Bias: -146.439833, T: 19200, Avg. loss: 65.905337\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 370.41, NNZs: 43, Bias: -146.636355, T: 19840, Avg. loss: 64.355840\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 370.25, NNZs: 43, Bias: -146.659145, T: 20480, Avg. loss: 21.269341\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 370.09, NNZs: 43, Bias: -146.687785, T: 21120, Avg. loss: 20.450873\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 369.91, NNZs: 43, Bias: -146.698993, T: 21760, Avg. loss: 22.740697\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 369.76, NNZs: 43, Bias: -146.714283, T: 22400, Avg. loss: 19.991567\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 369.59, NNZs: 43, Bias: -146.726763, T: 23040, Avg. loss: 21.026569\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 369.42, NNZs: 43, Bias: -146.750352, T: 23680, Avg. loss: 21.125934\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 369.27, NNZs: 43, Bias: -146.780689, T: 24320, Avg. loss: 20.144965\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 39\n",
      "Norm: 369.11, NNZs: 43, Bias: -146.804043, T: 24960, Avg. loss: 19.309358\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 40\n",
      "Norm: 368.96, NNZs: 43, Bias: -146.827747, T: 25600, Avg. loss: 20.089355\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 41\n",
      "Norm: 368.80, NNZs: 43, Bias: -146.853443, T: 26240, Avg. loss: 20.009606\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 42\n",
      "Norm: 368.64, NNZs: 43, Bias: -146.873811, T: 26880, Avg. loss: 21.250710\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 43\n",
      "Norm: 368.48, NNZs: 43, Bias: -146.893798, T: 27520, Avg. loss: 19.573946\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 44\n",
      "Norm: 368.31, NNZs: 43, Bias: -146.913452, T: 28160, Avg. loss: 19.744660\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 45\n",
      "Norm: 368.28, NNZs: 43, Bias: -146.916530, T: 28800, Avg. loss: 14.445777\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 46\n",
      "Norm: 368.24, NNZs: 43, Bias: -146.917062, T: 29440, Avg. loss: 14.126833\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 47\n",
      "Norm: 368.20, NNZs: 43, Bias: -146.922088, T: 30080, Avg. loss: 14.299824\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 48\n",
      "Norm: 368.17, NNZs: 43, Bias: -146.925114, T: 30720, Avg. loss: 13.727546\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 49\n",
      "Norm: 368.13, NNZs: 43, Bias: -146.928561, T: 31360, Avg. loss: 14.388444\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 50\n",
      "Norm: 368.09, NNZs: 43, Bias: -146.932129, T: 32000, Avg. loss: 14.416755\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 51\n",
      "Norm: 368.06, NNZs: 43, Bias: -146.934788, T: 32640, Avg. loss: 14.460151\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 52\n",
      "Norm: 368.02, NNZs: 43, Bias: -146.937244, T: 33280, Avg. loss: 13.553375\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 53\n",
      "Norm: 367.98, NNZs: 43, Bias: -146.941259, T: 33920, Avg. loss: 13.770285\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 54\n",
      "Norm: 367.95, NNZs: 43, Bias: -146.944609, T: 34560, Avg. loss: 13.334524\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 55\n",
      "Norm: 367.91, NNZs: 43, Bias: -146.947336, T: 35200, Avg. loss: 13.787130\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 56\n",
      "Norm: 367.88, NNZs: 43, Bias: -146.949216, T: 35840, Avg. loss: 14.037672\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 57\n",
      "Norm: 367.84, NNZs: 43, Bias: -146.952494, T: 36480, Avg. loss: 13.707786\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 58\n",
      "Norm: 367.80, NNZs: 43, Bias: -146.955823, T: 37120, Avg. loss: 13.844950\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 59\n",
      "Norm: 367.77, NNZs: 43, Bias: -146.958010, T: 37760, Avg. loss: 13.716983\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 60\n",
      "Norm: 367.76, NNZs: 43, Bias: -146.957804, T: 38400, Avg. loss: 12.356200\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 61\n",
      "Norm: 367.75, NNZs: 43, Bias: -146.958754, T: 39040, Avg. loss: 12.834871\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 62\n",
      "Norm: 367.75, NNZs: 43, Bias: -146.959058, T: 39680, Avg. loss: 12.716388\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 63\n",
      "Norm: 367.74, NNZs: 43, Bias: -146.959471, T: 40320, Avg. loss: 12.774392\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 64\n",
      "Norm: 367.73, NNZs: 43, Bias: -146.960902, T: 40960, Avg. loss: 12.642543\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 65\n",
      "Norm: 367.72, NNZs: 43, Bias: -146.960706, T: 41600, Avg. loss: 12.650759\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 66\n",
      "Norm: 367.72, NNZs: 43, Bias: -146.960918, T: 42240, Avg. loss: 12.468950\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 67\n",
      "Norm: 367.72, NNZs: 43, Bias: -146.961109, T: 42880, Avg. loss: 12.449299\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 68\n",
      "Norm: 367.72, NNZs: 43, Bias: -146.961288, T: 43520, Avg. loss: 12.432735\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 69\n",
      "Norm: 367.72, NNZs: 43, Bias: -146.961411, T: 44160, Avg. loss: 12.450873\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 367.72, NNZs: 43, Bias: -146.961509, T: 44800, Avg. loss: 12.452070\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 71\n",
      "Norm: 367.72, NNZs: 43, Bias: -146.961521, T: 45440, Avg. loss: 12.388822\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 72\n",
      "Norm: 367.72, NNZs: 43, Bias: -146.961535, T: 46080, Avg. loss: 12.388105\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 73\n",
      "Norm: 367.72, NNZs: 43, Bias: -146.961545, T: 46720, Avg. loss: 12.385210\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 74\n",
      "Norm: 367.72, NNZs: 43, Bias: -146.961563, T: 47360, Avg. loss: 12.386309\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 75\n",
      "Norm: 367.72, NNZs: 43, Bias: -146.961579, T: 48000, Avg. loss: 12.386430\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 76\n",
      "Norm: 367.72, NNZs: 43, Bias: -146.961582, T: 48640, Avg. loss: 12.372281\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 77\n",
      "Norm: 367.72, NNZs: 43, Bias: -146.961585, T: 49280, Avg. loss: 12.372185\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 78\n",
      "Norm: 367.72, NNZs: 43, Bias: -146.961589, T: 49920, Avg. loss: 12.372136\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 79\n",
      "Norm: 367.72, NNZs: 43, Bias: -146.961592, T: 50560, Avg. loss: 12.372059\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 80\n",
      "Norm: 367.72, NNZs: 43, Bias: -146.961595, T: 51200, Avg. loss: 12.372000\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 81\n",
      "Norm: 367.72, NNZs: 43, Bias: -146.961596, T: 51840, Avg. loss: 12.369175\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 82\n",
      "Norm: 367.72, NNZs: 43, Bias: -146.961597, T: 52480, Avg. loss: 12.369160\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 83\n",
      "Norm: 367.71, NNZs: 43, Bias: -146.961597, T: 53120, Avg. loss: 12.369146\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 84\n",
      "Norm: 367.71, NNZs: 43, Bias: -146.961598, T: 53760, Avg. loss: 12.369133\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 85\n",
      "Norm: 367.71, NNZs: 43, Bias: -146.961599, T: 54400, Avg. loss: 12.369119\n",
      "Total training time: 0.06 seconds.\n",
      "Convergence after 85 epochs took 0.06 seconds\n"
     ]
    }
   ],
   "source": [
    "clf_sgd_2 = SGDClassifier(loss='log', max_iter=100, random_state=2, verbose=1, learning_rate ='adaptive', eta0 = 0.5)\n",
    "start = time()\n",
    "clf_sgd_2.fit(X_train, y_train)\n",
    "log_time = time()-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training time of the model: 0.06 .\n",
      "accuracy: 0.8875\n",
      "f1: 0.43749999999999994\n",
      "The mean training time of the model: 0.06582164764404297 (std: 0.00533686147867773) .\n",
      "The mean accuracy time of the model: 0.88124999999999998 (std: 0.00883883476483181) .\n",
      "The mean F1 score time of the model: 0.42499999999999999 (std: 0.01767766952966368) .\n"
     ]
    }
   ],
   "source": [
    "y_hat = clf_sgd_2.predict(X_test)\n",
    "print(\"The training time of the model: %.2f .\"\n",
    "      % (log_time))\n",
    "print('accuracy:', accuracy_score(y_test, y_hat))\n",
    "print('f1:', f1_score(y_test, y_hat))\n",
    "setting2_time3 = log_time\n",
    "setting2_acc3 = accuracy_score(y_test, y_hat)\n",
    "setting2_f13 = f1_score(y_test, y_hat)\n",
    "set2_time = np.array([setting2_time1, setting2_time2, setting2_time3])\n",
    "mean_time = np.mean(set2_time)\n",
    "std_time = np.std(set2_time)\n",
    "set2_acc = np.array([setting2_acc1, setting2_acc2, setting2_acc3])\n",
    "mean_acc = np.mean(set2_acc)\n",
    "std_acc = np.std(set2_acc)\n",
    "set2_f1 = np.array([setting2_f1, setting2_f12, setting2_f13])\n",
    "mean_f1 = np.mean(set2_f1)\n",
    "std_f1 = np.std(set2_f1)\n",
    "print(\"The mean training time of the model: %.17f (std: %.17f) .\"\n",
    "      % (mean_time, std_time))\n",
    "print(\"The mean accuracy time of the model: %.17f (std: %.17f) .\"\n",
    "      % (mean_acc, std_acc))\n",
    "print(\"The mean F1 score time of the model: %.17f (std: %.17f) .\"\n",
    "      % (mean_f1, std_f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**setting 3(stable convergence) :  Î·  = 0.1, adaptive learning rate (random_state 0, 1, 2 for each the three repetition)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 40.75, NNZs: 42, Bias: -2.250000, T: 640, Avg. loss: 403.875308\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 45.17, NNZs: 42, Bias: -4.244676, T: 1280, Avg. loss: 363.079223\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 44.14, NNZs: 42, Bias: -6.154921, T: 1920, Avg. loss: 391.175144\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 35.39, NNZs: 42, Bias: -7.884341, T: 2560, Avg. loss: 423.600670\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 54.08, NNZs: 42, Bias: -10.382385, T: 3200, Avg. loss: 397.522231\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 78.60, NNZs: 43, Bias: -11.991451, T: 3840, Avg. loss: 381.560451\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 63.50, NNZs: 43, Bias: -13.891451, T: 4480, Avg. loss: 434.708349\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 48.99, NNZs: 43, Bias: -14.107418, T: 5120, Avg. loss: 130.045685\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 47.71, NNZs: 43, Bias: -14.450079, T: 5760, Avg. loss: 79.504766\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 48.70, NNZs: 43, Bias: -14.789952, T: 6400, Avg. loss: 74.750202\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 49.36, NNZs: 43, Bias: -15.170342, T: 7040, Avg. loss: 78.598806\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 49.49, NNZs: 43, Bias: -15.471401, T: 7680, Avg. loss: 73.444502\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 51.27, NNZs: 43, Bias: -15.938346, T: 8320, Avg. loss: 68.783257\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 52.04, NNZs: 43, Bias: -16.254405, T: 8960, Avg. loss: 80.996617\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 53.41, NNZs: 43, Bias: -16.674520, T: 9600, Avg. loss: 82.507444\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 54.29, NNZs: 43, Bias: -17.015933, T: 10240, Avg. loss: 74.238530\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 55.18, NNZs: 43, Bias: -17.439095, T: 10880, Avg. loss: 65.428448\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 56.54, NNZs: 43, Bias: -17.808409, T: 11520, Avg. loss: 70.677179\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 57.55, NNZs: 43, Bias: -18.208014, T: 12160, Avg. loss: 76.643992\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 58.37, NNZs: 43, Bias: -18.568057, T: 12800, Avg. loss: 77.621501\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 59.69, NNZs: 43, Bias: -18.970297, T: 13440, Avg. loss: 78.040991\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 60.29, NNZs: 43, Bias: -19.286606, T: 14080, Avg. loss: 68.550996\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 59.98, NNZs: 43, Bias: -19.328837, T: 14720, Avg. loss: 20.860196\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 59.97, NNZs: 43, Bias: -19.383279, T: 15360, Avg. loss: 14.235978\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 60.03, NNZs: 43, Bias: -19.431659, T: 16000, Avg. loss: 10.561529\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 60.05, NNZs: 43, Bias: -19.469648, T: 16640, Avg. loss: 11.981005\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 60.06, NNZs: 43, Bias: -19.499510, T: 17280, Avg. loss: 12.378990\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 60.11, NNZs: 43, Bias: -19.531894, T: 17920, Avg. loss: 11.377413\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 60.16, NNZs: 43, Bias: -19.572294, T: 18560, Avg. loss: 11.719169\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 60.20, NNZs: 43, Bias: -19.612203, T: 19200, Avg. loss: 9.806870\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 60.27, NNZs: 43, Bias: -19.658485, T: 19840, Avg. loss: 12.334180\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 60.35, NNZs: 43, Bias: -19.699548, T: 20480, Avg. loss: 11.409113\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 60.43, NNZs: 43, Bias: -19.745583, T: 21120, Avg. loss: 9.209725\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 60.50, NNZs: 43, Bias: -19.791708, T: 21760, Avg. loss: 9.693366\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 60.58, NNZs: 43, Bias: -19.834733, T: 22400, Avg. loss: 10.682035\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 60.65, NNZs: 43, Bias: -19.876063, T: 23040, Avg. loss: 11.964425\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 60.72, NNZs: 43, Bias: -19.920391, T: 23680, Avg. loss: 10.966976\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 60.81, NNZs: 43, Bias: -19.967214, T: 24320, Avg. loss: 11.718641\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 39\n",
      "Norm: 60.78, NNZs: 43, Bias: -19.970129, T: 24960, Avg. loss: 4.194410\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 40\n",
      "Norm: 60.77, NNZs: 43, Bias: -19.972959, T: 25600, Avg. loss: 3.700526\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 41\n",
      "Norm: 60.75, NNZs: 43, Bias: -19.975584, T: 26240, Avg. loss: 3.143523\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 42\n",
      "Norm: 60.74, NNZs: 43, Bias: -19.979116, T: 26880, Avg. loss: 2.607992\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 43\n",
      "Norm: 60.73, NNZs: 43, Bias: -19.982673, T: 27520, Avg. loss: 3.024355\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 44\n",
      "Norm: 60.72, NNZs: 43, Bias: -19.986529, T: 28160, Avg. loss: 3.035550\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 45\n",
      "Norm: 60.71, NNZs: 43, Bias: -19.989802, T: 28800, Avg. loss: 2.775921\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 46\n",
      "Norm: 60.70, NNZs: 43, Bias: -19.994698, T: 29440, Avg. loss: 3.008934\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 47\n",
      "Norm: 60.69, NNZs: 43, Bias: -19.998915, T: 30080, Avg. loss: 2.641341\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 48\n",
      "Norm: 60.68, NNZs: 43, Bias: -19.999084, T: 30720, Avg. loss: 1.937046\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 49\n",
      "Norm: 60.68, NNZs: 43, Bias: -19.999660, T: 31360, Avg. loss: 1.738896\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 50\n",
      "Norm: 60.68, NNZs: 43, Bias: -20.000500, T: 32000, Avg. loss: 1.852310\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 51\n",
      "Norm: 60.67, NNZs: 43, Bias: -20.000428, T: 32640, Avg. loss: 1.684832\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 52\n",
      "Norm: 60.67, NNZs: 43, Bias: -20.001121, T: 33280, Avg. loss: 1.752734\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 53\n",
      "Norm: 60.67, NNZs: 43, Bias: -20.001855, T: 33920, Avg. loss: 1.838668\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 54\n",
      "Norm: 60.66, NNZs: 43, Bias: -20.002373, T: 34560, Avg. loss: 1.826792\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 55\n",
      "Norm: 60.66, NNZs: 43, Bias: -20.002496, T: 35200, Avg. loss: 1.801001\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 56\n",
      "Norm: 60.66, NNZs: 43, Bias: -20.003521, T: 35840, Avg. loss: 1.818560\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 57\n",
      "Norm: 60.66, NNZs: 43, Bias: -20.003400, T: 36480, Avg. loss: 1.672931\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 58\n",
      "Norm: 60.66, NNZs: 43, Bias: -20.003598, T: 37120, Avg. loss: 1.645868\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 59\n",
      "Norm: 60.66, NNZs: 43, Bias: -20.003679, T: 37760, Avg. loss: 1.647165\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 60\n",
      "Norm: 60.66, NNZs: 43, Bias: -20.003751, T: 38400, Avg. loss: 1.628549\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 61\n",
      "Norm: 60.65, NNZs: 43, Bias: -20.003815, T: 39040, Avg. loss: 1.637275\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 62\n",
      "Norm: 60.65, NNZs: 43, Bias: -20.003767, T: 39680, Avg. loss: 1.612731\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 63\n",
      "Norm: 60.65, NNZs: 43, Bias: -20.004017, T: 40320, Avg. loss: 1.674021\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 64\n",
      "Norm: 60.65, NNZs: 43, Bias: -20.004057, T: 40960, Avg. loss: 1.641089\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 65\n",
      "Norm: 60.65, NNZs: 43, Bias: -20.004208, T: 41600, Avg. loss: 1.654612\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 66\n",
      "Norm: 60.65, NNZs: 43, Bias: -20.004247, T: 42240, Avg. loss: 1.628738\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 67\n",
      "Norm: 60.65, NNZs: 43, Bias: -20.004455, T: 42880, Avg. loss: 1.651704\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 68\n",
      "Norm: 60.65, NNZs: 43, Bias: -20.004436, T: 43520, Avg. loss: 1.598699\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 69\n",
      "Norm: 60.65, NNZs: 43, Bias: -20.004451, T: 44160, Avg. loss: 1.597938\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 70\n",
      "Norm: 60.65, NNZs: 43, Bias: -20.004470, T: 44800, Avg. loss: 1.598396\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 71\n",
      "Norm: 60.65, NNZs: 43, Bias: -20.004489, T: 45440, Avg. loss: 1.597952\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 72\n",
      "Norm: 60.65, NNZs: 43, Bias: -20.004517, T: 46080, Avg. loss: 1.596490\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 73\n",
      "Norm: 60.65, NNZs: 43, Bias: -20.004548, T: 46720, Avg. loss: 1.593578\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 74\n",
      "Norm: 60.65, NNZs: 43, Bias: -20.004571, T: 47360, Avg. loss: 1.595980\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 75\n",
      "Norm: 60.65, NNZs: 43, Bias: -20.004572, T: 48000, Avg. loss: 1.598104\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 76\n",
      "Norm: 60.65, NNZs: 43, Bias: -20.004595, T: 48640, Avg. loss: 1.597131\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 77\n",
      "Norm: 60.65, NNZs: 43, Bias: -20.004600, T: 49280, Avg. loss: 1.593995\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 78\n",
      "Norm: 60.65, NNZs: 43, Bias: -20.004617, T: 49920, Avg. loss: 1.595951\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 79\n",
      "Norm: 60.65, NNZs: 43, Bias: -20.004622, T: 50560, Avg. loss: 1.586214\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 80\n",
      "Norm: 60.65, NNZs: 43, Bias: -20.004627, T: 51200, Avg. loss: 1.586067\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 81\n",
      "Norm: 60.65, NNZs: 43, Bias: -20.004632, T: 51840, Avg. loss: 1.585922\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 82\n",
      "Norm: 60.65, NNZs: 43, Bias: -20.004637, T: 52480, Avg. loss: 1.585847\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 83\n",
      "Norm: 60.65, NNZs: 43, Bias: -20.004643, T: 53120, Avg. loss: 1.585566\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 84\n",
      "Norm: 60.65, NNZs: 43, Bias: -20.004647, T: 53760, Avg. loss: 1.585631\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 85\n",
      "Norm: 60.65, NNZs: 43, Bias: -20.004648, T: 54400, Avg. loss: 1.583308\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 86\n",
      "Norm: 60.65, NNZs: 43, Bias: -20.004649, T: 55040, Avg. loss: 1.583289\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 87\n",
      "Norm: 60.65, NNZs: 43, Bias: -20.004650, T: 55680, Avg. loss: 1.583293\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 88\n",
      "Norm: 60.65, NNZs: 43, Bias: -20.004651, T: 56320, Avg. loss: 1.583283\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 89\n",
      "Norm: 60.65, NNZs: 43, Bias: -20.004651, T: 56960, Avg. loss: 1.583273\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 90\n",
      "Norm: 60.65, NNZs: 43, Bias: -20.004652, T: 57600, Avg. loss: 1.583267\n",
      "Total training time: 0.06 seconds.\n",
      "Convergence after 90 epochs took 0.06 seconds\n"
     ]
    }
   ],
   "source": [
    "clf_sgd_3 = SGDClassifier(loss='log', max_iter=100, random_state=0, verbose=1, learning_rate ='adaptive', eta0 = 0.1)\n",
    "start = time()\n",
    "clf_sgd_3.fit(X_train, y_train)\n",
    "log_time = time()-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training time of the model: 0.06 .\n",
      "accuracy: 0.9\n",
      "f1: 0.5\n"
     ]
    }
   ],
   "source": [
    "y_hat = clf_sgd_3.predict(X_test)\n",
    "print(\"The training time of the model: %.2f .\"\n",
    "      % (log_time))\n",
    "print('accuracy:', accuracy_score(y_test, y_hat))\n",
    "print('f1:', f1_score(y_test, y_hat))\n",
    "setting3_time1 = log_time\n",
    "setting3_acc1 = accuracy_score(y_test, y_hat)\n",
    "setting3_f1 = f1_score(y_test, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 52.73, NNZs: 42, Bias: -2.350012, T: 640, Avg. loss: 395.731500\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 62.62, NNZs: 43, Bias: -4.250012, T: 1280, Avg. loss: 390.578438\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 67.60, NNZs: 43, Bias: -6.250012, T: 1920, Avg. loss: 378.853143\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 56.71, NNZs: 43, Bias: -8.150012, T: 2560, Avg. loss: 385.740560\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 61.34, NNZs: 43, Bias: -10.150012, T: 3200, Avg. loss: 419.292246\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 70.81, NNZs: 43, Bias: -11.950012, T: 3840, Avg. loss: 328.808963\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 86.01, NNZs: 43, Bias: -14.050012, T: 4480, Avg. loss: 388.172183\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 76.21, NNZs: 43, Bias: -15.950018, T: 5120, Avg. loss: 387.626414\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 78.87, NNZs: 43, Bias: -17.950597, T: 5760, Avg. loss: 383.277739\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 81.02, NNZs: 43, Bias: -19.750598, T: 6400, Avg. loss: 380.539960\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 98.21, NNZs: 43, Bias: -21.750598, T: 7040, Avg. loss: 354.234310\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 84.23, NNZs: 43, Bias: -22.050598, T: 7680, Avg. loss: 172.753704\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 79.18, NNZs: 43, Bias: -22.446173, T: 8320, Avg. loss: 116.451419\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 77.73, NNZs: 43, Bias: -22.746098, T: 8960, Avg. loss: 79.745052\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 77.76, NNZs: 43, Bias: -23.104575, T: 9600, Avg. loss: 73.052109\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 78.08, NNZs: 43, Bias: -23.446956, T: 10240, Avg. loss: 75.596357\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 78.53, NNZs: 43, Bias: -23.696443, T: 10880, Avg. loss: 73.339354\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 79.49, NNZs: 43, Bias: -24.070351, T: 11520, Avg. loss: 69.816835\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 79.80, NNZs: 43, Bias: -24.363379, T: 12160, Avg. loss: 71.371020\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 80.77, NNZs: 43, Bias: -24.641057, T: 12800, Avg. loss: 72.714702\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 81.12, NNZs: 43, Bias: -25.001057, T: 13440, Avg. loss: 75.032010\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 82.07, NNZs: 43, Bias: -25.401120, T: 14080, Avg. loss: 73.566774\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 83.11, NNZs: 43, Bias: -25.740508, T: 14720, Avg. loss: 73.348766\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 82.81, NNZs: 43, Bias: -25.783934, T: 15360, Avg. loss: 21.433067\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 82.68, NNZs: 43, Bias: -25.811383, T: 16000, Avg. loss: 16.198548\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 82.64, NNZs: 43, Bias: -25.843141, T: 16640, Avg. loss: 13.327301\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 82.58, NNZs: 43, Bias: -25.878572, T: 17280, Avg. loss: 15.954702\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 82.56, NNZs: 43, Bias: -25.902400, T: 17920, Avg. loss: 9.529031\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 82.54, NNZs: 43, Bias: -25.932608, T: 18560, Avg. loss: 11.345886\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 82.52, NNZs: 43, Bias: -25.977772, T: 19200, Avg. loss: 14.531277\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 82.50, NNZs: 43, Bias: -26.010946, T: 19840, Avg. loss: 13.928005\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 82.49, NNZs: 43, Bias: -26.048026, T: 20480, Avg. loss: 12.668986\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 82.47, NNZs: 43, Bias: -26.085464, T: 21120, Avg. loss: 13.279668\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 82.45, NNZs: 43, Bias: -26.088895, T: 21760, Avg. loss: 5.775088\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 82.42, NNZs: 43, Bias: -26.094230, T: 22400, Avg. loss: 5.309153\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 82.40, NNZs: 43, Bias: -26.096728, T: 23040, Avg. loss: 5.329486\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 82.38, NNZs: 43, Bias: -26.101743, T: 23680, Avg. loss: 5.415659\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 82.35, NNZs: 43, Bias: -26.107977, T: 24320, Avg. loss: 5.454845\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 39\n",
      "Norm: 82.33, NNZs: 43, Bias: -26.112495, T: 24960, Avg. loss: 5.320951\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 40\n",
      "Norm: 82.31, NNZs: 43, Bias: -26.117961, T: 25600, Avg. loss: 5.219219\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 41\n",
      "Norm: 82.28, NNZs: 43, Bias: -26.122602, T: 26240, Avg. loss: 5.343254\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 42\n",
      "Norm: 82.26, NNZs: 43, Bias: -26.125289, T: 26880, Avg. loss: 4.954419\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 43\n",
      "Norm: 82.24, NNZs: 43, Bias: -26.129717, T: 27520, Avg. loss: 5.666689\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 44\n",
      "Norm: 82.22, NNZs: 43, Bias: -26.134421, T: 28160, Avg. loss: 5.321086\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 45\n",
      "Norm: 82.20, NNZs: 43, Bias: -26.139222, T: 28800, Avg. loss: 4.644254\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 46\n",
      "Norm: 82.18, NNZs: 43, Bias: -26.143840, T: 29440, Avg. loss: 4.744887\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 47\n",
      "Norm: 82.15, NNZs: 43, Bias: -26.149187, T: 30080, Avg. loss: 5.178821\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 48\n",
      "Norm: 82.13, NNZs: 43, Bias: -26.152742, T: 30720, Avg. loss: 5.154243\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 49\n",
      "Norm: 82.11, NNZs: 43, Bias: -26.157872, T: 31360, Avg. loss: 5.524210\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 50\n",
      "Norm: 82.09, NNZs: 43, Bias: -26.162024, T: 32000, Avg. loss: 5.035192\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 51\n",
      "Norm: 82.08, NNZs: 43, Bias: -26.162477, T: 32640, Avg. loss: 3.843008\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 52\n",
      "Norm: 82.08, NNZs: 43, Bias: -26.163445, T: 33280, Avg. loss: 3.925760\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 53\n",
      "Norm: 82.07, NNZs: 43, Bias: -26.164322, T: 33920, Avg. loss: 3.965849\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 54\n",
      "Norm: 82.06, NNZs: 43, Bias: -26.164657, T: 34560, Avg. loss: 3.900464\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 55\n",
      "Norm: 82.06, NNZs: 43, Bias: -26.165406, T: 35200, Avg. loss: 3.872842\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 56\n",
      "Norm: 82.05, NNZs: 43, Bias: -26.166388, T: 35840, Avg. loss: 3.984164\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 57\n",
      "Norm: 82.05, NNZs: 43, Bias: -26.166454, T: 36480, Avg. loss: 3.682634\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 58\n",
      "Norm: 82.05, NNZs: 43, Bias: -26.166551, T: 37120, Avg. loss: 3.684517\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 59\n",
      "Norm: 82.05, NNZs: 43, Bias: -26.166775, T: 37760, Avg. loss: 3.647687\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 60\n",
      "Norm: 82.05, NNZs: 43, Bias: -26.166965, T: 38400, Avg. loss: 3.654391\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 61\n",
      "Norm: 82.05, NNZs: 43, Bias: -26.167052, T: 39040, Avg. loss: 3.690995\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 62\n",
      "Norm: 82.05, NNZs: 43, Bias: -26.167191, T: 39680, Avg. loss: 3.684451\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 63\n",
      "Norm: 82.05, NNZs: 43, Bias: -26.167418, T: 40320, Avg. loss: 3.674872\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 64\n",
      "Norm: 82.05, NNZs: 43, Bias: -26.167534, T: 40960, Avg. loss: 3.684933\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 65\n",
      "Norm: 82.04, NNZs: 43, Bias: -26.167544, T: 41600, Avg. loss: 3.620119\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 66\n",
      "Norm: 82.04, NNZs: 43, Bias: -26.167566, T: 42240, Avg. loss: 3.616434\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 67\n",
      "Norm: 82.04, NNZs: 43, Bias: -26.167583, T: 42880, Avg. loss: 3.615582\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 68\n",
      "Norm: 82.04, NNZs: 43, Bias: -26.167609, T: 43520, Avg. loss: 3.614441\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 69\n",
      "Norm: 82.04, NNZs: 43, Bias: -26.167633, T: 44160, Avg. loss: 3.614530\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 70\n",
      "Norm: 82.04, NNZs: 43, Bias: -26.167653, T: 44800, Avg. loss: 3.613882\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 71\n",
      "Norm: 82.04, NNZs: 43, Bias: -26.167675, T: 45440, Avg. loss: 3.612616\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 72\n",
      "Norm: 82.04, NNZs: 43, Bias: -26.167699, T: 46080, Avg. loss: 3.612820\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 73\n",
      "Norm: 82.04, NNZs: 43, Bias: -26.167729, T: 46720, Avg. loss: 3.612978\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 74\n",
      "Norm: 82.04, NNZs: 43, Bias: -26.167750, T: 47360, Avg. loss: 3.611122\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 75\n",
      "Norm: 82.04, NNZs: 43, Bias: -26.167782, T: 48000, Avg. loss: 3.612991\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 76\n",
      "Norm: 82.04, NNZs: 43, Bias: -26.167810, T: 48640, Avg. loss: 3.612008\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 77\n",
      "Norm: 82.04, NNZs: 43, Bias: -26.167835, T: 49280, Avg. loss: 3.611768\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 78\n",
      "Norm: 82.04, NNZs: 43, Bias: -26.167856, T: 49920, Avg. loss: 3.610795\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 79\n",
      "Norm: 82.04, NNZs: 43, Bias: -26.167894, T: 50560, Avg. loss: 3.612093\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 80\n",
      "Norm: 82.04, NNZs: 43, Bias: -26.167900, T: 51200, Avg. loss: 3.597289\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 81\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 82.04, NNZs: 43, Bias: -26.167906, T: 51840, Avg. loss: 3.597229\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 82\n",
      "Norm: 82.04, NNZs: 43, Bias: -26.167911, T: 52480, Avg. loss: 3.597192\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 83\n",
      "Norm: 82.04, NNZs: 43, Bias: -26.167917, T: 53120, Avg. loss: 3.597089\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 84\n",
      "Norm: 82.04, NNZs: 43, Bias: -26.167923, T: 53760, Avg. loss: 3.597134\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 85\n",
      "Norm: 82.04, NNZs: 43, Bias: -26.167929, T: 54400, Avg. loss: 3.597035\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 86\n",
      "Norm: 82.04, NNZs: 43, Bias: -26.167930, T: 55040, Avg. loss: 3.594032\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 87\n",
      "Norm: 82.04, NNZs: 43, Bias: -26.167931, T: 55680, Avg. loss: 3.594023\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 88\n",
      "Norm: 82.04, NNZs: 43, Bias: -26.167932, T: 56320, Avg. loss: 3.594012\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 89\n",
      "Norm: 82.04, NNZs: 43, Bias: -26.167933, T: 56960, Avg. loss: 3.594011\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 90\n",
      "Norm: 82.04, NNZs: 43, Bias: -26.167934, T: 57600, Avg. loss: 3.594004\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 91\n",
      "Norm: 82.04, NNZs: 43, Bias: -26.167936, T: 58240, Avg. loss: 3.593996\n",
      "Total training time: 0.05 seconds.\n",
      "Convergence after 91 epochs took 0.05 seconds\n"
     ]
    }
   ],
   "source": [
    "clf_sgd_3 = SGDClassifier(loss='log', max_iter=100, random_state=1, verbose=1, learning_rate ='adaptive', eta0 = 0.1)\n",
    "start = time()\n",
    "clf_sgd_3.fit(X_train, y_train)\n",
    "log_time = time()-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training time of the model: 0.06 .\n",
      "accuracy: 0.86875\n",
      "f1: 0.39999999999999997\n"
     ]
    }
   ],
   "source": [
    "y_hat = clf_sgd_3.predict(X_test)\n",
    "print(\"The training time of the model: %.2f .\"\n",
    "      % (log_time))\n",
    "print('accuracy:', accuracy_score(y_test, y_hat))\n",
    "print('f1:', f1_score(y_test, y_hat))\n",
    "setting3_time2 = log_time\n",
    "setting3_acc2 = accuracy_score(y_test, y_hat)\n",
    "setting3_f12 = f1_score(y_test, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 55.31, NNZs: 41, Bias: -2.350000, T: 640, Avg. loss: 381.805725\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 64.18, NNZs: 42, Bias: -4.350103, T: 1280, Avg. loss: 399.290291\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 67.36, NNZs: 42, Bias: -6.453052, T: 1920, Avg. loss: 376.934638\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 52.09, NNZs: 42, Bias: -8.448055, T: 2560, Avg. loss: 418.189823\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 73.59, NNZs: 43, Bias: -10.447155, T: 3200, Avg. loss: 376.037336\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 60.20, NNZs: 43, Bias: -12.447157, T: 3840, Avg. loss: 410.675623\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 67.11, NNZs: 43, Bias: -14.525940, T: 4480, Avg. loss: 386.571739\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 87.38, NNZs: 43, Bias: -16.525940, T: 5120, Avg. loss: 364.806967\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 81.53, NNZs: 43, Bias: -18.425940, T: 5760, Avg. loss: 398.779448\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 83.30, NNZs: 43, Bias: -20.125570, T: 6400, Avg. loss: 397.241311\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 87.99, NNZs: 43, Bias: -21.925459, T: 7040, Avg. loss: 330.025834\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 88.79, NNZs: 43, Bias: -24.224776, T: 7680, Avg. loss: 402.005644\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 94.20, NNZs: 43, Bias: -26.424777, T: 8320, Avg. loss: 367.172434\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 98.00, NNZs: 43, Bias: -28.226672, T: 8960, Avg. loss: 398.185493\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 111.11, NNZs: 43, Bias: -30.458023, T: 9600, Avg. loss: 376.277356\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 112.66, NNZs: 43, Bias: -32.357964, T: 10240, Avg. loss: 389.869502\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 105.73, NNZs: 43, Bias: -32.683462, T: 10880, Avg. loss: 137.255526\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 103.55, NNZs: 43, Bias: -33.016104, T: 11520, Avg. loss: 91.064440\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 102.95, NNZs: 43, Bias: -33.337601, T: 12160, Avg. loss: 75.829373\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 104.11, NNZs: 43, Bias: -33.707593, T: 12800, Avg. loss: 65.282514\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 104.36, NNZs: 43, Bias: -33.923205, T: 13440, Avg. loss: 68.705016\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 104.59, NNZs: 43, Bias: -34.206720, T: 14080, Avg. loss: 69.749227\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 105.40, NNZs: 43, Bias: -34.527278, T: 14720, Avg. loss: 69.350160\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 106.19, NNZs: 43, Bias: -34.865317, T: 15360, Avg. loss: 71.193793\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 107.28, NNZs: 43, Bias: -35.224085, T: 16000, Avg. loss: 66.683547\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 107.02, NNZs: 43, Bias: -35.251771, T: 16640, Avg. loss: 20.277068\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 106.85, NNZs: 43, Bias: -35.281405, T: 17280, Avg. loss: 17.835047\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 106.78, NNZs: 43, Bias: -35.315004, T: 17920, Avg. loss: 13.143283\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 106.72, NNZs: 43, Bias: -35.342096, T: 18560, Avg. loss: 14.621455\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 106.68, NNZs: 43, Bias: -35.377007, T: 19200, Avg. loss: 14.539322\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 106.63, NNZs: 43, Bias: -35.409972, T: 19840, Avg. loss: 13.423127\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 106.63, NNZs: 43, Bias: -35.449805, T: 20480, Avg. loss: 12.083067\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 106.58, NNZs: 43, Bias: -35.485052, T: 21120, Avg. loss: 12.734998\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 106.54, NNZs: 43, Bias: -35.505974, T: 21760, Avg. loss: 13.232281\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 106.49, NNZs: 43, Bias: -35.529012, T: 22400, Avg. loss: 12.229174\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 106.46, NNZs: 43, Bias: -35.564813, T: 23040, Avg. loss: 12.978597\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 106.42, NNZs: 43, Bias: -35.593696, T: 23680, Avg. loss: 14.022692\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 106.40, NNZs: 43, Bias: -35.599392, T: 24320, Avg. loss: 5.667620\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 39\n",
      "Norm: 106.38, NNZs: 43, Bias: -35.604810, T: 24960, Avg. loss: 5.423637\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 40\n",
      "Norm: 106.36, NNZs: 43, Bias: -35.609136, T: 25600, Avg. loss: 5.451425\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 41\n",
      "Norm: 106.34, NNZs: 43, Bias: -35.614449, T: 26240, Avg. loss: 5.279032\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 42\n",
      "Norm: 106.31, NNZs: 43, Bias: -35.617208, T: 26880, Avg. loss: 5.432712\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 43\n",
      "Norm: 106.29, NNZs: 43, Bias: -35.621560, T: 27520, Avg. loss: 5.207109\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 44\n",
      "Norm: 106.27, NNZs: 43, Bias: -35.625339, T: 28160, Avg. loss: 5.272775\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 45\n",
      "Norm: 106.25, NNZs: 43, Bias: -35.628737, T: 28800, Avg. loss: 4.959593\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 46\n",
      "Norm: 106.23, NNZs: 43, Bias: -35.632221, T: 29440, Avg. loss: 5.654190\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 47\n",
      "Norm: 106.21, NNZs: 43, Bias: -35.635679, T: 30080, Avg. loss: 5.451522\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 48\n",
      "Norm: 106.19, NNZs: 43, Bias: -35.639473, T: 30720, Avg. loss: 5.101442\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 49\n",
      "Norm: 106.17, NNZs: 43, Bias: -35.643064, T: 31360, Avg. loss: 5.584914\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 50\n",
      "Norm: 106.15, NNZs: 43, Bias: -35.645824, T: 32000, Avg. loss: 4.737021\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 51\n",
      "Norm: 106.12, NNZs: 43, Bias: -35.649488, T: 32640, Avg. loss: 5.735429\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 52\n",
      "Norm: 106.10, NNZs: 43, Bias: -35.654263, T: 33280, Avg. loss: 4.986390\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 53\n",
      "Norm: 106.08, NNZs: 43, Bias: -35.657144, T: 33920, Avg. loss: 4.999571\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 54\n",
      "Norm: 106.07, NNZs: 43, Bias: -35.661194, T: 34560, Avg. loss: 4.519529\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 55\n",
      "Norm: 106.05, NNZs: 43, Bias: -35.665291, T: 35200, Avg. loss: 4.869670\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 56\n",
      "Norm: 106.03, NNZs: 43, Bias: -35.668822, T: 35840, Avg. loss: 5.167357\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 57\n",
      "Norm: 106.00, NNZs: 43, Bias: -35.672703, T: 36480, Avg. loss: 5.136387\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 58\n",
      "Norm: 105.98, NNZs: 43, Bias: -35.677119, T: 37120, Avg. loss: 5.018548\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 59\n",
      "Norm: 105.96, NNZs: 43, Bias: -35.680973, T: 37760, Avg. loss: 4.870410\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 60\n",
      "Norm: 105.96, NNZs: 43, Bias: -35.680846, T: 38400, Avg. loss: 3.783617\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 61\n",
      "Norm: 105.95, NNZs: 43, Bias: -35.681586, T: 39040, Avg. loss: 4.029240\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 62\n",
      "Norm: 105.95, NNZs: 43, Bias: -35.681986, T: 39680, Avg. loss: 3.979827\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 63\n",
      "Norm: 105.95, NNZs: 43, Bias: -35.682367, T: 40320, Avg. loss: 3.880711\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 64\n",
      "Norm: 105.94, NNZs: 43, Bias: -35.683640, T: 40960, Avg. loss: 3.929231\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 65\n",
      "Norm: 105.94, NNZs: 43, Bias: -35.684090, T: 41600, Avg. loss: 3.769935\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 66\n",
      "Norm: 105.93, NNZs: 43, Bias: -35.684524, T: 42240, Avg. loss: 4.013202\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 67\n",
      "Norm: 105.93, NNZs: 43, Bias: -35.685529, T: 42880, Avg. loss: 3.911768\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 68\n",
      "Norm: 105.92, NNZs: 43, Bias: -35.686023, T: 43520, Avg. loss: 3.986844\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 69\n",
      "Norm: 105.92, NNZs: 43, Bias: -35.687077, T: 44160, Avg. loss: 3.928973\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 70\n",
      "Norm: 105.91, NNZs: 43, Bias: -35.687340, T: 44800, Avg. loss: 3.765579\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 71\n",
      "Norm: 105.91, NNZs: 43, Bias: -35.688176, T: 45440, Avg. loss: 3.947773\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 72\n",
      "Norm: 105.90, NNZs: 43, Bias: -35.688132, T: 46080, Avg. loss: 4.013754\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 73\n",
      "Norm: 105.90, NNZs: 43, Bias: -35.689397, T: 46720, Avg. loss: 3.638884\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 74\n",
      "Norm: 105.89, NNZs: 43, Bias: -35.690222, T: 47360, Avg. loss: 3.733144\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 75\n",
      "Norm: 105.89, NNZs: 43, Bias: -35.690731, T: 48000, Avg. loss: 3.971456\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 76\n",
      "Norm: 105.89, NNZs: 43, Bias: -35.691205, T: 48640, Avg. loss: 3.994685\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 77\n",
      "Norm: 105.88, NNZs: 43, Bias: -35.691361, T: 49280, Avg. loss: 3.870770\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 78\n",
      "Norm: 105.88, NNZs: 43, Bias: -35.692469, T: 49920, Avg. loss: 3.957042\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 79\n",
      "Norm: 105.88, NNZs: 43, Bias: -35.692459, T: 50560, Avg. loss: 3.736391\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 80\n",
      "Norm: 105.87, NNZs: 43, Bias: -35.692418, T: 51200, Avg. loss: 3.693335\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 81\n",
      "Norm: 105.87, NNZs: 43, Bias: -35.692598, T: 51840, Avg. loss: 3.720922\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 82\n",
      "Norm: 105.87, NNZs: 43, Bias: -35.692769, T: 52480, Avg. loss: 3.691122\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 83\n",
      "Norm: 105.87, NNZs: 43, Bias: -35.692905, T: 53120, Avg. loss: 3.716296\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 84\n",
      "Norm: 105.87, NNZs: 43, Bias: -35.692901, T: 53760, Avg. loss: 3.657191\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 85\n",
      "Norm: 105.87, NNZs: 43, Bias: -35.692912, T: 54400, Avg. loss: 3.654475\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 86\n",
      "Norm: 105.87, NNZs: 43, Bias: -35.692929, T: 55040, Avg. loss: 3.653764\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 87\n",
      "Norm: 105.87, NNZs: 43, Bias: -35.692945, T: 55680, Avg. loss: 3.652535\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 88\n",
      "Norm: 105.87, NNZs: 43, Bias: -35.692972, T: 56320, Avg. loss: 3.652135\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 89\n",
      "Norm: 105.87, NNZs: 43, Bias: -35.692977, T: 56960, Avg. loss: 3.638433\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 90\n",
      "Norm: 105.87, NNZs: 43, Bias: -35.692981, T: 57600, Avg. loss: 3.638350\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 91\n",
      "Norm: 105.87, NNZs: 43, Bias: -35.692986, T: 58240, Avg. loss: 3.638341\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 92\n",
      "Norm: 105.87, NNZs: 43, Bias: -35.692990, T: 58880, Avg. loss: 3.638325\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 93\n",
      "Norm: 105.87, NNZs: 43, Bias: -35.692995, T: 59520, Avg. loss: 3.638307\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 94\n",
      "Norm: 105.87, NNZs: 43, Bias: -35.692996, T: 60160, Avg. loss: 3.635461\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 95\n",
      "Norm: 105.87, NNZs: 43, Bias: -35.692997, T: 60800, Avg. loss: 3.635452\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 96\n",
      "Norm: 105.87, NNZs: 43, Bias: -35.692998, T: 61440, Avg. loss: 3.635447\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 97\n",
      "Norm: 105.87, NNZs: 43, Bias: -35.692999, T: 62080, Avg. loss: 3.635441\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 98\n",
      "Norm: 105.87, NNZs: 43, Bias: -35.693000, T: 62720, Avg. loss: 3.635435\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 99\n",
      "Norm: 105.87, NNZs: 43, Bias: -35.693000, T: 63360, Avg. loss: 3.635427\n",
      "Total training time: 0.07 seconds.\n",
      "Convergence after 99 epochs took 0.07 seconds\n"
     ]
    }
   ],
   "source": [
    "clf_sgd_3 = SGDClassifier(loss='log', max_iter=100, random_state= 2, verbose=1, learning_rate ='adaptive', eta0 = 0.1)\n",
    "start = time()\n",
    "clf_sgd_3.fit(X_train, y_train)\n",
    "log_time = time()-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training time of the model: 0.07 .\n",
      "accuracy: 0.8875\n",
      "f1: 0.43749999999999994\n",
      "The mean training time of the model: 0.06216629346211752 (std: 0.00497485763522954) .\n",
      "The mean accuracy time of the model: 0.88541666666666663 (std: 0.01284252917285203) .\n",
      "The mean F1 score time of the model: 0.44583333333333330 (std: 0.04124789556921529) .\n"
     ]
    }
   ],
   "source": [
    "y_hat = clf_sgd_3.predict(X_test)\n",
    "print(\"The training time of the model: %.2f .\"\n",
    "      % (log_time))\n",
    "print('accuracy:', accuracy_score(y_test, y_hat))\n",
    "print('f1:', f1_score(y_test, y_hat))\n",
    "setting3_time3 = log_time\n",
    "setting3_acc3 = accuracy_score(y_test, y_hat)\n",
    "setting3_f13 = f1_score(y_test, y_hat)\n",
    "set3_time = np.array([setting3_time1, setting3_time2, setting3_time3])\n",
    "mean_time = np.mean(set3_time)\n",
    "std_time = np.std(set3_time)\n",
    "set3_acc = np.array([setting3_acc1, setting3_acc2, setting3_acc3])\n",
    "mean_acc = np.mean(set3_acc)\n",
    "std_acc = np.std(set3_acc)\n",
    "set3_f1 = np.array([setting3_f1, setting3_f12, setting3_f13])\n",
    "mean_f1 = np.mean(set3_f1)\n",
    "std_f1 = np.std(set3_f1)\n",
    "print(\"The mean training time of the model: %.17f (std: %.17f) .\"\n",
    "      % (mean_time, std_time))\n",
    "print(\"The mean accuracy time of the model: %.17f (std: %.17f) .\"\n",
    "      % (mean_acc, std_acc))\n",
    "print(\"The mean F1 score time of the model: %.17f (std: %.17f) .\"\n",
    "      % (mean_f1, std_f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3.2 Single-hidden-layer Neural Networks\n",
    "**Settings: H âˆˆ {1,2,4,8,16,32,64}, max_iter = 500, others hyperparameter are default**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 5.30190509\n",
      "Iteration 2, loss = 0.98071396\n",
      "Iteration 3, loss = 0.97279874\n",
      "Iteration 4, loss = 0.96336836\n",
      "Iteration 5, loss = 0.95288064\n",
      "Iteration 6, loss = 0.94178766\n",
      "Iteration 7, loss = 0.93054988\n",
      "Iteration 8, loss = 0.91918299\n",
      "Iteration 9, loss = 0.90780993\n",
      "Iteration 10, loss = 0.89658983\n",
      "Iteration 11, loss = 0.88543929\n",
      "Iteration 12, loss = 0.87443858\n",
      "Iteration 13, loss = 0.86349945\n",
      "Iteration 14, loss = 0.85290758\n",
      "Iteration 15, loss = 0.84234171\n",
      "Iteration 16, loss = 0.83211855\n",
      "Iteration 17, loss = 0.82193938\n",
      "Iteration 18, loss = 0.81201568\n",
      "Iteration 19, loss = 0.80236513\n",
      "Iteration 20, loss = 0.79285935\n",
      "Iteration 21, loss = 0.78351278\n",
      "Iteration 22, loss = 0.77446026\n",
      "Iteration 23, loss = 0.76556272\n",
      "Iteration 24, loss = 0.75682134\n",
      "Iteration 25, loss = 0.74821154\n",
      "Iteration 26, loss = 0.73977557\n",
      "Iteration 27, loss = 0.73150662\n",
      "Iteration 28, loss = 0.72337294\n",
      "Iteration 29, loss = 0.71551917\n",
      "Iteration 30, loss = 0.70781064\n",
      "Iteration 31, loss = 0.70038202\n",
      "Iteration 32, loss = 0.69284059\n",
      "Iteration 33, loss = 0.68565126\n",
      "Iteration 34, loss = 0.67845839\n",
      "Iteration 35, loss = 0.67136620\n",
      "Iteration 36, loss = 0.66443894\n",
      "Iteration 37, loss = 0.65774300\n",
      "Iteration 38, loss = 0.65110648\n",
      "Iteration 39, loss = 0.64465077\n",
      "Iteration 40, loss = 0.63832395\n",
      "Iteration 41, loss = 0.63200439\n",
      "Iteration 42, loss = 0.62588099\n",
      "Iteration 43, loss = 0.61984773\n",
      "Iteration 44, loss = 0.61391803\n",
      "Iteration 45, loss = 0.60825547\n",
      "Iteration 46, loss = 0.60259795\n",
      "Iteration 47, loss = 0.59718402\n",
      "Iteration 48, loss = 0.59181436\n",
      "Iteration 49, loss = 0.58657075\n",
      "Iteration 50, loss = 0.58141911\n",
      "Iteration 51, loss = 0.57637264\n",
      "Iteration 52, loss = 0.57139157\n",
      "Iteration 53, loss = 0.56650729\n",
      "Iteration 54, loss = 0.56165424\n",
      "Iteration 55, loss = 0.55699980\n",
      "Iteration 56, loss = 0.55240731\n",
      "Iteration 57, loss = 0.54790278\n",
      "Iteration 58, loss = 0.54349525\n",
      "Iteration 59, loss = 0.53917702\n",
      "Iteration 60, loss = 0.53502523\n",
      "Iteration 61, loss = 0.53076340\n",
      "Iteration 62, loss = 0.52670491\n",
      "Iteration 63, loss = 0.52271562\n",
      "Iteration 64, loss = 0.51876223\n",
      "Iteration 65, loss = 0.51490484\n",
      "Iteration 66, loss = 0.51112642\n",
      "Iteration 67, loss = 0.50741751\n",
      "Iteration 68, loss = 0.50382297\n",
      "Iteration 69, loss = 0.50032011\n",
      "Iteration 70, loss = 0.49690994\n",
      "Iteration 71, loss = 0.49351852\n",
      "Iteration 72, loss = 0.49013977\n",
      "Iteration 73, loss = 0.48689358\n",
      "Iteration 74, loss = 0.48360392\n",
      "Iteration 75, loss = 0.48042180\n",
      "Iteration 76, loss = 0.47731307\n",
      "Iteration 77, loss = 0.47421515\n",
      "Iteration 78, loss = 0.47118296\n",
      "Iteration 79, loss = 0.46817063\n",
      "Iteration 80, loss = 0.46525516\n",
      "Iteration 81, loss = 0.46236528\n",
      "Iteration 82, loss = 0.45958471\n",
      "Iteration 83, loss = 0.45678119\n",
      "Iteration 84, loss = 0.45408387\n",
      "Iteration 85, loss = 0.45147473\n",
      "Iteration 86, loss = 0.44883082\n",
      "Iteration 87, loss = 0.44631155\n",
      "Iteration 88, loss = 0.44385938\n",
      "Iteration 89, loss = 0.44142837\n",
      "Iteration 90, loss = 0.43903733\n",
      "Iteration 91, loss = 0.43661154\n",
      "Iteration 92, loss = 0.43428450\n",
      "Iteration 93, loss = 0.43192065\n",
      "Iteration 94, loss = 0.42964338\n",
      "Iteration 95, loss = 0.42740777\n",
      "Iteration 96, loss = 0.42523209\n",
      "Iteration 97, loss = 0.42306274\n",
      "Iteration 98, loss = 0.42098252\n",
      "Iteration 99, loss = 0.41893192\n",
      "Iteration 100, loss = 0.41688663\n",
      "Iteration 101, loss = 0.41484690\n",
      "Iteration 102, loss = 0.41284617\n",
      "Iteration 103, loss = 0.41092307\n",
      "Iteration 104, loss = 0.40903718\n",
      "Iteration 105, loss = 0.40722663\n",
      "Iteration 106, loss = 0.40547402\n",
      "Iteration 107, loss = 0.40372153\n",
      "Iteration 108, loss = 0.40201015\n",
      "Iteration 109, loss = 0.40027866\n",
      "Iteration 110, loss = 0.39860949\n",
      "Iteration 111, loss = 0.39697930\n",
      "Iteration 112, loss = 0.39529843\n",
      "Iteration 113, loss = 0.39367293\n",
      "Iteration 114, loss = 0.39204730\n",
      "Iteration 115, loss = 0.39051057\n",
      "Iteration 116, loss = 0.38898453\n",
      "Iteration 117, loss = 0.38749564\n",
      "Iteration 118, loss = 0.38599743\n",
      "Iteration 119, loss = 0.38449988\n",
      "Iteration 120, loss = 0.38306322\n",
      "Iteration 121, loss = 0.38161164\n",
      "Iteration 122, loss = 0.38016285\n",
      "Iteration 123, loss = 0.37873078\n",
      "Iteration 124, loss = 0.37737964\n",
      "Iteration 125, loss = 0.37603851\n",
      "Iteration 126, loss = 0.37469167\n",
      "Iteration 127, loss = 0.37339008\n",
      "Iteration 128, loss = 0.37210747\n",
      "Iteration 129, loss = 0.37083119\n",
      "Iteration 130, loss = 0.36957770\n",
      "Iteration 131, loss = 0.36835248\n",
      "Iteration 132, loss = 0.36713723\n",
      "Iteration 133, loss = 0.36590483\n",
      "Iteration 134, loss = 0.36474956\n",
      "Iteration 135, loss = 0.36355341\n",
      "Iteration 136, loss = 0.36240800\n",
      "Iteration 137, loss = 0.36127413\n",
      "Iteration 138, loss = 0.36018174\n",
      "Iteration 139, loss = 0.35906771\n",
      "Iteration 140, loss = 0.35800568\n",
      "Iteration 141, loss = 0.35694863\n",
      "Iteration 142, loss = 0.35589360\n",
      "Iteration 143, loss = 0.35489745\n",
      "Iteration 144, loss = 0.35385989\n",
      "Iteration 145, loss = 0.35286554\n",
      "Iteration 146, loss = 0.35189124\n",
      "Iteration 147, loss = 0.35094352\n",
      "Iteration 148, loss = 0.35001397\n",
      "Iteration 149, loss = 0.34909514\n",
      "Iteration 150, loss = 0.34820960\n",
      "Iteration 151, loss = 0.34731696\n",
      "Iteration 152, loss = 0.34643975\n",
      "Iteration 153, loss = 0.34558420\n",
      "Iteration 154, loss = 0.34471863\n",
      "Iteration 155, loss = 0.34387224\n",
      "Iteration 156, loss = 0.34306029\n",
      "Iteration 157, loss = 0.34222008\n",
      "Iteration 158, loss = 0.34144695\n",
      "Iteration 159, loss = 0.34066422\n",
      "Iteration 160, loss = 0.33991446\n",
      "Iteration 161, loss = 0.33912211\n",
      "Iteration 162, loss = 0.33835243\n",
      "Iteration 163, loss = 0.33755614\n",
      "Iteration 164, loss = 0.33675370\n",
      "Iteration 165, loss = 0.33596205\n",
      "Iteration 166, loss = 0.33520588\n",
      "Iteration 167, loss = 0.33440544\n",
      "Iteration 168, loss = 0.33365211\n",
      "Iteration 169, loss = 0.33291996\n",
      "Iteration 170, loss = 0.33219652\n",
      "Iteration 171, loss = 0.33149818\n",
      "Iteration 172, loss = 0.33082372\n",
      "Iteration 173, loss = 0.33011561\n",
      "Iteration 174, loss = 0.32948276\n",
      "Iteration 175, loss = 0.32882570\n",
      "Iteration 176, loss = 0.32819345\n",
      "Iteration 177, loss = 0.32756934\n",
      "Iteration 178, loss = 0.32695155\n",
      "Iteration 179, loss = 0.32634521\n",
      "Iteration 180, loss = 0.32575532\n",
      "Iteration 181, loss = 0.32518170\n",
      "Iteration 182, loss = 0.32462922\n",
      "Iteration 183, loss = 0.32404067\n",
      "Iteration 184, loss = 0.32346261\n",
      "Iteration 185, loss = 0.32288504\n",
      "Iteration 186, loss = 0.32231058\n",
      "Iteration 187, loss = 0.32175897\n",
      "Iteration 188, loss = 0.32118341\n",
      "Iteration 189, loss = 0.32063586\n",
      "Iteration 190, loss = 0.32010303\n",
      "Iteration 191, loss = 0.31954122\n",
      "Iteration 192, loss = 0.31901499\n",
      "Iteration 193, loss = 0.31851025\n",
      "Iteration 194, loss = 0.31801094\n",
      "Iteration 195, loss = 0.31750486\n",
      "Iteration 196, loss = 0.31700230\n",
      "Iteration 197, loss = 0.31650675\n",
      "Iteration 198, loss = 0.31604312\n",
      "Iteration 199, loss = 0.31556007\n",
      "Iteration 200, loss = 0.31508062\n",
      "Iteration 201, loss = 0.31460046\n",
      "Iteration 202, loss = 0.31412144\n",
      "Iteration 203, loss = 0.31362357\n",
      "Iteration 204, loss = 0.31313969\n",
      "Iteration 205, loss = 0.31265728\n",
      "Iteration 206, loss = 0.31217121\n",
      "Iteration 207, loss = 0.31174746\n",
      "Iteration 208, loss = 0.31127826\n",
      "Iteration 209, loss = 0.31086063\n",
      "Iteration 210, loss = 0.31040987\n",
      "Iteration 211, loss = 0.30998284\n",
      "Iteration 212, loss = 0.30957772\n",
      "Iteration 213, loss = 0.30916639\n",
      "Iteration 214, loss = 0.30875095\n",
      "Iteration 215, loss = 0.30834983\n",
      "Iteration 216, loss = 0.30793955\n",
      "Iteration 217, loss = 0.30753604\n",
      "Iteration 218, loss = 0.30713840\n",
      "Iteration 219, loss = 0.30674926\n",
      "Iteration 220, loss = 0.30637627\n",
      "Iteration 221, loss = 0.30597977\n",
      "Iteration 222, loss = 0.30560304\n",
      "Iteration 223, loss = 0.30524199\n",
      "Iteration 224, loss = 0.30487412\n",
      "Iteration 225, loss = 0.30450383\n",
      "Iteration 226, loss = 0.30413024\n",
      "Iteration 227, loss = 0.30376659\n",
      "Iteration 228, loss = 0.30340980\n",
      "Iteration 229, loss = 0.30305522\n",
      "Iteration 230, loss = 0.30271862\n",
      "Iteration 231, loss = 0.30237389\n",
      "Iteration 232, loss = 0.30205186\n",
      "Iteration 233, loss = 0.30172339\n",
      "Iteration 234, loss = 0.30141554\n",
      "Iteration 235, loss = 0.30108856\n",
      "Iteration 236, loss = 0.30077275\n",
      "Iteration 237, loss = 0.30044762\n",
      "Iteration 238, loss = 0.30013833\n",
      "Iteration 239, loss = 0.29982577\n",
      "Iteration 240, loss = 0.29951341\n",
      "Iteration 241, loss = 0.29921624\n",
      "Iteration 242, loss = 0.29892042\n",
      "Iteration 243, loss = 0.29860061\n",
      "Iteration 244, loss = 0.29833549\n",
      "Iteration 245, loss = 0.29803292\n",
      "Iteration 246, loss = 0.29774186\n",
      "Iteration 247, loss = 0.29747115\n",
      "Iteration 248, loss = 0.29720360\n",
      "Iteration 249, loss = 0.29691251\n",
      "Iteration 250, loss = 0.29663439\n",
      "Iteration 251, loss = 0.29636730\n",
      "Iteration 252, loss = 0.29609175\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 253, loss = 0.29582374\n",
      "Iteration 254, loss = 0.29557408\n",
      "Iteration 255, loss = 0.29530857\n",
      "Iteration 256, loss = 0.29504473\n",
      "Iteration 257, loss = 0.29479309\n",
      "Iteration 258, loss = 0.29452211\n",
      "Iteration 259, loss = 0.29426956\n",
      "Iteration 260, loss = 0.29400416\n",
      "Iteration 261, loss = 0.29375460\n",
      "Iteration 262, loss = 0.29349639\n",
      "Iteration 263, loss = 0.29324435\n",
      "Iteration 264, loss = 0.29301439\n",
      "Iteration 265, loss = 0.29279013\n",
      "Iteration 266, loss = 0.29257037\n",
      "Iteration 267, loss = 0.29233585\n",
      "Iteration 268, loss = 0.29213204\n",
      "Iteration 269, loss = 0.29189750\n",
      "Iteration 270, loss = 0.29167999\n",
      "Iteration 271, loss = 0.29144503\n",
      "Iteration 272, loss = 0.29124700\n",
      "Iteration 273, loss = 0.29104669\n",
      "Iteration 274, loss = 0.29082308\n",
      "Iteration 275, loss = 0.29060802\n",
      "Iteration 276, loss = 0.29039614\n",
      "Iteration 277, loss = 0.29018559\n",
      "Iteration 278, loss = 0.28996368\n",
      "Iteration 279, loss = 0.28974764\n",
      "Iteration 280, loss = 0.28955445\n",
      "Iteration 281, loss = 0.28933976\n",
      "Iteration 282, loss = 0.28912940\n",
      "Iteration 283, loss = 0.28892829\n",
      "Iteration 284, loss = 0.28872788\n",
      "Iteration 285, loss = 0.28853546\n",
      "Iteration 286, loss = 0.28835435\n",
      "Iteration 287, loss = 0.28818020\n",
      "Iteration 288, loss = 0.28800674\n",
      "Iteration 289, loss = 0.28783396\n",
      "Iteration 290, loss = 0.28766933\n",
      "Iteration 291, loss = 0.28750884\n",
      "Iteration 292, loss = 0.28734435\n",
      "Iteration 293, loss = 0.28717579\n",
      "Iteration 294, loss = 0.28701164\n",
      "Iteration 295, loss = 0.28684367\n",
      "Iteration 296, loss = 0.28668541\n",
      "Iteration 297, loss = 0.28650691\n",
      "Iteration 298, loss = 0.28634090\n",
      "Iteration 299, loss = 0.28617815\n",
      "Iteration 300, loss = 0.28601064\n",
      "Iteration 301, loss = 0.28583946\n",
      "Iteration 302, loss = 0.28567267\n",
      "Iteration 303, loss = 0.28551064\n",
      "Iteration 304, loss = 0.28535373\n",
      "Iteration 305, loss = 0.28518761\n",
      "Iteration 306, loss = 0.28503150\n",
      "Iteration 307, loss = 0.28486937\n",
      "Iteration 308, loss = 0.28470678\n",
      "Iteration 309, loss = 0.28456824\n",
      "Iteration 310, loss = 0.28439804\n",
      "Iteration 311, loss = 0.28423836\n",
      "Iteration 312, loss = 0.28408753\n",
      "Iteration 313, loss = 0.28393477\n",
      "Iteration 314, loss = 0.28378872\n",
      "Iteration 315, loss = 0.28364870\n",
      "Iteration 316, loss = 0.28350395\n",
      "Iteration 317, loss = 0.28337565\n",
      "Iteration 318, loss = 0.28323055\n",
      "Iteration 319, loss = 0.28310546\n",
      "Iteration 320, loss = 0.28297940\n",
      "Iteration 321, loss = 0.28284857\n",
      "Iteration 322, loss = 0.28272921\n",
      "Iteration 323, loss = 0.28260275\n",
      "Iteration 324, loss = 0.28247834\n",
      "Iteration 325, loss = 0.28234296\n",
      "Iteration 326, loss = 0.28221021\n",
      "Iteration 327, loss = 0.28208128\n",
      "Iteration 328, loss = 0.28196173\n",
      "Iteration 329, loss = 0.28181063\n",
      "Iteration 330, loss = 0.28168763\n",
      "Iteration 331, loss = 0.28156920\n",
      "Iteration 332, loss = 0.28144496\n",
      "Iteration 333, loss = 0.28132387\n",
      "Iteration 334, loss = 0.28119570\n",
      "Iteration 335, loss = 0.28107542\n",
      "Iteration 336, loss = 0.28095334\n",
      "Iteration 337, loss = 0.28084663\n",
      "Iteration 338, loss = 0.28073213\n",
      "Iteration 339, loss = 0.28062467\n",
      "Iteration 340, loss = 0.28051933\n",
      "Iteration 341, loss = 0.28041004\n",
      "Iteration 342, loss = 0.28030848\n",
      "Iteration 343, loss = 0.28021180\n",
      "Iteration 344, loss = 0.28010360\n",
      "Iteration 345, loss = 0.28000969\n",
      "Iteration 346, loss = 0.27990249\n",
      "Iteration 347, loss = 0.27979342\n",
      "Iteration 348, loss = 0.27968431\n",
      "Iteration 349, loss = 0.27956995\n",
      "Iteration 350, loss = 0.27946031\n",
      "Iteration 351, loss = 0.27935620\n",
      "Iteration 352, loss = 0.27923823\n",
      "Iteration 353, loss = 0.27914251\n",
      "Iteration 354, loss = 0.27902837\n",
      "Iteration 355, loss = 0.27892738\n",
      "Iteration 356, loss = 0.27882708\n",
      "Iteration 357, loss = 0.27872594\n",
      "Iteration 358, loss = 0.27863036\n",
      "Iteration 359, loss = 0.27854366\n",
      "Iteration 360, loss = 0.27845394\n",
      "Iteration 361, loss = 0.27835922\n",
      "Iteration 362, loss = 0.27827667\n",
      "Iteration 363, loss = 0.27818074\n",
      "Iteration 364, loss = 0.27809703\n",
      "Iteration 365, loss = 0.27800976\n",
      "Iteration 366, loss = 0.27792812\n",
      "Iteration 367, loss = 0.27785105\n",
      "Iteration 368, loss = 0.27777061\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "The training time of the model: 0.65 .\n",
      "accuracy: 0.89375\n",
      "f1: 0.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn import  metrics\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "#model H=1\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(1, ), solver='sgd', \n",
    "                     max_iter=500, random_state=0, verbose=True)\n",
    "start = time()\n",
    "mlp.fit(X_train, y_train)\n",
    "NN_time = time() - start\n",
    "y_hat = mlp.predict(X_test)\n",
    "print(\"The training time of the model: %.2f .\"\n",
    "      % (NN_time))\n",
    "print('accuracy:', accuracy_score(y_test, y_hat))\n",
    "print('f1:', f1_score(y_test, y_hat))\n",
    "setting1_time1 = NN_time\n",
    "setting1_acc1 = accuracy_score(y_test, y_hat)\n",
    "setting1_f1 = f1_score(y_test, y_hat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.62013677\n",
      "Iteration 2, loss = 0.61796160\n",
      "Iteration 3, loss = 0.61459456\n",
      "Iteration 4, loss = 0.61045843\n",
      "Iteration 5, loss = 0.60588168\n",
      "Iteration 6, loss = 0.60109844\n",
      "Iteration 7, loss = 0.59610420\n",
      "Iteration 8, loss = 0.59102740\n",
      "Iteration 9, loss = 0.58600197\n",
      "Iteration 10, loss = 0.58093395\n",
      "Iteration 11, loss = 0.57586373\n",
      "Iteration 12, loss = 0.57099397\n",
      "Iteration 13, loss = 0.56613137\n",
      "Iteration 14, loss = 0.56136947\n",
      "Iteration 15, loss = 0.55672248\n",
      "Iteration 16, loss = 0.55217767\n",
      "Iteration 17, loss = 0.54775001\n",
      "Iteration 18, loss = 0.54341804\n",
      "Iteration 19, loss = 0.53921503\n",
      "Iteration 20, loss = 0.53492468\n",
      "Iteration 21, loss = 0.53079479\n",
      "Iteration 22, loss = 0.52667190\n",
      "Iteration 23, loss = 0.52276136\n",
      "Iteration 24, loss = 0.51879308\n",
      "Iteration 25, loss = 0.51494613\n",
      "Iteration 26, loss = 0.51116439\n",
      "Iteration 27, loss = 0.50742286\n",
      "Iteration 28, loss = 0.50379121\n",
      "Iteration 29, loss = 0.50023339\n",
      "Iteration 30, loss = 0.49673913\n",
      "Iteration 31, loss = 0.49329963\n",
      "Iteration 32, loss = 0.48993266\n",
      "Iteration 33, loss = 0.48664961\n",
      "Iteration 34, loss = 0.48343463\n",
      "Iteration 35, loss = 0.48032438\n",
      "Iteration 36, loss = 0.47722034\n",
      "Iteration 37, loss = 0.47421222\n",
      "Iteration 38, loss = 0.47124939\n",
      "Iteration 39, loss = 0.46831660\n",
      "Iteration 40, loss = 0.46543244\n",
      "Iteration 41, loss = 0.46262499\n",
      "Iteration 42, loss = 0.45989451\n",
      "Iteration 43, loss = 0.45714049\n",
      "Iteration 44, loss = 0.45449529\n",
      "Iteration 45, loss = 0.45183629\n",
      "Iteration 46, loss = 0.44918974\n",
      "Iteration 47, loss = 0.44664207\n",
      "Iteration 48, loss = 0.44409736\n",
      "Iteration 49, loss = 0.44161506\n",
      "Iteration 50, loss = 0.43916395\n",
      "Iteration 51, loss = 0.43677072\n",
      "Iteration 52, loss = 0.43444991\n",
      "Iteration 53, loss = 0.43216965\n",
      "Iteration 54, loss = 0.42993882\n",
      "Iteration 55, loss = 0.42767123\n",
      "Iteration 56, loss = 0.42549613\n",
      "Iteration 57, loss = 0.42336769\n",
      "Iteration 58, loss = 0.42128452\n",
      "Iteration 59, loss = 0.41924962\n",
      "Iteration 60, loss = 0.41723081\n",
      "Iteration 61, loss = 0.41526257\n",
      "Iteration 62, loss = 0.41329414\n",
      "Iteration 63, loss = 0.41142459\n",
      "Iteration 64, loss = 0.40954050\n",
      "Iteration 65, loss = 0.40768542\n",
      "Iteration 66, loss = 0.40588472\n",
      "Iteration 67, loss = 0.40410531\n",
      "Iteration 68, loss = 0.40231610\n",
      "Iteration 69, loss = 0.40055797\n",
      "Iteration 70, loss = 0.39883567\n",
      "Iteration 71, loss = 0.39714447\n",
      "Iteration 72, loss = 0.39548988\n",
      "Iteration 73, loss = 0.39379141\n",
      "Iteration 74, loss = 0.39218273\n",
      "Iteration 75, loss = 0.39055029\n",
      "Iteration 76, loss = 0.38900023\n",
      "Iteration 77, loss = 0.38745005\n",
      "Iteration 78, loss = 0.38595152\n",
      "Iteration 79, loss = 0.38452999\n",
      "Iteration 80, loss = 0.38309379\n",
      "Iteration 81, loss = 0.38170323\n",
      "Iteration 82, loss = 0.38031038\n",
      "Iteration 83, loss = 0.37898309\n",
      "Iteration 84, loss = 0.37763010\n",
      "Iteration 85, loss = 0.37625860\n",
      "Iteration 86, loss = 0.37490297\n",
      "Iteration 87, loss = 0.37359836\n",
      "Iteration 88, loss = 0.37229597\n",
      "Iteration 89, loss = 0.37101703\n",
      "Iteration 90, loss = 0.36977185\n",
      "Iteration 91, loss = 0.36853228\n",
      "Iteration 92, loss = 0.36732678\n",
      "Iteration 93, loss = 0.36614202\n",
      "Iteration 94, loss = 0.36497252\n",
      "Iteration 95, loss = 0.36383446\n",
      "Iteration 96, loss = 0.36270449\n",
      "Iteration 97, loss = 0.36155776\n",
      "Iteration 98, loss = 0.36047340\n",
      "Iteration 99, loss = 0.35939854\n",
      "Iteration 100, loss = 0.35834542\n",
      "Iteration 101, loss = 0.35730470\n",
      "Iteration 102, loss = 0.35630716\n",
      "Iteration 103, loss = 0.35525711\n",
      "Iteration 104, loss = 0.35425867\n",
      "Iteration 105, loss = 0.35325930\n",
      "Iteration 106, loss = 0.35225192\n",
      "Iteration 107, loss = 0.35131555\n",
      "Iteration 108, loss = 0.35034729\n",
      "Iteration 109, loss = 0.34942403\n",
      "Iteration 110, loss = 0.34851765\n",
      "Iteration 111, loss = 0.34761121\n",
      "Iteration 112, loss = 0.34669773\n",
      "Iteration 113, loss = 0.34578722\n",
      "Iteration 114, loss = 0.34490051\n",
      "Iteration 115, loss = 0.34404038\n",
      "Iteration 116, loss = 0.34317981\n",
      "Iteration 117, loss = 0.34232908\n",
      "Iteration 118, loss = 0.34148344\n",
      "Iteration 119, loss = 0.34067522\n",
      "Iteration 120, loss = 0.33986463\n",
      "Iteration 121, loss = 0.33904708\n",
      "Iteration 122, loss = 0.33823065\n",
      "Iteration 123, loss = 0.33744724\n",
      "Iteration 124, loss = 0.33668509\n",
      "Iteration 125, loss = 0.33596340\n",
      "Iteration 126, loss = 0.33525130\n",
      "Iteration 127, loss = 0.33456064\n",
      "Iteration 128, loss = 0.33383680\n",
      "Iteration 129, loss = 0.33312180\n",
      "Iteration 130, loss = 0.33240422\n",
      "Iteration 131, loss = 0.33169352\n",
      "Iteration 132, loss = 0.33099061\n",
      "Iteration 133, loss = 0.33030251\n",
      "Iteration 134, loss = 0.32965261\n",
      "Iteration 135, loss = 0.32898294\n",
      "Iteration 136, loss = 0.32833091\n",
      "Iteration 137, loss = 0.32767523\n",
      "Iteration 138, loss = 0.32703799\n",
      "Iteration 139, loss = 0.32641907\n",
      "Iteration 140, loss = 0.32578653\n",
      "Iteration 141, loss = 0.32521131\n",
      "Iteration 142, loss = 0.32463357\n",
      "Iteration 143, loss = 0.32405765\n",
      "Iteration 144, loss = 0.32351823\n",
      "Iteration 145, loss = 0.32294589\n",
      "Iteration 146, loss = 0.32240374\n",
      "Iteration 147, loss = 0.32184748\n",
      "Iteration 148, loss = 0.32131606\n",
      "Iteration 149, loss = 0.32077168\n",
      "Iteration 150, loss = 0.32024717\n",
      "Iteration 151, loss = 0.31971386\n",
      "Iteration 152, loss = 0.31921692\n",
      "Iteration 153, loss = 0.31869119\n",
      "Iteration 154, loss = 0.31819330\n",
      "Iteration 155, loss = 0.31769072\n",
      "Iteration 156, loss = 0.31718637\n",
      "Iteration 157, loss = 0.31667040\n",
      "Iteration 158, loss = 0.31617748\n",
      "Iteration 159, loss = 0.31564391\n",
      "Iteration 160, loss = 0.31514354\n",
      "Iteration 161, loss = 0.31464820\n",
      "Iteration 162, loss = 0.31416429\n",
      "Iteration 163, loss = 0.31369001\n",
      "Iteration 164, loss = 0.31320444\n",
      "Iteration 165, loss = 0.31273234\n",
      "Iteration 166, loss = 0.31223414\n",
      "Iteration 167, loss = 0.31179464\n",
      "Iteration 168, loss = 0.31134163\n",
      "Iteration 169, loss = 0.31090279\n",
      "Iteration 170, loss = 0.31049087\n",
      "Iteration 171, loss = 0.31005755\n",
      "Iteration 172, loss = 0.30962489\n",
      "Iteration 173, loss = 0.30924238\n",
      "Iteration 174, loss = 0.30883937\n",
      "Iteration 175, loss = 0.30843969\n",
      "Iteration 176, loss = 0.30805592\n",
      "Iteration 177, loss = 0.30766676\n",
      "Iteration 178, loss = 0.30730211\n",
      "Iteration 179, loss = 0.30692782\n",
      "Iteration 180, loss = 0.30656469\n",
      "Iteration 181, loss = 0.30619909\n",
      "Iteration 182, loss = 0.30581185\n",
      "Iteration 183, loss = 0.30543625\n",
      "Iteration 184, loss = 0.30506589\n",
      "Iteration 185, loss = 0.30470612\n",
      "Iteration 186, loss = 0.30436287\n",
      "Iteration 187, loss = 0.30400363\n",
      "Iteration 188, loss = 0.30365395\n",
      "Iteration 189, loss = 0.30331714\n",
      "Iteration 190, loss = 0.30297840\n",
      "Iteration 191, loss = 0.30265729\n",
      "Iteration 192, loss = 0.30233216\n",
      "Iteration 193, loss = 0.30202050\n",
      "Iteration 194, loss = 0.30170352\n",
      "Iteration 195, loss = 0.30138637\n",
      "Iteration 196, loss = 0.30108620\n",
      "Iteration 197, loss = 0.30077528\n",
      "Iteration 198, loss = 0.30047410\n",
      "Iteration 199, loss = 0.30018498\n",
      "Iteration 200, loss = 0.29987847\n",
      "Iteration 201, loss = 0.29959285\n",
      "Iteration 202, loss = 0.29929260\n",
      "Iteration 203, loss = 0.29898538\n",
      "Iteration 204, loss = 0.29868667\n",
      "Iteration 205, loss = 0.29839829\n",
      "Iteration 206, loss = 0.29811101\n",
      "Iteration 207, loss = 0.29781959\n",
      "Iteration 208, loss = 0.29755358\n",
      "Iteration 209, loss = 0.29726145\n",
      "Iteration 210, loss = 0.29697535\n",
      "Iteration 211, loss = 0.29668910\n",
      "Iteration 212, loss = 0.29642257\n",
      "Iteration 213, loss = 0.29616756\n",
      "Iteration 214, loss = 0.29591109\n",
      "Iteration 215, loss = 0.29564919\n",
      "Iteration 216, loss = 0.29539004\n",
      "Iteration 217, loss = 0.29512117\n",
      "Iteration 218, loss = 0.29486379\n",
      "Iteration 219, loss = 0.29460789\n",
      "Iteration 220, loss = 0.29434499\n",
      "Iteration 221, loss = 0.29409312\n",
      "Iteration 222, loss = 0.29384895\n",
      "Iteration 223, loss = 0.29359971\n",
      "Iteration 224, loss = 0.29335527\n",
      "Iteration 225, loss = 0.29310606\n",
      "Iteration 226, loss = 0.29287251\n",
      "Iteration 227, loss = 0.29263282\n",
      "Iteration 228, loss = 0.29240585\n",
      "Iteration 229, loss = 0.29217312\n",
      "Iteration 230, loss = 0.29194444\n",
      "Iteration 231, loss = 0.29173712\n",
      "Iteration 232, loss = 0.29152052\n",
      "Iteration 233, loss = 0.29128727\n",
      "Iteration 234, loss = 0.29108231\n",
      "Iteration 235, loss = 0.29087166\n",
      "Iteration 236, loss = 0.29067600\n",
      "Iteration 237, loss = 0.29045852\n",
      "Iteration 238, loss = 0.29026874\n",
      "Iteration 239, loss = 0.29005650\n",
      "Iteration 240, loss = 0.28984929\n",
      "Iteration 241, loss = 0.28964303\n",
      "Iteration 242, loss = 0.28943659\n",
      "Iteration 243, loss = 0.28923984\n",
      "Iteration 244, loss = 0.28903550\n",
      "Iteration 245, loss = 0.28885890\n",
      "Iteration 246, loss = 0.28867429\n",
      "Iteration 247, loss = 0.28849002\n",
      "Iteration 248, loss = 0.28830628\n",
      "Iteration 249, loss = 0.28811971\n",
      "Iteration 250, loss = 0.28794307\n",
      "Iteration 251, loss = 0.28775846\n",
      "Iteration 252, loss = 0.28757223\n",
      "Iteration 253, loss = 0.28740430\n",
      "Iteration 254, loss = 0.28723204\n",
      "Iteration 255, loss = 0.28706489\n",
      "Iteration 256, loss = 0.28690601\n",
      "Iteration 257, loss = 0.28674333\n",
      "Iteration 258, loss = 0.28659326\n",
      "Iteration 259, loss = 0.28644024\n",
      "Iteration 260, loss = 0.28629075\n",
      "Iteration 261, loss = 0.28614511\n",
      "Iteration 262, loss = 0.28598023\n",
      "Iteration 263, loss = 0.28582740\n",
      "Iteration 264, loss = 0.28566475\n",
      "Iteration 265, loss = 0.28550637\n",
      "Iteration 266, loss = 0.28535022\n",
      "Iteration 267, loss = 0.28519510\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 268, loss = 0.28503658\n",
      "Iteration 269, loss = 0.28488684\n",
      "Iteration 270, loss = 0.28473724\n",
      "Iteration 271, loss = 0.28458590\n",
      "Iteration 272, loss = 0.28443608\n",
      "Iteration 273, loss = 0.28427822\n",
      "Iteration 274, loss = 0.28411753\n",
      "Iteration 275, loss = 0.28398095\n",
      "Iteration 276, loss = 0.28382336\n",
      "Iteration 277, loss = 0.28367474\n",
      "Iteration 278, loss = 0.28353665\n",
      "Iteration 279, loss = 0.28338071\n",
      "Iteration 280, loss = 0.28324432\n",
      "Iteration 281, loss = 0.28310635\n",
      "Iteration 282, loss = 0.28296518\n",
      "Iteration 283, loss = 0.28282043\n",
      "Iteration 284, loss = 0.28267206\n",
      "Iteration 285, loss = 0.28253338\n",
      "Iteration 286, loss = 0.28241827\n",
      "Iteration 287, loss = 0.28227726\n",
      "Iteration 288, loss = 0.28214967\n",
      "Iteration 289, loss = 0.28202758\n",
      "Iteration 290, loss = 0.28189496\n",
      "Iteration 291, loss = 0.28175940\n",
      "Iteration 292, loss = 0.28163765\n",
      "Iteration 293, loss = 0.28151466\n",
      "Iteration 294, loss = 0.28137776\n",
      "Iteration 295, loss = 0.28128086\n",
      "Iteration 296, loss = 0.28114859\n",
      "Iteration 297, loss = 0.28105016\n",
      "Iteration 298, loss = 0.28094062\n",
      "Iteration 299, loss = 0.28082857\n",
      "Iteration 300, loss = 0.28071856\n",
      "Iteration 301, loss = 0.28060968\n",
      "Iteration 302, loss = 0.28050097\n",
      "Iteration 303, loss = 0.28038676\n",
      "Iteration 304, loss = 0.28026979\n",
      "Iteration 305, loss = 0.28016893\n",
      "Iteration 306, loss = 0.28005330\n",
      "Iteration 307, loss = 0.27994166\n",
      "Iteration 308, loss = 0.27983254\n",
      "Iteration 309, loss = 0.27972317\n",
      "Iteration 310, loss = 0.27962437\n",
      "Iteration 311, loss = 0.27950414\n",
      "Iteration 312, loss = 0.27940762\n",
      "Iteration 313, loss = 0.27930736\n",
      "Iteration 314, loss = 0.27919774\n",
      "Iteration 315, loss = 0.27910563\n",
      "Iteration 316, loss = 0.27901624\n",
      "Iteration 317, loss = 0.27891634\n",
      "Iteration 318, loss = 0.27882642\n",
      "Iteration 319, loss = 0.27872587\n",
      "Iteration 320, loss = 0.27863176\n",
      "Iteration 321, loss = 0.27853786\n",
      "Iteration 322, loss = 0.27843473\n",
      "Iteration 323, loss = 0.27834078\n",
      "Iteration 324, loss = 0.27823824\n",
      "Iteration 325, loss = 0.27814183\n",
      "Iteration 326, loss = 0.27804883\n",
      "Iteration 327, loss = 0.27796131\n",
      "Iteration 328, loss = 0.27787521\n",
      "Iteration 329, loss = 0.27778606\n",
      "Iteration 330, loss = 0.27769212\n",
      "Iteration 331, loss = 0.27760314\n",
      "Iteration 332, loss = 0.27751239\n",
      "Iteration 333, loss = 0.27743434\n",
      "Iteration 334, loss = 0.27734977\n",
      "Iteration 335, loss = 0.27726631\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "The training time of the model: 0.56 .\n",
      "accuracy: 0.89375\n",
      "f1: 0.0\n"
     ]
    }
   ],
   "source": [
    "#repeat model H = 1 multiple times\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(1, ), solver='sgd', \n",
    "                     max_iter=500, random_state=1, verbose=True)\n",
    "start = time()\n",
    "mlp.fit(X_train, y_train)\n",
    "NN_time = time() - start\n",
    "y_hat = mlp.predict(X_test)\n",
    "print(\"The training time of the model: %.2f .\"\n",
    "      % (NN_time))\n",
    "print('accuracy:', accuracy_score(y_test, y_hat))\n",
    "print('f1:', f1_score(y_test, y_hat))\n",
    "setting1_time2 = NN_time\n",
    "setting1_acc2 = accuracy_score(y_test, y_hat)\n",
    "setting1_f12 = f1_score(y_test, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.59399803\n",
      "Iteration 2, loss = 0.59199687\n",
      "Iteration 3, loss = 0.58886964\n",
      "Iteration 4, loss = 0.58507170\n",
      "Iteration 5, loss = 0.58080805\n",
      "Iteration 6, loss = 0.57638199\n",
      "Iteration 7, loss = 0.57180208\n",
      "Iteration 8, loss = 0.56716726\n",
      "Iteration 9, loss = 0.56251133\n",
      "Iteration 10, loss = 0.55791695\n",
      "Iteration 11, loss = 0.55347222\n",
      "Iteration 12, loss = 0.54899461\n",
      "Iteration 13, loss = 0.54463441\n",
      "Iteration 14, loss = 0.54029270\n",
      "Iteration 15, loss = 0.53603334\n",
      "Iteration 16, loss = 0.53189052\n",
      "Iteration 17, loss = 0.52783404\n",
      "Iteration 18, loss = 0.52382791\n",
      "Iteration 19, loss = 0.51995576\n",
      "Iteration 20, loss = 0.51616916\n",
      "Iteration 21, loss = 0.51240956\n",
      "Iteration 22, loss = 0.50870552\n",
      "Iteration 23, loss = 0.50507765\n",
      "Iteration 24, loss = 0.50147994\n",
      "Iteration 25, loss = 0.49791951\n",
      "Iteration 26, loss = 0.49450325\n",
      "Iteration 27, loss = 0.49111971\n",
      "Iteration 28, loss = 0.48777787\n",
      "Iteration 29, loss = 0.48449228\n",
      "Iteration 30, loss = 0.48127325\n",
      "Iteration 31, loss = 0.47812346\n",
      "Iteration 32, loss = 0.47498112\n",
      "Iteration 33, loss = 0.47195473\n",
      "Iteration 34, loss = 0.46893768\n",
      "Iteration 35, loss = 0.46604229\n",
      "Iteration 36, loss = 0.46318733\n",
      "Iteration 37, loss = 0.46041976\n",
      "Iteration 38, loss = 0.45763828\n",
      "Iteration 39, loss = 0.45491292\n",
      "Iteration 40, loss = 0.45223940\n",
      "Iteration 41, loss = 0.44959111\n",
      "Iteration 42, loss = 0.44697258\n",
      "Iteration 43, loss = 0.44447953\n",
      "Iteration 44, loss = 0.44197653\n",
      "Iteration 45, loss = 0.43949696\n",
      "Iteration 46, loss = 0.43714112\n",
      "Iteration 47, loss = 0.43480604\n",
      "Iteration 48, loss = 0.43245167\n",
      "Iteration 49, loss = 0.43017203\n",
      "Iteration 50, loss = 0.42791177\n",
      "Iteration 51, loss = 0.42566586\n",
      "Iteration 52, loss = 0.42345847\n",
      "Iteration 53, loss = 0.42130433\n",
      "Iteration 54, loss = 0.41923202\n",
      "Iteration 55, loss = 0.41720171\n",
      "Iteration 56, loss = 0.41518910\n",
      "Iteration 57, loss = 0.41324466\n",
      "Iteration 58, loss = 0.41124498\n",
      "Iteration 59, loss = 0.40927137\n",
      "Iteration 60, loss = 0.40744845\n",
      "Iteration 61, loss = 0.40555937\n",
      "Iteration 62, loss = 0.40375265\n",
      "Iteration 63, loss = 0.40196525\n",
      "Iteration 64, loss = 0.40020180\n",
      "Iteration 65, loss = 0.39843837\n",
      "Iteration 66, loss = 0.39672396\n",
      "Iteration 67, loss = 0.39505239\n",
      "Iteration 68, loss = 0.39334200\n",
      "Iteration 69, loss = 0.39167242\n",
      "Iteration 70, loss = 0.39005236\n",
      "Iteration 71, loss = 0.38846802\n",
      "Iteration 72, loss = 0.38697359\n",
      "Iteration 73, loss = 0.38545238\n",
      "Iteration 74, loss = 0.38395004\n",
      "Iteration 75, loss = 0.38248793\n",
      "Iteration 76, loss = 0.38102737\n",
      "Iteration 77, loss = 0.37960684\n",
      "Iteration 78, loss = 0.37819448\n",
      "Iteration 79, loss = 0.37684454\n",
      "Iteration 80, loss = 0.37549122\n",
      "Iteration 81, loss = 0.37420908\n",
      "Iteration 82, loss = 0.37287754\n",
      "Iteration 83, loss = 0.37161765\n",
      "Iteration 84, loss = 0.37034594\n",
      "Iteration 85, loss = 0.36912977\n",
      "Iteration 86, loss = 0.36788574\n",
      "Iteration 87, loss = 0.36667180\n",
      "Iteration 88, loss = 0.36552409\n",
      "Iteration 89, loss = 0.36432670\n",
      "Iteration 90, loss = 0.36315065\n",
      "Iteration 91, loss = 0.36206101\n",
      "Iteration 92, loss = 0.36095655\n",
      "Iteration 93, loss = 0.35986607\n",
      "Iteration 94, loss = 0.35882086\n",
      "Iteration 95, loss = 0.35778913\n",
      "Iteration 96, loss = 0.35675718\n",
      "Iteration 97, loss = 0.35578459\n",
      "Iteration 98, loss = 0.35478794\n",
      "Iteration 99, loss = 0.35379788\n",
      "Iteration 100, loss = 0.35284102\n",
      "Iteration 101, loss = 0.35186086\n",
      "Iteration 102, loss = 0.35087565\n",
      "Iteration 103, loss = 0.34991801\n",
      "Iteration 104, loss = 0.34895134\n",
      "Iteration 105, loss = 0.34798894\n",
      "Iteration 106, loss = 0.34708568\n",
      "Iteration 107, loss = 0.34615471\n",
      "Iteration 108, loss = 0.34524311\n",
      "Iteration 109, loss = 0.34433982\n",
      "Iteration 110, loss = 0.34346798\n",
      "Iteration 111, loss = 0.34258974\n",
      "Iteration 112, loss = 0.34174172\n",
      "Iteration 113, loss = 0.34090191\n",
      "Iteration 114, loss = 0.34008325\n",
      "Iteration 115, loss = 0.33927847\n",
      "Iteration 116, loss = 0.33849266\n",
      "Iteration 117, loss = 0.33773305\n",
      "Iteration 118, loss = 0.33697839\n",
      "Iteration 119, loss = 0.33625442\n",
      "Iteration 120, loss = 0.33553022\n",
      "Iteration 121, loss = 0.33479914\n",
      "Iteration 122, loss = 0.33406327\n",
      "Iteration 123, loss = 0.33334037\n",
      "Iteration 124, loss = 0.33263680\n",
      "Iteration 125, loss = 0.33192894\n",
      "Iteration 126, loss = 0.33122702\n",
      "Iteration 127, loss = 0.33060240\n",
      "Iteration 128, loss = 0.32995878\n",
      "Iteration 129, loss = 0.32932821\n",
      "Iteration 130, loss = 0.32870091\n",
      "Iteration 131, loss = 0.32806391\n",
      "Iteration 132, loss = 0.32742151\n",
      "Iteration 133, loss = 0.32680003\n",
      "Iteration 134, loss = 0.32618054\n",
      "Iteration 135, loss = 0.32557178\n",
      "Iteration 136, loss = 0.32498366\n",
      "Iteration 137, loss = 0.32436990\n",
      "Iteration 138, loss = 0.32379449\n",
      "Iteration 139, loss = 0.32317907\n",
      "Iteration 140, loss = 0.32256909\n",
      "Iteration 141, loss = 0.32198263\n",
      "Iteration 142, loss = 0.32140781\n",
      "Iteration 143, loss = 0.32083254\n",
      "Iteration 144, loss = 0.32025590\n",
      "Iteration 145, loss = 0.31968317\n",
      "Iteration 146, loss = 0.31914830\n",
      "Iteration 147, loss = 0.31861453\n",
      "Iteration 148, loss = 0.31810677\n",
      "Iteration 149, loss = 0.31758197\n",
      "Iteration 150, loss = 0.31709998\n",
      "Iteration 151, loss = 0.31659212\n",
      "Iteration 152, loss = 0.31611337\n",
      "Iteration 153, loss = 0.31561718\n",
      "Iteration 154, loss = 0.31515336\n",
      "Iteration 155, loss = 0.31469664\n",
      "Iteration 156, loss = 0.31423233\n",
      "Iteration 157, loss = 0.31377245\n",
      "Iteration 158, loss = 0.31333045\n",
      "Iteration 159, loss = 0.31288322\n",
      "Iteration 160, loss = 0.31242604\n",
      "Iteration 161, loss = 0.31198222\n",
      "Iteration 162, loss = 0.31153032\n",
      "Iteration 163, loss = 0.31112119\n",
      "Iteration 164, loss = 0.31069219\n",
      "Iteration 165, loss = 0.31028642\n",
      "Iteration 166, loss = 0.30985868\n",
      "Iteration 167, loss = 0.30942979\n",
      "Iteration 168, loss = 0.30902125\n",
      "Iteration 169, loss = 0.30860465\n",
      "Iteration 170, loss = 0.30818013\n",
      "Iteration 171, loss = 0.30776954\n",
      "Iteration 172, loss = 0.30740117\n",
      "Iteration 173, loss = 0.30701317\n",
      "Iteration 174, loss = 0.30665839\n",
      "Iteration 175, loss = 0.30630040\n",
      "Iteration 176, loss = 0.30592051\n",
      "Iteration 177, loss = 0.30557119\n",
      "Iteration 178, loss = 0.30520286\n",
      "Iteration 179, loss = 0.30484736\n",
      "Iteration 180, loss = 0.30448213\n",
      "Iteration 181, loss = 0.30409935\n",
      "Iteration 182, loss = 0.30374768\n",
      "Iteration 183, loss = 0.30341787\n",
      "Iteration 184, loss = 0.30306616\n",
      "Iteration 185, loss = 0.30274730\n",
      "Iteration 186, loss = 0.30242468\n",
      "Iteration 187, loss = 0.30208558\n",
      "Iteration 188, loss = 0.30176016\n",
      "Iteration 189, loss = 0.30145361\n",
      "Iteration 190, loss = 0.30110613\n",
      "Iteration 191, loss = 0.30078327\n",
      "Iteration 192, loss = 0.30048327\n",
      "Iteration 193, loss = 0.30015555\n",
      "Iteration 194, loss = 0.29983616\n",
      "Iteration 195, loss = 0.29950337\n",
      "Iteration 196, loss = 0.29918686\n",
      "Iteration 197, loss = 0.29889077\n",
      "Iteration 198, loss = 0.29857922\n",
      "Iteration 199, loss = 0.29828070\n",
      "Iteration 200, loss = 0.29799923\n",
      "Iteration 201, loss = 0.29770972\n",
      "Iteration 202, loss = 0.29742629\n",
      "Iteration 203, loss = 0.29715289\n",
      "Iteration 204, loss = 0.29686768\n",
      "Iteration 205, loss = 0.29658458\n",
      "Iteration 206, loss = 0.29629206\n",
      "Iteration 207, loss = 0.29602532\n",
      "Iteration 208, loss = 0.29573878\n",
      "Iteration 209, loss = 0.29548421\n",
      "Iteration 210, loss = 0.29522057\n",
      "Iteration 211, loss = 0.29497935\n",
      "Iteration 212, loss = 0.29473609\n",
      "Iteration 213, loss = 0.29450280\n",
      "Iteration 214, loss = 0.29425502\n",
      "Iteration 215, loss = 0.29401928\n",
      "Iteration 216, loss = 0.29379615\n",
      "Iteration 217, loss = 0.29356641\n",
      "Iteration 218, loss = 0.29334855\n",
      "Iteration 219, loss = 0.29311915\n",
      "Iteration 220, loss = 0.29289703\n",
      "Iteration 221, loss = 0.29268063\n",
      "Iteration 222, loss = 0.29247976\n",
      "Iteration 223, loss = 0.29226394\n",
      "Iteration 224, loss = 0.29204406\n",
      "Iteration 225, loss = 0.29185027\n",
      "Iteration 226, loss = 0.29162106\n",
      "Iteration 227, loss = 0.29142593\n",
      "Iteration 228, loss = 0.29121544\n",
      "Iteration 229, loss = 0.29101322\n",
      "Iteration 230, loss = 0.29080541\n",
      "Iteration 231, loss = 0.29060834\n",
      "Iteration 232, loss = 0.29040364\n",
      "Iteration 233, loss = 0.29019657\n",
      "Iteration 234, loss = 0.28998288\n",
      "Iteration 235, loss = 0.28979657\n",
      "Iteration 236, loss = 0.28959040\n",
      "Iteration 237, loss = 0.28940754\n",
      "Iteration 238, loss = 0.28920481\n",
      "Iteration 239, loss = 0.28901255\n",
      "Iteration 240, loss = 0.28883247\n",
      "Iteration 241, loss = 0.28864004\n",
      "Iteration 242, loss = 0.28845068\n",
      "Iteration 243, loss = 0.28826827\n",
      "Iteration 244, loss = 0.28807590\n",
      "Iteration 245, loss = 0.28788577\n",
      "Iteration 246, loss = 0.28770092\n",
      "Iteration 247, loss = 0.28751789\n",
      "Iteration 248, loss = 0.28734238\n",
      "Iteration 249, loss = 0.28717519\n",
      "Iteration 250, loss = 0.28700169\n",
      "Iteration 251, loss = 0.28682550\n",
      "Iteration 252, loss = 0.28664877\n",
      "Iteration 253, loss = 0.28647738\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 254, loss = 0.28630591\n",
      "Iteration 255, loss = 0.28612653\n",
      "Iteration 256, loss = 0.28595542\n",
      "Iteration 257, loss = 0.28576422\n",
      "Iteration 258, loss = 0.28559343\n",
      "Iteration 259, loss = 0.28542798\n",
      "Iteration 260, loss = 0.28527406\n",
      "Iteration 261, loss = 0.28512481\n",
      "Iteration 262, loss = 0.28497571\n",
      "Iteration 263, loss = 0.28482177\n",
      "Iteration 264, loss = 0.28467105\n",
      "Iteration 265, loss = 0.28452993\n",
      "Iteration 266, loss = 0.28437866\n",
      "Iteration 267, loss = 0.28425259\n",
      "Iteration 268, loss = 0.28411515\n",
      "Iteration 269, loss = 0.28396182\n",
      "Iteration 270, loss = 0.28381871\n",
      "Iteration 271, loss = 0.28369036\n",
      "Iteration 272, loss = 0.28355681\n",
      "Iteration 273, loss = 0.28344436\n",
      "Iteration 274, loss = 0.28330173\n",
      "Iteration 275, loss = 0.28317507\n",
      "Iteration 276, loss = 0.28304413\n",
      "Iteration 277, loss = 0.28291655\n",
      "Iteration 278, loss = 0.28279117\n",
      "Iteration 279, loss = 0.28265954\n",
      "Iteration 280, loss = 0.28253379\n",
      "Iteration 281, loss = 0.28239876\n",
      "Iteration 282, loss = 0.28228186\n",
      "Iteration 283, loss = 0.28215681\n",
      "Iteration 284, loss = 0.28203492\n",
      "Iteration 285, loss = 0.28191204\n",
      "Iteration 286, loss = 0.28179233\n",
      "Iteration 287, loss = 0.28166977\n",
      "Iteration 288, loss = 0.28153493\n",
      "Iteration 289, loss = 0.28141500\n",
      "Iteration 290, loss = 0.28128783\n",
      "Iteration 291, loss = 0.28117701\n",
      "Iteration 292, loss = 0.28106217\n",
      "Iteration 293, loss = 0.28094137\n",
      "Iteration 294, loss = 0.28082788\n",
      "Iteration 295, loss = 0.28070692\n",
      "Iteration 296, loss = 0.28059659\n",
      "Iteration 297, loss = 0.28047873\n",
      "Iteration 298, loss = 0.28036586\n",
      "Iteration 299, loss = 0.28025512\n",
      "Iteration 300, loss = 0.28013410\n",
      "Iteration 301, loss = 0.28002182\n",
      "Iteration 302, loss = 0.27990649\n",
      "Iteration 303, loss = 0.27979046\n",
      "Iteration 304, loss = 0.27968403\n",
      "Iteration 305, loss = 0.27957861\n",
      "Iteration 306, loss = 0.27948916\n",
      "Iteration 307, loss = 0.27939797\n",
      "Iteration 308, loss = 0.27930386\n",
      "Iteration 309, loss = 0.27920276\n",
      "Iteration 310, loss = 0.27911153\n",
      "Iteration 311, loss = 0.27901287\n",
      "Iteration 312, loss = 0.27893029\n",
      "Iteration 313, loss = 0.27884873\n",
      "Iteration 314, loss = 0.27876893\n",
      "Iteration 315, loss = 0.27867858\n",
      "Iteration 316, loss = 0.27859170\n",
      "Iteration 317, loss = 0.27850606\n",
      "Iteration 318, loss = 0.27842060\n",
      "Iteration 319, loss = 0.27833450\n",
      "Iteration 320, loss = 0.27825460\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "The training time of the model: 0.54 .\n",
      "accuracy: 0.89375\n",
      "f1: 0.0\n",
      "The mean training time of the model H = 1: 0.58144625027974450 (std: 0.04624578322252040) .\n",
      "The mean accuracy time of the model H = 1: 0.89375000000000016 (std: 0.00000000000000011) .\n",
      "The mean F1 score time of the model H = 1: 0.00000000000000000 (std: 0.00000000000000000) .\n"
     ]
    }
   ],
   "source": [
    "#repeat model H = 1 multiple times\n",
    "#report the mean and standard deviation of the training time, accuracy, and F1 score for the setting.\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(1, ), solver='sgd', \n",
    "                     max_iter=500, random_state=2, verbose=True)\n",
    "start = time()\n",
    "mlp.fit(X_train, y_train)\n",
    "NN_time = time() - start\n",
    "y_hat = mlp.predict(X_test)\n",
    "print(\"The training time of the model: %.2f .\"\n",
    "      % (NN_time))\n",
    "print('accuracy:', accuracy_score(y_test, y_hat))\n",
    "print('f1:', f1_score(y_test, y_hat))\n",
    "setting1_time3 = NN_time\n",
    "setting1_acc3 = accuracy_score(y_test, y_hat)\n",
    "setting1_f13 = f1_score(y_test, y_hat)\n",
    "set1_time = np.array([setting1_time1, setting1_time2, setting1_time3])\n",
    "mean_time = np.mean(set1_time)\n",
    "std_time = np.std(set1_time)\n",
    "set1_acc = np.array([setting1_acc1, setting1_acc2, setting1_acc3])\n",
    "mean1_acc = np.mean(set1_acc)\n",
    "std_acc = np.std(set1_acc)\n",
    "set1_f1 = np.array([setting1_f1, setting1_f12, setting1_f13])\n",
    "mean1_f1 = np.mean(set1_f1)\n",
    "std_f1 = np.std(set1_f1)\n",
    "print(\"The mean training time of the model H = 1: %.17f (std: %.17f) .\"\n",
    "      % (mean_time, std_time))\n",
    "print(\"The mean accuracy time of the model H = 1: %.17f (std: %.17f) .\"\n",
    "      % (mean1_acc, std_acc))\n",
    "print(\"The mean F1 score time of the model H = 1: %.17f (std: %.17f) .\"\n",
    "      % (mean1_f1, std_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = inf\n",
      "Iteration 2, loss = 0.49467050\n",
      "Iteration 3, loss = 0.50072489\n",
      "Iteration 4, loss = 0.49807756\n",
      "Iteration 5, loss = 0.49485297\n",
      "Iteration 6, loss = 0.49160170\n",
      "Iteration 7, loss = 0.48832542\n",
      "Iteration 8, loss = 0.48512662\n",
      "Iteration 9, loss = 0.48195882\n",
      "Iteration 10, loss = 0.47886734\n",
      "Iteration 11, loss = 0.47583932\n",
      "Iteration 12, loss = 0.47275475\n",
      "Iteration 13, loss = 0.46975843\n",
      "Iteration 14, loss = 0.46687462\n",
      "Iteration 15, loss = 0.46396511\n",
      "Iteration 16, loss = 0.46119997\n",
      "Iteration 17, loss = 0.45840930\n",
      "Iteration 18, loss = 0.45568144\n",
      "Iteration 19, loss = 0.45303784\n",
      "Iteration 20, loss = 0.45041810\n",
      "Iteration 21, loss = 0.44787793\n",
      "Iteration 22, loss = 0.44539725\n",
      "Iteration 23, loss = 0.44292624\n",
      "Iteration 24, loss = 0.44053494\n",
      "Iteration 25, loss = 0.43814540\n",
      "Iteration 26, loss = 0.43573898\n",
      "Iteration 27, loss = 0.43340645\n",
      "Iteration 28, loss = 0.43104237\n",
      "Iteration 29, loss = 0.42881245\n",
      "Iteration 30, loss = 0.42655376\n",
      "Iteration 31, loss = 0.42438443\n",
      "Iteration 32, loss = 0.42223071\n",
      "Iteration 33, loss = 0.42012744\n",
      "Iteration 34, loss = 0.41807994\n",
      "Iteration 35, loss = 0.41607268\n",
      "Iteration 36, loss = 0.41410318\n",
      "Iteration 37, loss = 0.41211692\n",
      "Iteration 38, loss = 0.41015688\n",
      "Iteration 39, loss = 0.40828547\n",
      "Iteration 40, loss = 0.40641554\n",
      "Iteration 41, loss = 0.40459975\n",
      "Iteration 42, loss = 0.40282843\n",
      "Iteration 43, loss = 0.40107475\n",
      "Iteration 44, loss = 0.39938345\n",
      "Iteration 45, loss = 0.39763707\n",
      "Iteration 46, loss = 0.39598540\n",
      "Iteration 47, loss = 0.39432436\n",
      "Iteration 48, loss = 0.39267245\n",
      "Iteration 49, loss = 0.39111137\n",
      "Iteration 50, loss = 0.38948150\n",
      "Iteration 51, loss = 0.38792369\n",
      "Iteration 52, loss = 0.38636015\n",
      "Iteration 53, loss = 0.38485228\n",
      "Iteration 54, loss = 0.38334730\n",
      "Iteration 55, loss = 0.38190029\n",
      "Iteration 56, loss = 0.38047693\n",
      "Iteration 57, loss = 0.37909429\n",
      "Iteration 58, loss = 0.37769352\n",
      "Iteration 59, loss = 0.37631252\n",
      "Iteration 60, loss = 0.37501997\n",
      "Iteration 61, loss = 0.37372634\n",
      "Iteration 62, loss = 0.37245819\n",
      "Iteration 63, loss = 0.37122541\n",
      "Iteration 64, loss = 0.36999453\n",
      "Iteration 65, loss = 0.36876282\n",
      "Iteration 66, loss = 0.36756140\n",
      "Iteration 67, loss = 0.36635676\n",
      "Iteration 68, loss = 0.36520008\n",
      "Iteration 69, loss = 0.36404916\n",
      "Iteration 70, loss = 0.36291097\n",
      "Iteration 71, loss = 0.36172724\n",
      "Iteration 72, loss = 0.36061762\n",
      "Iteration 73, loss = 0.35947595\n",
      "Iteration 74, loss = 0.35839434\n",
      "Iteration 75, loss = 0.35735791\n",
      "Iteration 76, loss = 0.35634476\n",
      "Iteration 77, loss = 0.35535025\n",
      "Iteration 78, loss = 0.35434387\n",
      "Iteration 79, loss = 0.35335420\n",
      "Iteration 80, loss = 0.35238766\n",
      "Iteration 81, loss = 0.35140991\n",
      "Iteration 82, loss = 0.35045596\n",
      "Iteration 83, loss = 0.34951312\n",
      "Iteration 84, loss = 0.34855894\n",
      "Iteration 85, loss = 0.34763007\n",
      "Iteration 86, loss = 0.34670295\n",
      "Iteration 87, loss = 0.34579458\n",
      "Iteration 88, loss = 0.34490827\n",
      "Iteration 89, loss = 0.34405657\n",
      "Iteration 90, loss = 0.34319629\n",
      "Iteration 91, loss = 0.34236659\n",
      "Iteration 92, loss = 0.34155349\n",
      "Iteration 93, loss = 0.34074328\n",
      "Iteration 94, loss = 0.33997439\n",
      "Iteration 95, loss = 0.33912554\n",
      "Iteration 96, loss = 0.33833431\n",
      "Iteration 97, loss = 0.33752045\n",
      "Iteration 98, loss = 0.33670394\n",
      "Iteration 99, loss = 0.33593300\n",
      "Iteration 100, loss = 0.33514263\n",
      "Iteration 101, loss = 0.33436872\n",
      "Iteration 102, loss = 0.33360666\n",
      "Iteration 103, loss = 0.33287781\n",
      "Iteration 104, loss = 0.33214880\n",
      "Iteration 105, loss = 0.33144854\n",
      "Iteration 106, loss = 0.33073032\n",
      "Iteration 107, loss = 0.33001057\n",
      "Iteration 108, loss = 0.32934804\n",
      "Iteration 109, loss = 0.32864080\n",
      "Iteration 110, loss = 0.32800857\n",
      "Iteration 111, loss = 0.32735855\n",
      "Iteration 112, loss = 0.32676120\n",
      "Iteration 113, loss = 0.32616711\n",
      "Iteration 114, loss = 0.32558957\n",
      "Iteration 115, loss = 0.32498888\n",
      "Iteration 116, loss = 0.32441503\n",
      "Iteration 117, loss = 0.32385147\n",
      "Iteration 118, loss = 0.32325604\n",
      "Iteration 119, loss = 0.32268148\n",
      "Iteration 120, loss = 0.32212827\n",
      "Iteration 121, loss = 0.32157821\n",
      "Iteration 122, loss = 0.32102369\n",
      "Iteration 123, loss = 0.32050499\n",
      "Iteration 124, loss = 0.31994737\n",
      "Iteration 125, loss = 0.31942574\n",
      "Iteration 126, loss = 0.31888457\n",
      "Iteration 127, loss = 0.31836786\n",
      "Iteration 128, loss = 0.31784149\n",
      "Iteration 129, loss = 0.31731763\n",
      "Iteration 130, loss = 0.31679647\n",
      "Iteration 131, loss = 0.31628703\n",
      "Iteration 132, loss = 0.31578121\n",
      "Iteration 133, loss = 0.31528912\n",
      "Iteration 134, loss = 0.31479756\n",
      "Iteration 135, loss = 0.31434163\n",
      "Iteration 136, loss = 0.31386825\n",
      "Iteration 137, loss = 0.31343021\n",
      "Iteration 138, loss = 0.31296626\n",
      "Iteration 139, loss = 0.31251826\n",
      "Iteration 140, loss = 0.31209282\n",
      "Iteration 141, loss = 0.31166017\n",
      "Iteration 142, loss = 0.31125582\n",
      "Iteration 143, loss = 0.31081598\n",
      "Iteration 144, loss = 0.31041345\n",
      "Iteration 145, loss = 0.30999980\n",
      "Iteration 146, loss = 0.30958524\n",
      "Iteration 147, loss = 0.30916920\n",
      "Iteration 148, loss = 0.30876591\n",
      "Iteration 149, loss = 0.30834273\n",
      "Iteration 150, loss = 0.30791682\n",
      "Iteration 151, loss = 0.30750917\n",
      "Iteration 152, loss = 0.30712661\n",
      "Iteration 153, loss = 0.30669457\n",
      "Iteration 154, loss = 0.30632166\n",
      "Iteration 155, loss = 0.30595281\n",
      "Iteration 156, loss = 0.30559031\n",
      "Iteration 157, loss = 0.30521667\n",
      "Iteration 158, loss = 0.30487037\n",
      "Iteration 159, loss = 0.30453321\n",
      "Iteration 160, loss = 0.30417557\n",
      "Iteration 161, loss = 0.30384816\n",
      "Iteration 162, loss = 0.30350274\n",
      "Iteration 163, loss = 0.30317198\n",
      "Iteration 164, loss = 0.30284555\n",
      "Iteration 165, loss = 0.30252122\n",
      "Iteration 166, loss = 0.30218997\n",
      "Iteration 167, loss = 0.30185748\n",
      "Iteration 168, loss = 0.30153680\n",
      "Iteration 169, loss = 0.30122315\n",
      "Iteration 170, loss = 0.30091889\n",
      "Iteration 171, loss = 0.30064325\n",
      "Iteration 172, loss = 0.30034597\n",
      "Iteration 173, loss = 0.30005359\n",
      "Iteration 174, loss = 0.29978153\n",
      "Iteration 175, loss = 0.29947648\n",
      "Iteration 176, loss = 0.29918537\n",
      "Iteration 177, loss = 0.29888252\n",
      "Iteration 178, loss = 0.29860038\n",
      "Iteration 179, loss = 0.29832596\n",
      "Iteration 180, loss = 0.29805795\n",
      "Iteration 181, loss = 0.29779225\n",
      "Iteration 182, loss = 0.29751725\n",
      "Iteration 183, loss = 0.29724791\n",
      "Iteration 184, loss = 0.29696022\n",
      "Iteration 185, loss = 0.29667832\n",
      "Iteration 186, loss = 0.29640471\n",
      "Iteration 187, loss = 0.29612490\n",
      "Iteration 188, loss = 0.29586574\n",
      "Iteration 189, loss = 0.29560359\n",
      "Iteration 190, loss = 0.29532907\n",
      "Iteration 191, loss = 0.29508921\n",
      "Iteration 192, loss = 0.29482978\n",
      "Iteration 193, loss = 0.29459388\n",
      "Iteration 194, loss = 0.29435863\n",
      "Iteration 195, loss = 0.29411686\n",
      "Iteration 196, loss = 0.29388188\n",
      "Iteration 197, loss = 0.29364284\n",
      "Iteration 198, loss = 0.29341527\n",
      "Iteration 199, loss = 0.29320452\n",
      "Iteration 200, loss = 0.29298565\n",
      "Iteration 201, loss = 0.29277153\n",
      "Iteration 202, loss = 0.29255501\n",
      "Iteration 203, loss = 0.29233270\n",
      "Iteration 204, loss = 0.29213253\n",
      "Iteration 205, loss = 0.29191678\n",
      "Iteration 206, loss = 0.29171545\n",
      "Iteration 207, loss = 0.29150868\n",
      "Iteration 208, loss = 0.29128833\n",
      "Iteration 209, loss = 0.29108647\n",
      "Iteration 210, loss = 0.29087211\n",
      "Iteration 211, loss = 0.29065826\n",
      "Iteration 212, loss = 0.29045254\n",
      "Iteration 213, loss = 0.29025760\n",
      "Iteration 214, loss = 0.29005781\n",
      "Iteration 215, loss = 0.28985768\n",
      "Iteration 216, loss = 0.28967221\n",
      "Iteration 217, loss = 0.28947243\n",
      "Iteration 218, loss = 0.28928372\n",
      "Iteration 219, loss = 0.28908504\n",
      "Iteration 220, loss = 0.28890970\n",
      "Iteration 221, loss = 0.28872386\n",
      "Iteration 222, loss = 0.28855051\n",
      "Iteration 223, loss = 0.28838373\n",
      "Iteration 224, loss = 0.28822047\n",
      "Iteration 225, loss = 0.28805572\n",
      "Iteration 226, loss = 0.28786935\n",
      "Iteration 227, loss = 0.28769215\n",
      "Iteration 228, loss = 0.28751071\n",
      "Iteration 229, loss = 0.28734622\n",
      "Iteration 230, loss = 0.28716807\n",
      "Iteration 231, loss = 0.28700978\n",
      "Iteration 232, loss = 0.28685627\n",
      "Iteration 233, loss = 0.28669240\n",
      "Iteration 234, loss = 0.28653541\n",
      "Iteration 235, loss = 0.28636982\n",
      "Iteration 236, loss = 0.28621414\n",
      "Iteration 237, loss = 0.28605424\n",
      "Iteration 238, loss = 0.28590470\n",
      "Iteration 239, loss = 0.28575235\n",
      "Iteration 240, loss = 0.28561596\n",
      "Iteration 241, loss = 0.28547197\n",
      "Iteration 242, loss = 0.28532618\n",
      "Iteration 243, loss = 0.28516692\n",
      "Iteration 244, loss = 0.28502117\n",
      "Iteration 245, loss = 0.28486764\n",
      "Iteration 246, loss = 0.28471884\n",
      "Iteration 247, loss = 0.28456514\n",
      "Iteration 248, loss = 0.28442006\n",
      "Iteration 249, loss = 0.28426466\n",
      "Iteration 250, loss = 0.28410930\n",
      "Iteration 251, loss = 0.28396478\n",
      "Iteration 252, loss = 0.28381468\n",
      "Iteration 253, loss = 0.28367084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 254, loss = 0.28353343\n",
      "Iteration 255, loss = 0.28339113\n",
      "Iteration 256, loss = 0.28324918\n",
      "Iteration 257, loss = 0.28311042\n",
      "Iteration 258, loss = 0.28296880\n",
      "Iteration 259, loss = 0.28284513\n",
      "Iteration 260, loss = 0.28272314\n",
      "Iteration 261, loss = 0.28261298\n",
      "Iteration 262, loss = 0.28249698\n",
      "Iteration 263, loss = 0.28238562\n",
      "Iteration 264, loss = 0.28226924\n",
      "Iteration 265, loss = 0.28215638\n",
      "Iteration 266, loss = 0.28204299\n",
      "Iteration 267, loss = 0.28190827\n",
      "Iteration 268, loss = 0.28179539\n",
      "Iteration 269, loss = 0.28165690\n",
      "Iteration 270, loss = 0.28154654\n",
      "Iteration 271, loss = 0.28142615\n",
      "Iteration 272, loss = 0.28131518\n",
      "Iteration 273, loss = 0.28120235\n",
      "Iteration 274, loss = 0.28108047\n",
      "Iteration 275, loss = 0.28096470\n",
      "Iteration 276, loss = 0.28085779\n",
      "Iteration 277, loss = 0.28074016\n",
      "Iteration 278, loss = 0.28063052\n",
      "Iteration 279, loss = 0.28052588\n",
      "Iteration 280, loss = 0.28041242\n",
      "Iteration 281, loss = 0.28030954\n",
      "Iteration 282, loss = 0.28019967\n",
      "Iteration 283, loss = 0.28009141\n",
      "Iteration 284, loss = 0.27998223\n",
      "Iteration 285, loss = 0.27986741\n",
      "Iteration 286, loss = 0.27977205\n",
      "Iteration 287, loss = 0.27967026\n",
      "Iteration 288, loss = 0.27958307\n",
      "Iteration 289, loss = 0.27948041\n",
      "Iteration 290, loss = 0.27939231\n",
      "Iteration 291, loss = 0.27930028\n",
      "Iteration 292, loss = 0.27921058\n",
      "Iteration 293, loss = 0.27912387\n",
      "Iteration 294, loss = 0.27903683\n",
      "Iteration 295, loss = 0.27895497\n",
      "Iteration 296, loss = 0.27886024\n",
      "Iteration 297, loss = 0.27877326\n",
      "Iteration 298, loss = 0.27868436\n",
      "Iteration 299, loss = 0.27859284\n",
      "Iteration 300, loss = 0.27851684\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "The training time of the model: 0.45 .\n",
      "accuracy: 0.89375\n",
      "f1: 0.0\n"
     ]
    }
   ],
   "source": [
    "#model H = 2\n",
    "mlp2 = MLPClassifier(hidden_layer_sizes=(2, ), solver='sgd', \n",
    "                     max_iter=500, random_state=0, verbose=True)\n",
    "start = time()\n",
    "mlp2.fit(X_train, y_train)\n",
    "NN_time = time() - start\n",
    "y_hat = mlp2.predict(X_test)\n",
    "print(\"The training time of the model: %.2f .\"\n",
    "      % (NN_time))\n",
    "print('accuracy:', accuracy_score(y_test, y_hat))\n",
    "print('f1:', f1_score(y_test, y_hat))\n",
    "setting2_time1 = NN_time\n",
    "setting2_acc1 = accuracy_score(y_test, y_hat)\n",
    "setting2_f1 = f1_score(y_test, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.43284476\n",
      "Iteration 2, loss = 0.41808008\n",
      "Iteration 3, loss = 0.40759811\n",
      "Iteration 4, loss = 0.39188236\n",
      "Iteration 5, loss = 0.38507168\n",
      "Iteration 6, loss = 0.37511831\n",
      "Iteration 7, loss = 0.36893055\n",
      "Iteration 8, loss = 0.36635275\n",
      "Iteration 9, loss = 0.36443572\n",
      "Iteration 10, loss = 0.36262370\n",
      "Iteration 11, loss = 0.36144680\n",
      "Iteration 12, loss = 0.36035850\n",
      "Iteration 13, loss = 0.35920100\n",
      "Iteration 14, loss = 0.35813780\n",
      "Iteration 15, loss = 0.35706874\n",
      "Iteration 16, loss = 0.35606283\n",
      "Iteration 17, loss = 0.35499934\n",
      "Iteration 18, loss = 0.35400064\n",
      "Iteration 19, loss = 0.35298148\n",
      "Iteration 20, loss = 0.35197177\n",
      "Iteration 21, loss = 0.35101150\n",
      "Iteration 22, loss = 0.35005871\n",
      "Iteration 23, loss = 0.34908549\n",
      "Iteration 24, loss = 0.34817158\n",
      "Iteration 25, loss = 0.34725946\n",
      "Iteration 26, loss = 0.34633250\n",
      "Iteration 27, loss = 0.34542462\n",
      "Iteration 28, loss = 0.34453397\n",
      "Iteration 29, loss = 0.34367799\n",
      "Iteration 30, loss = 0.34279934\n",
      "Iteration 31, loss = 0.34193698\n",
      "Iteration 32, loss = 0.34108677\n",
      "Iteration 33, loss = 0.34031417\n",
      "Iteration 34, loss = 0.33952912\n",
      "Iteration 35, loss = 0.33877681\n",
      "Iteration 36, loss = 0.33801307\n",
      "Iteration 37, loss = 0.33722363\n",
      "Iteration 38, loss = 0.33645882\n",
      "Iteration 39, loss = 0.33573011\n",
      "Iteration 40, loss = 0.33500588\n",
      "Iteration 41, loss = 0.33431003\n",
      "Iteration 42, loss = 0.33360739\n",
      "Iteration 43, loss = 0.33292170\n",
      "Iteration 44, loss = 0.33224437\n",
      "Iteration 45, loss = 0.33157076\n",
      "Iteration 46, loss = 0.33092863\n",
      "Iteration 47, loss = 0.33024269\n",
      "Iteration 48, loss = 0.32959942\n",
      "Iteration 49, loss = 0.32893408\n",
      "Iteration 50, loss = 0.32825220\n",
      "Iteration 51, loss = 0.32757459\n",
      "Iteration 52, loss = 0.32689402\n",
      "Iteration 53, loss = 0.32623315\n",
      "Iteration 54, loss = 0.32557454\n",
      "Iteration 55, loss = 0.32494229\n",
      "Iteration 56, loss = 0.32427715\n",
      "Iteration 57, loss = 0.32365706\n",
      "Iteration 58, loss = 0.32304157\n",
      "Iteration 59, loss = 0.32246189\n",
      "Iteration 60, loss = 0.32191000\n",
      "Iteration 61, loss = 0.32136640\n",
      "Iteration 62, loss = 0.32081909\n",
      "Iteration 63, loss = 0.32028285\n",
      "Iteration 64, loss = 0.31972816\n",
      "Iteration 65, loss = 0.31919834\n",
      "Iteration 66, loss = 0.31868027\n",
      "Iteration 67, loss = 0.31817596\n",
      "Iteration 68, loss = 0.31765517\n",
      "Iteration 69, loss = 0.31718144\n",
      "Iteration 70, loss = 0.31667801\n",
      "Iteration 71, loss = 0.31619305\n",
      "Iteration 72, loss = 0.31569380\n",
      "Iteration 73, loss = 0.31521688\n",
      "Iteration 74, loss = 0.31471144\n",
      "Iteration 75, loss = 0.31422857\n",
      "Iteration 76, loss = 0.31375367\n",
      "Iteration 77, loss = 0.31329031\n",
      "Iteration 78, loss = 0.31282496\n",
      "Iteration 79, loss = 0.31237204\n",
      "Iteration 80, loss = 0.31192955\n",
      "Iteration 81, loss = 0.31146528\n",
      "Iteration 82, loss = 0.31101159\n",
      "Iteration 83, loss = 0.31057134\n",
      "Iteration 84, loss = 0.31013891\n",
      "Iteration 85, loss = 0.30969335\n",
      "Iteration 86, loss = 0.30926680\n",
      "Iteration 87, loss = 0.30883121\n",
      "Iteration 88, loss = 0.30840696\n",
      "Iteration 89, loss = 0.30799578\n",
      "Iteration 90, loss = 0.30760876\n",
      "Iteration 91, loss = 0.30721923\n",
      "Iteration 92, loss = 0.30684307\n",
      "Iteration 93, loss = 0.30644776\n",
      "Iteration 94, loss = 0.30608062\n",
      "Iteration 95, loss = 0.30568921\n",
      "Iteration 96, loss = 0.30528281\n",
      "Iteration 97, loss = 0.30491444\n",
      "Iteration 98, loss = 0.30452105\n",
      "Iteration 99, loss = 0.30415571\n",
      "Iteration 100, loss = 0.30380391\n",
      "Iteration 101, loss = 0.30345232\n",
      "Iteration 102, loss = 0.30310995\n",
      "Iteration 103, loss = 0.30276622\n",
      "Iteration 104, loss = 0.30242411\n",
      "Iteration 105, loss = 0.30209149\n",
      "Iteration 106, loss = 0.30175076\n",
      "Iteration 107, loss = 0.30143894\n",
      "Iteration 108, loss = 0.30112103\n",
      "Iteration 109, loss = 0.30078613\n",
      "Iteration 110, loss = 0.30048461\n",
      "Iteration 111, loss = 0.30014775\n",
      "Iteration 112, loss = 0.29983824\n",
      "Iteration 113, loss = 0.29951459\n",
      "Iteration 114, loss = 0.29920111\n",
      "Iteration 115, loss = 0.29889339\n",
      "Iteration 116, loss = 0.29857581\n",
      "Iteration 117, loss = 0.29828036\n",
      "Iteration 118, loss = 0.29800395\n",
      "Iteration 119, loss = 0.29771811\n",
      "Iteration 120, loss = 0.29744626\n",
      "Iteration 121, loss = 0.29719020\n",
      "Iteration 122, loss = 0.29692528\n",
      "Iteration 123, loss = 0.29665398\n",
      "Iteration 124, loss = 0.29639471\n",
      "Iteration 125, loss = 0.29614133\n",
      "Iteration 126, loss = 0.29588326\n",
      "Iteration 127, loss = 0.29562957\n",
      "Iteration 128, loss = 0.29537414\n",
      "Iteration 129, loss = 0.29513581\n",
      "Iteration 130, loss = 0.29488233\n",
      "Iteration 131, loss = 0.29462197\n",
      "Iteration 132, loss = 0.29435941\n",
      "Iteration 133, loss = 0.29410623\n",
      "Iteration 134, loss = 0.29384165\n",
      "Iteration 135, loss = 0.29359404\n",
      "Iteration 136, loss = 0.29332578\n",
      "Iteration 137, loss = 0.29307101\n",
      "Iteration 138, loss = 0.29281620\n",
      "Iteration 139, loss = 0.29258248\n",
      "Iteration 140, loss = 0.29235491\n",
      "Iteration 141, loss = 0.29211654\n",
      "Iteration 142, loss = 0.29188698\n",
      "Iteration 143, loss = 0.29166773\n",
      "Iteration 144, loss = 0.29144961\n",
      "Iteration 145, loss = 0.29122232\n",
      "Iteration 146, loss = 0.29100142\n",
      "Iteration 147, loss = 0.29078168\n",
      "Iteration 148, loss = 0.29056568\n",
      "Iteration 149, loss = 0.29034940\n",
      "Iteration 150, loss = 0.29015347\n",
      "Iteration 151, loss = 0.28994954\n",
      "Iteration 152, loss = 0.28975697\n",
      "Iteration 153, loss = 0.28956113\n",
      "Iteration 154, loss = 0.28935622\n",
      "Iteration 155, loss = 0.28917371\n",
      "Iteration 156, loss = 0.28898230\n",
      "Iteration 157, loss = 0.28880266\n",
      "Iteration 158, loss = 0.28860473\n",
      "Iteration 159, loss = 0.28842629\n",
      "Iteration 160, loss = 0.28823781\n",
      "Iteration 161, loss = 0.28804968\n",
      "Iteration 162, loss = 0.28785958\n",
      "Iteration 163, loss = 0.28765809\n",
      "Iteration 164, loss = 0.28747816\n",
      "Iteration 165, loss = 0.28730593\n",
      "Iteration 166, loss = 0.28714206\n",
      "Iteration 167, loss = 0.28695738\n",
      "Iteration 168, loss = 0.28678772\n",
      "Iteration 169, loss = 0.28661940\n",
      "Iteration 170, loss = 0.28646665\n",
      "Iteration 171, loss = 0.28632038\n",
      "Iteration 172, loss = 0.28616404\n",
      "Iteration 173, loss = 0.28600638\n",
      "Iteration 174, loss = 0.28585124\n",
      "Iteration 175, loss = 0.28570529\n",
      "Iteration 176, loss = 0.28555220\n",
      "Iteration 177, loss = 0.28539583\n",
      "Iteration 178, loss = 0.28525074\n",
      "Iteration 179, loss = 0.28509874\n",
      "Iteration 180, loss = 0.28496734\n",
      "Iteration 181, loss = 0.28481333\n",
      "Iteration 182, loss = 0.28468316\n",
      "Iteration 183, loss = 0.28453284\n",
      "Iteration 184, loss = 0.28438930\n",
      "Iteration 185, loss = 0.28424463\n",
      "Iteration 186, loss = 0.28409972\n",
      "Iteration 187, loss = 0.28396110\n",
      "Iteration 188, loss = 0.28381603\n",
      "Iteration 189, loss = 0.28367987\n",
      "Iteration 190, loss = 0.28354870\n",
      "Iteration 191, loss = 0.28341594\n",
      "Iteration 192, loss = 0.28328391\n",
      "Iteration 193, loss = 0.28315516\n",
      "Iteration 194, loss = 0.28301445\n",
      "Iteration 195, loss = 0.28288725\n",
      "Iteration 196, loss = 0.28274582\n",
      "Iteration 197, loss = 0.28260609\n",
      "Iteration 198, loss = 0.28247083\n",
      "Iteration 199, loss = 0.28233582\n",
      "Iteration 200, loss = 0.28219006\n",
      "Iteration 201, loss = 0.28206142\n",
      "Iteration 202, loss = 0.28191410\n",
      "Iteration 203, loss = 0.28180197\n",
      "Iteration 204, loss = 0.28167190\n",
      "Iteration 205, loss = 0.28154712\n",
      "Iteration 206, loss = 0.28142368\n",
      "Iteration 207, loss = 0.28130971\n",
      "Iteration 208, loss = 0.28120912\n",
      "Iteration 209, loss = 0.28109227\n",
      "Iteration 210, loss = 0.28098090\n",
      "Iteration 211, loss = 0.28086361\n",
      "Iteration 212, loss = 0.28073576\n",
      "Iteration 213, loss = 0.28062099\n",
      "Iteration 214, loss = 0.28050878\n",
      "Iteration 215, loss = 0.28039748\n",
      "Iteration 216, loss = 0.28028366\n",
      "Iteration 217, loss = 0.28018397\n",
      "Iteration 218, loss = 0.28007416\n",
      "Iteration 219, loss = 0.27996308\n",
      "Iteration 220, loss = 0.27986761\n",
      "Iteration 221, loss = 0.27977697\n",
      "Iteration 222, loss = 0.27967941\n",
      "Iteration 223, loss = 0.27957910\n",
      "Iteration 224, loss = 0.27948301\n",
      "Iteration 225, loss = 0.27938139\n",
      "Iteration 226, loss = 0.27926700\n",
      "Iteration 227, loss = 0.27916975\n",
      "Iteration 228, loss = 0.27906007\n",
      "Iteration 229, loss = 0.27895940\n",
      "Iteration 230, loss = 0.27886377\n",
      "Iteration 231, loss = 0.27876103\n",
      "Iteration 232, loss = 0.27867218\n",
      "Iteration 233, loss = 0.27857051\n",
      "Iteration 234, loss = 0.27847340\n",
      "Iteration 235, loss = 0.27838335\n",
      "Iteration 236, loss = 0.27829078\n",
      "Iteration 237, loss = 0.27820125\n",
      "Iteration 238, loss = 0.27811126\n",
      "Iteration 239, loss = 0.27802477\n",
      "Iteration 240, loss = 0.27793675\n",
      "Iteration 241, loss = 0.27784776\n",
      "Iteration 242, loss = 0.27776253\n",
      "Iteration 243, loss = 0.27767599\n",
      "Iteration 244, loss = 0.27759246\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "The training time of the model: 0.35 .\n",
      "accuracy: 0.89375\n",
      "f1: 0.0\n"
     ]
    }
   ],
   "source": [
    "#repeat model H = 2 multiple times\n",
    "mlp2 = MLPClassifier(hidden_layer_sizes=(2, ), solver='sgd', \n",
    "                     max_iter=500, random_state=1, verbose=True)\n",
    "start = time()\n",
    "mlp2.fit(X_train, y_train)\n",
    "NN_time = time() - start\n",
    "y_hat = mlp2.predict(X_test)\n",
    "print(\"The training time of the model: %.2f .\"\n",
    "      % (NN_time))\n",
    "print('accuracy:', accuracy_score(y_test, y_hat))\n",
    "print('f1:', f1_score(y_test, y_hat))\n",
    "setting2_time2 = NN_time\n",
    "setting2_acc2 = accuracy_score(y_test, y_hat)\n",
    "setting2_f12 = f1_score(y_test, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.50292487\n",
      "Iteration 2, loss = 0.50162999\n",
      "Iteration 3, loss = 0.49964434\n",
      "Iteration 4, loss = 0.49718380\n",
      "Iteration 5, loss = 0.49447050\n",
      "Iteration 6, loss = 0.49157523\n",
      "Iteration 7, loss = 0.48855141\n",
      "Iteration 8, loss = 0.48557015\n",
      "Iteration 9, loss = 0.48247288\n",
      "Iteration 10, loss = 0.47941026\n",
      "Iteration 11, loss = 0.47632268\n",
      "Iteration 12, loss = 0.47329451\n",
      "Iteration 13, loss = 0.47030114\n",
      "Iteration 14, loss = 0.46741295\n",
      "Iteration 15, loss = 0.46452310\n",
      "Iteration 16, loss = 0.46171988\n",
      "Iteration 17, loss = 0.45896570\n",
      "Iteration 18, loss = 0.45624468\n",
      "Iteration 19, loss = 0.45359863\n",
      "Iteration 20, loss = 0.45099597\n",
      "Iteration 21, loss = 0.44836656\n",
      "Iteration 22, loss = 0.44580026\n",
      "Iteration 23, loss = 0.44328089\n",
      "Iteration 24, loss = 0.44079782\n",
      "Iteration 25, loss = 0.43836558\n",
      "Iteration 26, loss = 0.43602057\n",
      "Iteration 27, loss = 0.43367594\n",
      "Iteration 28, loss = 0.43136278\n",
      "Iteration 29, loss = 0.42909559\n",
      "Iteration 30, loss = 0.42686257\n",
      "Iteration 31, loss = 0.42471198\n",
      "Iteration 32, loss = 0.42259354\n",
      "Iteration 33, loss = 0.42051079\n",
      "Iteration 34, loss = 0.41848077\n",
      "Iteration 35, loss = 0.41643605\n",
      "Iteration 36, loss = 0.41457245\n",
      "Iteration 37, loss = 0.41266965\n",
      "Iteration 38, loss = 0.41081815\n",
      "Iteration 39, loss = 0.40897562\n",
      "Iteration 40, loss = 0.40718648\n",
      "Iteration 41, loss = 0.40535086\n",
      "Iteration 42, loss = 0.40357508\n",
      "Iteration 43, loss = 0.40181365\n",
      "Iteration 44, loss = 0.40010188\n",
      "Iteration 45, loss = 0.39843319\n",
      "Iteration 46, loss = 0.39680414\n",
      "Iteration 47, loss = 0.39518879\n",
      "Iteration 48, loss = 0.39357838\n",
      "Iteration 49, loss = 0.39196363\n",
      "Iteration 50, loss = 0.39036590\n",
      "Iteration 51, loss = 0.38881682\n",
      "Iteration 52, loss = 0.38727259\n",
      "Iteration 53, loss = 0.38576289\n",
      "Iteration 54, loss = 0.38428413\n",
      "Iteration 55, loss = 0.38283611\n",
      "Iteration 56, loss = 0.38142208\n",
      "Iteration 57, loss = 0.38001650\n",
      "Iteration 58, loss = 0.37861779\n",
      "Iteration 59, loss = 0.37723702\n",
      "Iteration 60, loss = 0.37592116\n",
      "Iteration 61, loss = 0.37458304\n",
      "Iteration 62, loss = 0.37332160\n",
      "Iteration 63, loss = 0.37206819\n",
      "Iteration 64, loss = 0.37080429\n",
      "Iteration 65, loss = 0.36956729\n",
      "Iteration 66, loss = 0.36834407\n",
      "Iteration 67, loss = 0.36714910\n",
      "Iteration 68, loss = 0.36594654\n",
      "Iteration 69, loss = 0.36478269\n",
      "Iteration 70, loss = 0.36361771\n",
      "Iteration 71, loss = 0.36249502\n",
      "Iteration 72, loss = 0.36135720\n",
      "Iteration 73, loss = 0.36024746\n",
      "Iteration 74, loss = 0.35915223\n",
      "Iteration 75, loss = 0.35807760\n",
      "Iteration 76, loss = 0.35699975\n",
      "Iteration 77, loss = 0.35599179\n",
      "Iteration 78, loss = 0.35495582\n",
      "Iteration 79, loss = 0.35392295\n",
      "Iteration 80, loss = 0.35293369\n",
      "Iteration 81, loss = 0.35202178\n",
      "Iteration 82, loss = 0.35102533\n",
      "Iteration 83, loss = 0.35012043\n",
      "Iteration 84, loss = 0.34916046\n",
      "Iteration 85, loss = 0.34825102\n",
      "Iteration 86, loss = 0.34735144\n",
      "Iteration 87, loss = 0.34648915\n",
      "Iteration 88, loss = 0.34561012\n",
      "Iteration 89, loss = 0.34472199\n",
      "Iteration 90, loss = 0.34386432\n",
      "Iteration 91, loss = 0.34301325\n",
      "Iteration 92, loss = 0.34215917\n",
      "Iteration 93, loss = 0.34134645\n",
      "Iteration 94, loss = 0.34053900\n",
      "Iteration 95, loss = 0.33973988\n",
      "Iteration 96, loss = 0.33893345\n",
      "Iteration 97, loss = 0.33815337\n",
      "Iteration 98, loss = 0.33735502\n",
      "Iteration 99, loss = 0.33659551\n",
      "Iteration 100, loss = 0.33583616\n",
      "Iteration 101, loss = 0.33506472\n",
      "Iteration 102, loss = 0.33432379\n",
      "Iteration 103, loss = 0.33357220\n",
      "Iteration 104, loss = 0.33283077\n",
      "Iteration 105, loss = 0.33212444\n",
      "Iteration 106, loss = 0.33143024\n",
      "Iteration 107, loss = 0.33072437\n",
      "Iteration 108, loss = 0.33005085\n",
      "Iteration 109, loss = 0.32936401\n",
      "Iteration 110, loss = 0.32870107\n",
      "Iteration 111, loss = 0.32806202\n",
      "Iteration 112, loss = 0.32737621\n",
      "Iteration 113, loss = 0.32675373\n",
      "Iteration 114, loss = 0.32611265\n",
      "Iteration 115, loss = 0.32551446\n",
      "Iteration 116, loss = 0.32491263\n",
      "Iteration 117, loss = 0.32429859\n",
      "Iteration 118, loss = 0.32371677\n",
      "Iteration 119, loss = 0.32313412\n",
      "Iteration 120, loss = 0.32257085\n",
      "Iteration 121, loss = 0.32203191\n",
      "Iteration 122, loss = 0.32148666\n",
      "Iteration 123, loss = 0.32093260\n",
      "Iteration 124, loss = 0.32037209\n",
      "Iteration 125, loss = 0.31983257\n",
      "Iteration 126, loss = 0.31929153\n",
      "Iteration 127, loss = 0.31875890\n",
      "Iteration 128, loss = 0.31821718\n",
      "Iteration 129, loss = 0.31770916\n",
      "Iteration 130, loss = 0.31718815\n",
      "Iteration 131, loss = 0.31669180\n",
      "Iteration 132, loss = 0.31615605\n",
      "Iteration 133, loss = 0.31566111\n",
      "Iteration 134, loss = 0.31520462\n",
      "Iteration 135, loss = 0.31472289\n",
      "Iteration 136, loss = 0.31426420\n",
      "Iteration 137, loss = 0.31382745\n",
      "Iteration 138, loss = 0.31335898\n",
      "Iteration 139, loss = 0.31288263\n",
      "Iteration 140, loss = 0.31239647\n",
      "Iteration 141, loss = 0.31194771\n",
      "Iteration 142, loss = 0.31148885\n",
      "Iteration 143, loss = 0.31107090\n",
      "Iteration 144, loss = 0.31063035\n",
      "Iteration 145, loss = 0.31020433\n",
      "Iteration 146, loss = 0.30979520\n",
      "Iteration 147, loss = 0.30936778\n",
      "Iteration 148, loss = 0.30895292\n",
      "Iteration 149, loss = 0.30854620\n",
      "Iteration 150, loss = 0.30815320\n",
      "Iteration 151, loss = 0.30779319\n",
      "Iteration 152, loss = 0.30740302\n",
      "Iteration 153, loss = 0.30703540\n",
      "Iteration 154, loss = 0.30664508\n",
      "Iteration 155, loss = 0.30624423\n",
      "Iteration 156, loss = 0.30586240\n",
      "Iteration 157, loss = 0.30548832\n",
      "Iteration 158, loss = 0.30514093\n",
      "Iteration 159, loss = 0.30478717\n",
      "Iteration 160, loss = 0.30442684\n",
      "Iteration 161, loss = 0.30409778\n",
      "Iteration 162, loss = 0.30374897\n",
      "Iteration 163, loss = 0.30340632\n",
      "Iteration 164, loss = 0.30306427\n",
      "Iteration 165, loss = 0.30272179\n",
      "Iteration 166, loss = 0.30238638\n",
      "Iteration 167, loss = 0.30205087\n",
      "Iteration 168, loss = 0.30173099\n",
      "Iteration 169, loss = 0.30141335\n",
      "Iteration 170, loss = 0.30107878\n",
      "Iteration 171, loss = 0.30074727\n",
      "Iteration 172, loss = 0.30044121\n",
      "Iteration 173, loss = 0.30012257\n",
      "Iteration 174, loss = 0.29981847\n",
      "Iteration 175, loss = 0.29951447\n",
      "Iteration 176, loss = 0.29920825\n",
      "Iteration 177, loss = 0.29889924\n",
      "Iteration 178, loss = 0.29860447\n",
      "Iteration 179, loss = 0.29829715\n",
      "Iteration 180, loss = 0.29800410\n",
      "Iteration 181, loss = 0.29768640\n",
      "Iteration 182, loss = 0.29742761\n",
      "Iteration 183, loss = 0.29711761\n",
      "Iteration 184, loss = 0.29684071\n",
      "Iteration 185, loss = 0.29657952\n",
      "Iteration 186, loss = 0.29629627\n",
      "Iteration 187, loss = 0.29602576\n",
      "Iteration 188, loss = 0.29575986\n",
      "Iteration 189, loss = 0.29549576\n",
      "Iteration 190, loss = 0.29522435\n",
      "Iteration 191, loss = 0.29495113\n",
      "Iteration 192, loss = 0.29469223\n",
      "Iteration 193, loss = 0.29443468\n",
      "Iteration 194, loss = 0.29418420\n",
      "Iteration 195, loss = 0.29393135\n",
      "Iteration 196, loss = 0.29369267\n",
      "Iteration 197, loss = 0.29344129\n",
      "Iteration 198, loss = 0.29321376\n",
      "Iteration 199, loss = 0.29298731\n",
      "Iteration 200, loss = 0.29275465\n",
      "Iteration 201, loss = 0.29253379\n",
      "Iteration 202, loss = 0.29229620\n",
      "Iteration 203, loss = 0.29206794\n",
      "Iteration 204, loss = 0.29182596\n",
      "Iteration 205, loss = 0.29160265\n",
      "Iteration 206, loss = 0.29137203\n",
      "Iteration 207, loss = 0.29115711\n",
      "Iteration 208, loss = 0.29093441\n",
      "Iteration 209, loss = 0.29072599\n",
      "Iteration 210, loss = 0.29052412\n",
      "Iteration 211, loss = 0.29032847\n",
      "Iteration 212, loss = 0.29012939\n",
      "Iteration 213, loss = 0.28990599\n",
      "Iteration 214, loss = 0.28971440\n",
      "Iteration 215, loss = 0.28951200\n",
      "Iteration 216, loss = 0.28932196\n",
      "Iteration 217, loss = 0.28913328\n",
      "Iteration 218, loss = 0.28893893\n",
      "Iteration 219, loss = 0.28874425\n",
      "Iteration 220, loss = 0.28854932\n",
      "Iteration 221, loss = 0.28834878\n",
      "Iteration 222, loss = 0.28814983\n",
      "Iteration 223, loss = 0.28796291\n",
      "Iteration 224, loss = 0.28777646\n",
      "Iteration 225, loss = 0.28758547\n",
      "Iteration 226, loss = 0.28739763\n",
      "Iteration 227, loss = 0.28721894\n",
      "Iteration 228, loss = 0.28704440\n",
      "Iteration 229, loss = 0.28688001\n",
      "Iteration 230, loss = 0.28671725\n",
      "Iteration 231, loss = 0.28654525\n",
      "Iteration 232, loss = 0.28639532\n",
      "Iteration 233, loss = 0.28623994\n",
      "Iteration 234, loss = 0.28607123\n",
      "Iteration 235, loss = 0.28592390\n",
      "Iteration 236, loss = 0.28574476\n",
      "Iteration 237, loss = 0.28559164\n",
      "Iteration 238, loss = 0.28542514\n",
      "Iteration 239, loss = 0.28527546\n",
      "Iteration 240, loss = 0.28512069\n",
      "Iteration 241, loss = 0.28496910\n",
      "Iteration 242, loss = 0.28481203\n",
      "Iteration 243, loss = 0.28466349\n",
      "Iteration 244, loss = 0.28450832\n",
      "Iteration 245, loss = 0.28436567\n",
      "Iteration 246, loss = 0.28421833\n",
      "Iteration 247, loss = 0.28407187\n",
      "Iteration 248, loss = 0.28393530\n",
      "Iteration 249, loss = 0.28378995\n",
      "Iteration 250, loss = 0.28365878\n",
      "Iteration 251, loss = 0.28352299\n",
      "Iteration 252, loss = 0.28338461\n",
      "Iteration 253, loss = 0.28325154\n",
      "Iteration 254, loss = 0.28311982\n",
      "Iteration 255, loss = 0.28298399\n",
      "Iteration 256, loss = 0.28284298\n",
      "Iteration 257, loss = 0.28269562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 258, loss = 0.28256417\n",
      "Iteration 259, loss = 0.28243445\n",
      "Iteration 260, loss = 0.28231472\n",
      "Iteration 261, loss = 0.28218695\n",
      "Iteration 262, loss = 0.28206558\n",
      "Iteration 263, loss = 0.28193085\n",
      "Iteration 264, loss = 0.28181708\n",
      "Iteration 265, loss = 0.28167798\n",
      "Iteration 266, loss = 0.28156531\n",
      "Iteration 267, loss = 0.28144460\n",
      "Iteration 268, loss = 0.28132967\n",
      "Iteration 269, loss = 0.28120942\n",
      "Iteration 270, loss = 0.28109523\n",
      "Iteration 271, loss = 0.28098500\n",
      "Iteration 272, loss = 0.28087654\n",
      "Iteration 273, loss = 0.28076548\n",
      "Iteration 274, loss = 0.28067558\n",
      "Iteration 275, loss = 0.28057207\n",
      "Iteration 276, loss = 0.28046775\n",
      "Iteration 277, loss = 0.28035953\n",
      "Iteration 278, loss = 0.28024734\n",
      "Iteration 279, loss = 0.28013482\n",
      "Iteration 280, loss = 0.28003218\n",
      "Iteration 281, loss = 0.27991657\n",
      "Iteration 282, loss = 0.27979938\n",
      "Iteration 283, loss = 0.27968726\n",
      "Iteration 284, loss = 0.27957536\n",
      "Iteration 285, loss = 0.27947054\n",
      "Iteration 286, loss = 0.27935674\n",
      "Iteration 287, loss = 0.27925704\n",
      "Iteration 288, loss = 0.27914688\n",
      "Iteration 289, loss = 0.27904256\n",
      "Iteration 290, loss = 0.27893556\n",
      "Iteration 291, loss = 0.27884253\n",
      "Iteration 292, loss = 0.27873906\n",
      "Iteration 293, loss = 0.27864194\n",
      "Iteration 294, loss = 0.27855138\n",
      "Iteration 295, loss = 0.27846402\n",
      "Iteration 296, loss = 0.27837797\n",
      "Iteration 297, loss = 0.27829170\n",
      "Iteration 298, loss = 0.27820501\n",
      "Iteration 299, loss = 0.27812828\n",
      "Iteration 300, loss = 0.27805379\n",
      "Iteration 301, loss = 0.27797491\n",
      "Iteration 302, loss = 0.27788583\n",
      "Iteration 303, loss = 0.27780652\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "The training time of the model: 0.47 .\n",
      "accuracy: 0.89375\n",
      "f1: 0.0\n",
      "The mean training time of the model H = 2: 0.42552947998046875 (std: 0.05156286599511159) .\n",
      "The mean accuracy time of the model H = 2: 0.89375000000000016 (std: 0.00000000000000011) .\n",
      "The mean F1 score time of the model H = 2: 0.00000000000000000 (std: 0.00000000000000000) .\n"
     ]
    }
   ],
   "source": [
    "#repeat model H = 2 multiple times\n",
    "#report the mean and standard deviation of the training time, accuracy, and F1 score for the setting.\n",
    "mlp2 = MLPClassifier(hidden_layer_sizes=(2, ), solver='sgd', \n",
    "                     max_iter=500, random_state=2, verbose=True)\n",
    "start = time()\n",
    "mlp2.fit(X_train, y_train)\n",
    "NN_time = time() - start\n",
    "y_hat = mlp2.predict(X_test)\n",
    "print(\"The training time of the model: %.2f .\"\n",
    "      % (NN_time))\n",
    "print('accuracy:', accuracy_score(y_test, y_hat))\n",
    "print('f1:', f1_score(y_test, y_hat))\n",
    "setting2_time3 = NN_time\n",
    "setting2_acc3 = accuracy_score(y_test, y_hat)\n",
    "setting2_f13 = f1_score(y_test, y_hat)\n",
    "set2_time = np.array([setting2_time1, setting2_time2, setting2_time3])\n",
    "mean_time = np.mean(set2_time)\n",
    "std_time = np.std(set2_time)\n",
    "set2_acc = np.array([setting2_acc1, setting2_acc2, setting2_acc3])\n",
    "mean2_acc = np.mean(set2_acc)\n",
    "std_acc = np.std(set2_acc)\n",
    "set2_f1 = np.array([setting2_f1, setting2_f12, setting2_f13])\n",
    "mean2_f1 = np.mean(set2_f1)\n",
    "std_f1 = np.std(set2_f1)\n",
    "print(\"The mean training time of the model H = 2: %.17f (std: %.17f) .\"\n",
    "      % (mean_time, std_time))\n",
    "print(\"The mean accuracy time of the model H = 2: %.17f (std: %.17f) .\"\n",
    "      % (mean2_acc, std_acc))\n",
    "print(\"The mean F1 score time of the model H = 2: %.17f (std: %.17f) .\"\n",
    "      % (mean2_f1, std_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = inf\n",
      "Iteration 2, loss = 12.63995516\n",
      "Iteration 3, loss = 6.52959432\n",
      "Iteration 4, loss = 0.84999108\n",
      "Iteration 5, loss = 0.42040080\n",
      "Iteration 6, loss = 0.44067818\n",
      "Iteration 7, loss = 0.44312122\n",
      "Iteration 8, loss = 0.44205819\n",
      "Iteration 9, loss = 0.44026272\n",
      "Iteration 10, loss = 0.43828005\n",
      "Iteration 11, loss = 0.43618186\n",
      "Iteration 12, loss = 0.43401577\n",
      "Iteration 13, loss = 0.43180266\n",
      "Iteration 14, loss = 0.42964932\n",
      "Iteration 15, loss = 0.42746757\n",
      "Iteration 16, loss = 0.42535254\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "The training time of the model: 0.03 .\n",
      "accuracy: 0.89375\n",
      "f1: 0.0\n"
     ]
    }
   ],
   "source": [
    "#model H = 4\n",
    "mlp4 = MLPClassifier(hidden_layer_sizes=(4, ), solver='sgd', \n",
    "                     max_iter=500, random_state=0, verbose=True)\n",
    "start = time()\n",
    "mlp4.fit(X_train, y_train)\n",
    "NN_time = time() - start\n",
    "y_hat = mlp4.predict(X_test)\n",
    "print(\"The training time of the model: %.2f .\"\n",
    "      % (NN_time))\n",
    "print('accuracy:', accuracy_score(y_test, y_hat))\n",
    "print('f1:', f1_score(y_test, y_hat))\n",
    "setting4_time1 = NN_time\n",
    "setting4_acc1 = accuracy_score(y_test, y_hat)\n",
    "setting4_f1 = f1_score(y_test, y_hat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.84091667\n",
      "Iteration 2, loss = 0.70332644\n",
      "Iteration 3, loss = 0.76450295\n",
      "Iteration 4, loss = 0.80295882\n",
      "Iteration 5, loss = 0.80366247\n",
      "Iteration 6, loss = 0.79805598\n",
      "Iteration 7, loss = 0.78887844\n",
      "Iteration 8, loss = 0.77785499\n",
      "Iteration 9, loss = 0.76170686\n",
      "Iteration 10, loss = 0.71468341\n",
      "Iteration 11, loss = 0.55938790\n",
      "Iteration 12, loss = 0.56858517\n",
      "Iteration 13, loss = 0.52796838\n",
      "Iteration 14, loss = 0.48869749\n",
      "Iteration 15, loss = 0.46292809\n",
      "Iteration 16, loss = 0.44181971\n",
      "Iteration 17, loss = 0.42432364\n",
      "Iteration 18, loss = 0.41074331\n",
      "Iteration 19, loss = 0.39674851\n",
      "Iteration 20, loss = 0.39042810\n",
      "Iteration 21, loss = 0.38307786\n",
      "Iteration 22, loss = 0.37935291\n",
      "Iteration 23, loss = 0.37462564\n",
      "Iteration 24, loss = 0.37340352\n",
      "Iteration 25, loss = 0.37641888\n",
      "Iteration 26, loss = 0.37004944\n",
      "Iteration 27, loss = 0.36834149\n",
      "Iteration 28, loss = 0.36934165\n",
      "Iteration 29, loss = 0.36704440\n",
      "Iteration 30, loss = 0.36216231\n",
      "Iteration 31, loss = 0.36070714\n",
      "Iteration 32, loss = 0.36190786\n",
      "Iteration 33, loss = 0.35839225\n",
      "Iteration 34, loss = 0.35866813\n",
      "Iteration 35, loss = 0.35683680\n",
      "Iteration 36, loss = 0.35696381\n",
      "Iteration 37, loss = 0.35715747\n",
      "Iteration 38, loss = 0.35468608\n",
      "Iteration 39, loss = 0.35382271\n",
      "Iteration 40, loss = 0.35410949\n",
      "Iteration 41, loss = 0.35401572\n",
      "Iteration 42, loss = 0.35308584\n",
      "Iteration 43, loss = 0.35036312\n",
      "Iteration 44, loss = 0.34957243\n",
      "Iteration 45, loss = 0.34981766\n",
      "Iteration 46, loss = 0.34908113\n",
      "Iteration 47, loss = 0.34979788\n",
      "Iteration 48, loss = 0.34779924\n",
      "Iteration 49, loss = 0.34881813\n",
      "Iteration 50, loss = 0.34633773\n",
      "Iteration 51, loss = 0.34685661\n",
      "Iteration 52, loss = 0.34547194\n",
      "Iteration 53, loss = 0.34427930\n",
      "Iteration 54, loss = 0.34575294\n",
      "Iteration 55, loss = 0.34359584\n",
      "Iteration 56, loss = 0.34238747\n",
      "Iteration 57, loss = 0.34109080\n",
      "Iteration 58, loss = 0.34074849\n",
      "Iteration 59, loss = 0.34036575\n",
      "Iteration 60, loss = 0.34007633\n",
      "Iteration 61, loss = 0.33992476\n",
      "Iteration 62, loss = 0.33978295\n",
      "Iteration 63, loss = 0.34070898\n",
      "Iteration 64, loss = 0.34006123\n",
      "Iteration 65, loss = 0.33929972\n",
      "Iteration 66, loss = 0.33954163\n",
      "Iteration 67, loss = 0.33916721\n",
      "Iteration 68, loss = 0.33804604\n",
      "Iteration 69, loss = 0.33812441\n",
      "Iteration 70, loss = 0.33741619\n",
      "Iteration 71, loss = 0.34206207\n",
      "Iteration 72, loss = 0.33787308\n",
      "Iteration 73, loss = 0.33734978\n",
      "Iteration 74, loss = 0.33802227\n",
      "Iteration 75, loss = 0.33755867\n",
      "Iteration 76, loss = 0.33663071\n",
      "Iteration 77, loss = 0.33788126\n",
      "Iteration 78, loss = 0.33567664\n",
      "Iteration 79, loss = 0.33637045\n",
      "Iteration 80, loss = 0.33506835\n",
      "Iteration 81, loss = 0.33498068\n",
      "Iteration 82, loss = 0.33484750\n",
      "Iteration 83, loss = 0.33541307\n",
      "Iteration 84, loss = 0.33461029\n",
      "Iteration 85, loss = 0.33595989\n",
      "Iteration 86, loss = 0.33409826\n",
      "Iteration 87, loss = 0.33371815\n",
      "Iteration 88, loss = 0.33354516\n",
      "Iteration 89, loss = 0.33392177\n",
      "Iteration 90, loss = 0.33320681\n",
      "Iteration 91, loss = 0.33322997\n",
      "Iteration 92, loss = 0.33322552\n",
      "Iteration 93, loss = 0.33386651\n",
      "Iteration 94, loss = 0.33424204\n",
      "Iteration 95, loss = 0.33374194\n",
      "Iteration 96, loss = 0.33229384\n",
      "Iteration 97, loss = 0.33196769\n",
      "Iteration 98, loss = 0.33308315\n",
      "Iteration 99, loss = 0.33152432\n",
      "Iteration 100, loss = 0.33121593\n",
      "Iteration 101, loss = 0.33152813\n",
      "Iteration 102, loss = 0.33222482\n",
      "Iteration 103, loss = 0.33359856\n",
      "Iteration 104, loss = 0.33229293\n",
      "Iteration 105, loss = 0.33068651\n",
      "Iteration 106, loss = 0.33033126\n",
      "Iteration 107, loss = 0.33024546\n",
      "Iteration 108, loss = 0.33092476\n",
      "Iteration 109, loss = 0.33074685\n",
      "Iteration 110, loss = 0.33206815\n",
      "Iteration 111, loss = 0.33075326\n",
      "Iteration 112, loss = 0.32988836\n",
      "Iteration 113, loss = 0.32946086\n",
      "Iteration 114, loss = 0.32973480\n",
      "Iteration 115, loss = 0.32971619\n",
      "Iteration 116, loss = 0.32879564\n",
      "Iteration 117, loss = 0.32925723\n",
      "Iteration 118, loss = 0.32913729\n",
      "Iteration 119, loss = 0.32888770\n",
      "Iteration 120, loss = 0.32873908\n",
      "Iteration 121, loss = 0.32832124\n",
      "Iteration 122, loss = 0.32942536\n",
      "Iteration 123, loss = 0.32844803\n",
      "Iteration 124, loss = 0.32798578\n",
      "Iteration 125, loss = 0.32850990\n",
      "Iteration 126, loss = 0.32932262\n",
      "Iteration 127, loss = 0.32986063\n",
      "Iteration 128, loss = 0.32781498\n",
      "Iteration 129, loss = 0.32829244\n",
      "Iteration 130, loss = 0.32770484\n",
      "Iteration 131, loss = 0.32779044\n",
      "Iteration 132, loss = 0.32871599\n",
      "Iteration 133, loss = 0.32673983\n",
      "Iteration 134, loss = 0.33017328\n",
      "Iteration 135, loss = 0.32736996\n",
      "Iteration 136, loss = 0.32715000\n",
      "Iteration 137, loss = 0.32678131\n",
      "Iteration 138, loss = 0.32676699\n",
      "Iteration 139, loss = 0.32650887\n",
      "Iteration 140, loss = 0.32687217\n",
      "Iteration 141, loss = 0.32607094\n",
      "Iteration 142, loss = 0.32645952\n",
      "Iteration 143, loss = 0.32732324\n",
      "Iteration 144, loss = 0.32596805\n",
      "Iteration 145, loss = 0.32734773\n",
      "Iteration 146, loss = 0.32715351\n",
      "Iteration 147, loss = 0.32530667\n",
      "Iteration 148, loss = 0.32539841\n",
      "Iteration 149, loss = 0.32520810\n",
      "Iteration 150, loss = 0.32916295\n",
      "Iteration 151, loss = 0.32859730\n",
      "Iteration 152, loss = 0.32611665\n",
      "Iteration 153, loss = 0.32491389\n",
      "Iteration 154, loss = 0.32482725\n",
      "Iteration 155, loss = 0.32433110\n",
      "Iteration 156, loss = 0.32478565\n",
      "Iteration 157, loss = 0.32482361\n",
      "Iteration 158, loss = 0.32474795\n",
      "Iteration 159, loss = 0.32466474\n",
      "Iteration 160, loss = 0.32454210\n",
      "Iteration 161, loss = 0.32451423\n",
      "Iteration 162, loss = 0.32430588\n",
      "Iteration 163, loss = 0.32445646\n",
      "Iteration 164, loss = 0.32339947\n",
      "Iteration 165, loss = 0.32399513\n",
      "Iteration 166, loss = 0.32607599\n",
      "Iteration 167, loss = 0.32537278\n",
      "Iteration 168, loss = 0.32374658\n",
      "Iteration 169, loss = 0.32383360\n",
      "Iteration 170, loss = 0.32335191\n",
      "Iteration 171, loss = 0.32489014\n",
      "Iteration 172, loss = 0.32889259\n",
      "Iteration 173, loss = 0.32476399\n",
      "Iteration 174, loss = 0.32411667\n",
      "Iteration 175, loss = 0.32427686\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "The training time of the model: 0.35 .\n",
      "accuracy: 0.89375\n",
      "f1: 0.0\n"
     ]
    }
   ],
   "source": [
    "#repeat model H = 4 multiple times\n",
    "mlp4 = MLPClassifier(hidden_layer_sizes=(4, ), solver='sgd', \n",
    "                     max_iter=500, random_state=1, verbose=True)\n",
    "start = time()\n",
    "mlp4.fit(X_train, y_train)\n",
    "NN_time = time() - start\n",
    "y_hat = mlp4.predict(X_test)\n",
    "print(\"The training time of the model: %.2f .\"\n",
    "      % (NN_time))\n",
    "print('accuracy:', accuracy_score(y_test, y_hat))\n",
    "print('f1:', f1_score(y_test, y_hat))\n",
    "setting4_time2 = NN_time\n",
    "setting4_acc2 = accuracy_score(y_test, y_hat)\n",
    "setting4_f12 = f1_score(y_test, y_hat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.56885491\n",
      "Iteration 2, loss = 0.55923261\n",
      "Iteration 3, loss = 0.55138118\n",
      "Iteration 4, loss = 0.54532775\n",
      "Iteration 5, loss = 0.53961766\n",
      "Iteration 6, loss = 0.53371240\n",
      "Iteration 7, loss = 0.52754186\n",
      "Iteration 8, loss = 0.52040361\n",
      "Iteration 9, loss = 0.51208757\n",
      "Iteration 10, loss = 0.49840882\n",
      "Iteration 11, loss = 0.47547416\n",
      "Iteration 12, loss = 0.42664142\n",
      "Iteration 13, loss = 0.35654341\n",
      "Iteration 14, loss = 0.33736962\n",
      "Iteration 15, loss = 0.35497482\n",
      "Iteration 16, loss = 0.34094519\n",
      "Iteration 17, loss = 0.32660354\n",
      "Iteration 18, loss = 0.33191529\n",
      "Iteration 19, loss = 0.32840508\n",
      "Iteration 20, loss = 0.32560950\n",
      "Iteration 21, loss = 0.32582050\n",
      "Iteration 22, loss = 0.32611202\n",
      "Iteration 23, loss = 0.32510264\n",
      "Iteration 24, loss = 0.32451637\n",
      "Iteration 25, loss = 0.32361894\n",
      "Iteration 26, loss = 0.32291613\n",
      "Iteration 27, loss = 0.32361582\n",
      "Iteration 28, loss = 0.32191396\n",
      "Iteration 29, loss = 0.32182101\n",
      "Iteration 30, loss = 0.32268437\n",
      "Iteration 31, loss = 0.32322738\n",
      "Iteration 32, loss = 0.32086008\n",
      "Iteration 33, loss = 0.32076646\n",
      "Iteration 34, loss = 0.32263364\n",
      "Iteration 35, loss = 0.32157754\n",
      "Iteration 36, loss = 0.32043203\n",
      "Iteration 37, loss = 0.32028114\n",
      "Iteration 38, loss = 0.32033426\n",
      "Iteration 39, loss = 0.31851625\n",
      "Iteration 40, loss = 0.31764780\n",
      "Iteration 41, loss = 0.31847625\n",
      "Iteration 42, loss = 0.31728323\n",
      "Iteration 43, loss = 0.31776897\n",
      "Iteration 44, loss = 0.31736227\n",
      "Iteration 45, loss = 0.31588162\n",
      "Iteration 46, loss = 0.31707442\n",
      "Iteration 47, loss = 0.31748468\n",
      "Iteration 48, loss = 0.31713287\n",
      "Iteration 49, loss = 0.31737600\n",
      "Iteration 50, loss = 0.31661753\n",
      "Iteration 51, loss = 0.31487996\n",
      "Iteration 52, loss = 0.31525488\n",
      "Iteration 53, loss = 0.31630469\n",
      "Iteration 54, loss = 0.31477650\n",
      "Iteration 55, loss = 0.31387490\n",
      "Iteration 56, loss = 0.31373109\n",
      "Iteration 57, loss = 0.31290211\n",
      "Iteration 58, loss = 0.31437025\n",
      "Iteration 59, loss = 0.31285874\n",
      "Iteration 60, loss = 0.31344806\n",
      "Iteration 61, loss = 0.31581055\n",
      "Iteration 62, loss = 0.31392163\n",
      "Iteration 63, loss = 0.31143391\n",
      "Iteration 64, loss = 0.31314957\n",
      "Iteration 65, loss = 0.31374372\n",
      "Iteration 66, loss = 0.31259580\n",
      "Iteration 67, loss = 0.31054405\n",
      "Iteration 68, loss = 0.31041219\n",
      "Iteration 69, loss = 0.31178045\n",
      "Iteration 70, loss = 0.31055576\n",
      "Iteration 71, loss = 0.31047344\n",
      "Iteration 72, loss = 0.30985390\n",
      "Iteration 73, loss = 0.31088913\n",
      "Iteration 74, loss = 0.30892988\n",
      "Iteration 75, loss = 0.31030908\n",
      "Iteration 76, loss = 0.31127610\n",
      "Iteration 77, loss = 0.31039058\n",
      "Iteration 78, loss = 0.30830155\n",
      "Iteration 79, loss = 0.30860989\n",
      "Iteration 80, loss = 0.30851554\n",
      "Iteration 81, loss = 0.30749507\n",
      "Iteration 82, loss = 0.30755879\n",
      "Iteration 83, loss = 0.30756712\n",
      "Iteration 84, loss = 0.30724753\n",
      "Iteration 85, loss = 0.30782615\n",
      "Iteration 86, loss = 0.30720435\n",
      "Iteration 87, loss = 0.30776145\n",
      "Iteration 88, loss = 0.30672977\n",
      "Iteration 89, loss = 0.30707722\n",
      "Iteration 90, loss = 0.30769304\n",
      "Iteration 91, loss = 0.30661771\n",
      "Iteration 92, loss = 0.30565398\n",
      "Iteration 93, loss = 0.30608669\n",
      "Iteration 94, loss = 0.30586971\n",
      "Iteration 95, loss = 0.30632660\n",
      "Iteration 96, loss = 0.30832176\n",
      "Iteration 97, loss = 0.30709825\n",
      "Iteration 98, loss = 0.30529944\n",
      "Iteration 99, loss = 0.30500566\n",
      "Iteration 100, loss = 0.30470442\n",
      "Iteration 101, loss = 0.30456488\n",
      "Iteration 102, loss = 0.30458109\n",
      "Iteration 103, loss = 0.30496598\n",
      "Iteration 104, loss = 0.30406176\n",
      "Iteration 105, loss = 0.30425634\n",
      "Iteration 106, loss = 0.30516516\n",
      "Iteration 107, loss = 0.30434584\n",
      "Iteration 108, loss = 0.30424207\n",
      "Iteration 109, loss = 0.30478468\n",
      "Iteration 110, loss = 0.30451834\n",
      "Iteration 111, loss = 0.30380464\n",
      "Iteration 112, loss = 0.30424871\n",
      "Iteration 113, loss = 0.30308710\n",
      "Iteration 114, loss = 0.30355735\n",
      "Iteration 115, loss = 0.30300641\n",
      "Iteration 116, loss = 0.30274329\n",
      "Iteration 117, loss = 0.30252307\n",
      "Iteration 118, loss = 0.30253743\n",
      "Iteration 119, loss = 0.30311589\n",
      "Iteration 120, loss = 0.30235495\n",
      "Iteration 121, loss = 0.30254791\n",
      "Iteration 122, loss = 0.30287036\n",
      "Iteration 123, loss = 0.30330200\n",
      "Iteration 124, loss = 0.30194297\n",
      "Iteration 125, loss = 0.30267502\n",
      "Iteration 126, loss = 0.30232018\n",
      "Iteration 127, loss = 0.30149700\n",
      "Iteration 128, loss = 0.30124309\n",
      "Iteration 129, loss = 0.30166881\n",
      "Iteration 130, loss = 0.30234197\n",
      "Iteration 131, loss = 0.30129284\n",
      "Iteration 132, loss = 0.30311274\n",
      "Iteration 133, loss = 0.30083639\n",
      "Iteration 134, loss = 0.30102333\n",
      "Iteration 135, loss = 0.30074655\n",
      "Iteration 136, loss = 0.30067930\n",
      "Iteration 137, loss = 0.30106807\n",
      "Iteration 138, loss = 0.30025236\n",
      "Iteration 139, loss = 0.30082167\n",
      "Iteration 140, loss = 0.30066017\n",
      "Iteration 141, loss = 0.30110927\n",
      "Iteration 142, loss = 0.30124284\n",
      "Iteration 143, loss = 0.30072472\n",
      "Iteration 144, loss = 0.29975286\n",
      "Iteration 145, loss = 0.29987915\n",
      "Iteration 146, loss = 0.29995989\n",
      "Iteration 147, loss = 0.29943086\n",
      "Iteration 148, loss = 0.29917559\n",
      "Iteration 149, loss = 0.29958830\n",
      "Iteration 150, loss = 0.29992246\n",
      "Iteration 151, loss = 0.30031233\n",
      "Iteration 152, loss = 0.30154995\n",
      "Iteration 153, loss = 0.30006151\n",
      "Iteration 154, loss = 0.29915076\n",
      "Iteration 155, loss = 0.29978644\n",
      "Iteration 156, loss = 0.29891144\n",
      "Iteration 157, loss = 0.29870484\n",
      "Iteration 158, loss = 0.29852741\n",
      "Iteration 159, loss = 0.29896215\n",
      "Iteration 160, loss = 0.29847818\n",
      "Iteration 161, loss = 0.29866256\n",
      "Iteration 162, loss = 0.29840018\n",
      "Iteration 163, loss = 0.29843237\n",
      "Iteration 164, loss = 0.29862891\n",
      "Iteration 165, loss = 0.29839307\n",
      "Iteration 166, loss = 0.29807862\n",
      "Iteration 167, loss = 0.29829030\n",
      "Iteration 168, loss = 0.29808184\n",
      "Iteration 169, loss = 0.29865381\n",
      "Iteration 170, loss = 0.29924667\n",
      "Iteration 171, loss = 0.29818060\n",
      "Iteration 172, loss = 0.29770169\n",
      "Iteration 173, loss = 0.29828171\n",
      "Iteration 174, loss = 0.29931952\n",
      "Iteration 175, loss = 0.29875330\n",
      "Iteration 176, loss = 0.29722022\n",
      "Iteration 177, loss = 0.29713287\n",
      "Iteration 178, loss = 0.29726306\n",
      "Iteration 179, loss = 0.29736179\n",
      "Iteration 180, loss = 0.29715451\n",
      "Iteration 181, loss = 0.29814038\n",
      "Iteration 182, loss = 0.29742247\n",
      "Iteration 183, loss = 0.29692286\n",
      "Iteration 184, loss = 0.29676100\n",
      "Iteration 185, loss = 0.29665681\n",
      "Iteration 186, loss = 0.29774471\n",
      "Iteration 187, loss = 0.29805171\n",
      "Iteration 188, loss = 0.29815126\n",
      "Iteration 189, loss = 0.29799938\n",
      "Iteration 190, loss = 0.29748717\n",
      "Iteration 191, loss = 0.29576500\n",
      "Iteration 192, loss = 0.29868637\n",
      "Iteration 193, loss = 0.29888283\n",
      "Iteration 194, loss = 0.29773036\n",
      "Iteration 195, loss = 0.29630502\n",
      "Iteration 196, loss = 0.29658586\n",
      "Iteration 197, loss = 0.29608392\n",
      "Iteration 198, loss = 0.29614192\n",
      "Iteration 199, loss = 0.29653510\n",
      "Iteration 200, loss = 0.29602789\n",
      "Iteration 201, loss = 0.29634675\n",
      "Iteration 202, loss = 0.29588199\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "The training time of the model: 0.37 .\n",
      "accuracy: 0.8875\n",
      "f1: 0.0\n",
      "The mean training time of the model H = 4: 0.25299024581909180 (std: 0.15457256367309091) .\n",
      "The mean accuracy time of the model H = 4: 0.89166666666666661 (std: 0.00294627825494399) .\n",
      "The mean F1 score time of the model H = 4: 0.00000000000000000 (std: 0.00000000000000000) .\n"
     ]
    }
   ],
   "source": [
    "#repeat model H = 4 multiple times\n",
    "#report the mean and standard deviation of the training time, accuracy, and F1 score for the setting.\n",
    "mlp4 = MLPClassifier(hidden_layer_sizes=(4, ), solver='sgd', \n",
    "                     max_iter=500, random_state=2, verbose=True)\n",
    "start = time()\n",
    "mlp4.fit(X_train, y_train)\n",
    "NN_time = time() - start\n",
    "y_hat = mlp4.predict(X_test)\n",
    "print(\"The training time of the model: %.2f .\"\n",
    "      % (NN_time))\n",
    "print('accuracy:', accuracy_score(y_test, y_hat))\n",
    "print('f1:', f1_score(y_test, y_hat))\n",
    "setting4_time3 = NN_time\n",
    "setting4_acc3 = accuracy_score(y_test, y_hat)\n",
    "setting4_f13 = f1_score(y_test, y_hat)\n",
    "set4_time = np.array([setting4_time1, setting4_time2, setting4_time3])\n",
    "mean_time = np.mean(set4_time)\n",
    "std_time = np.std(set4_time)\n",
    "set4_acc = np.array([setting4_acc1, setting4_acc2, setting4_acc3])\n",
    "mean4_acc = np.mean(set4_acc)\n",
    "std_acc = np.std(set4_acc)\n",
    "set4_f1 = np.array([setting4_f1, setting4_f12, setting4_f13])\n",
    "mean4_f1 = np.mean(set4_f1)\n",
    "std_f1 = np.std(set4_f1)\n",
    "print(\"The mean training time of the model H = 4: %.17f (std: %.17f) .\"\n",
    "      % (mean_time, std_time))\n",
    "print(\"The mean accuracy time of the model H = 4: %.17f (std: %.17f) .\"\n",
    "      % (mean4_acc, std_acc))\n",
    "print(\"The mean F1 score time of the model H = 4: %.17f (std: %.17f) .\"\n",
    "      % (mean4_f1, std_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = inf\n",
      "Iteration 2, loss = inf\n",
      "Iteration 3, loss = 1.77549404\n",
      "Iteration 4, loss = 1.28025009\n",
      "Iteration 5, loss = 0.73615723\n",
      "Iteration 6, loss = 0.49613119\n",
      "Iteration 7, loss = 0.46746807\n",
      "Iteration 8, loss = 1.69110568\n",
      "Iteration 9, loss = 1.16074402\n",
      "Iteration 10, loss = 1.26086396\n",
      "Iteration 11, loss = 1.22193611\n",
      "Iteration 12, loss = 0.67800487\n",
      "Iteration 13, loss = 0.37081311\n",
      "Iteration 14, loss = 0.33238445\n",
      "Iteration 15, loss = 0.84114159\n",
      "Iteration 16, loss = 0.47306759\n",
      "Iteration 17, loss = 0.32705077\n",
      "Iteration 18, loss = 0.31820200\n",
      "Iteration 19, loss = 0.37144144\n",
      "Iteration 20, loss = 0.30492235\n",
      "Iteration 21, loss = 0.31206430\n",
      "Iteration 22, loss = 0.34462353\n",
      "Iteration 23, loss = 0.30220939\n",
      "Iteration 24, loss = 0.32331055\n",
      "Iteration 25, loss = 0.33851231\n",
      "Iteration 26, loss = 0.30238929\n",
      "Iteration 27, loss = 0.31765740\n",
      "Iteration 28, loss = 0.29421428\n",
      "Iteration 29, loss = 0.31345128\n",
      "Iteration 30, loss = 0.30613590\n",
      "Iteration 31, loss = 0.32202940\n",
      "Iteration 32, loss = 0.32379914\n",
      "Iteration 33, loss = 0.30235721\n",
      "Iteration 34, loss = 0.29450574\n",
      "Iteration 35, loss = 0.36874482\n",
      "Iteration 36, loss = 0.29901880\n",
      "Iteration 37, loss = 0.29390145\n",
      "Iteration 38, loss = 0.30251637\n",
      "Iteration 39, loss = 0.29277614\n",
      "Iteration 40, loss = 0.29377920\n",
      "Iteration 41, loss = 0.30231625\n",
      "Iteration 42, loss = 0.29907560\n",
      "Iteration 43, loss = 0.33355996\n",
      "Iteration 44, loss = 0.29113075\n",
      "Iteration 45, loss = 0.30449825\n",
      "Iteration 46, loss = 0.29250520\n",
      "Iteration 47, loss = 0.31224141\n",
      "Iteration 48, loss = 0.30658545\n",
      "Iteration 49, loss = 0.28667511\n",
      "Iteration 50, loss = 0.28673830\n",
      "Iteration 51, loss = 0.29514424\n",
      "Iteration 52, loss = 0.28336422\n",
      "Iteration 53, loss = 0.30043442\n",
      "Iteration 54, loss = 0.28482321\n",
      "Iteration 55, loss = 0.28239505\n",
      "Iteration 56, loss = 0.28217435\n",
      "Iteration 57, loss = 0.29602689\n",
      "Iteration 58, loss = 0.32655315\n",
      "Iteration 59, loss = 0.30019428\n",
      "Iteration 60, loss = 0.29630610\n",
      "Iteration 61, loss = 0.28449433\n",
      "Iteration 62, loss = 0.28474605\n",
      "Iteration 63, loss = 0.30180231\n",
      "Iteration 64, loss = 0.28913185\n",
      "Iteration 65, loss = 0.27875285\n",
      "Iteration 66, loss = 0.27739390\n",
      "Iteration 67, loss = 0.31715001\n",
      "Iteration 68, loss = 0.28480125\n",
      "Iteration 69, loss = 0.28093804\n",
      "Iteration 70, loss = 0.27867871\n",
      "Iteration 71, loss = 0.28344233\n",
      "Iteration 72, loss = 0.29697244\n",
      "Iteration 73, loss = 0.27905902\n",
      "Iteration 74, loss = 0.28567492\n",
      "Iteration 75, loss = 0.27610399\n",
      "Iteration 76, loss = 0.27832223\n",
      "Iteration 77, loss = 0.31974837\n",
      "Iteration 78, loss = 0.28849592\n",
      "Iteration 79, loss = 0.27548631\n",
      "Iteration 80, loss = 0.27922309\n",
      "Iteration 81, loss = 0.27586354\n",
      "Iteration 82, loss = 0.27283967\n",
      "Iteration 83, loss = 0.29141886\n",
      "Iteration 84, loss = 0.27321692\n",
      "Iteration 85, loss = 0.27158727\n",
      "Iteration 86, loss = 0.27598958\n",
      "Iteration 87, loss = 0.31008406\n",
      "Iteration 88, loss = 0.37045794\n",
      "Iteration 89, loss = 0.30843619\n",
      "Iteration 90, loss = 0.29438437\n",
      "Iteration 91, loss = 0.27559583\n",
      "Iteration 92, loss = 0.27889120\n",
      "Iteration 93, loss = 0.29455595\n",
      "Iteration 94, loss = 0.26952511\n",
      "Iteration 95, loss = 0.27033963\n",
      "Iteration 96, loss = 0.27142027\n",
      "Iteration 97, loss = 0.27499258\n",
      "Iteration 98, loss = 0.29220084\n",
      "Iteration 99, loss = 0.31431794\n",
      "Iteration 100, loss = 0.27255673\n",
      "Iteration 101, loss = 0.29065535\n",
      "Iteration 102, loss = 0.27418522\n",
      "Iteration 103, loss = 0.26960415\n",
      "Iteration 104, loss = 0.27127203\n",
      "Iteration 105, loss = 0.27205458\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "The training time of the model: 0.23 .\n",
      "accuracy: 0.9125\n",
      "f1: 0.3636363636363636\n"
     ]
    }
   ],
   "source": [
    "#model H = 8\n",
    "mlp8 = MLPClassifier(hidden_layer_sizes=(8, ), solver='sgd', \n",
    "                     max_iter=500, random_state=0, verbose=True)\n",
    "start = time()\n",
    "mlp8.fit(X_train, y_train)\n",
    "NN_time = time() - start\n",
    "y_hat = mlp8.predict(X_test)\n",
    "print(\"The training time of the model: %.2f .\"\n",
    "      % (NN_time))\n",
    "print('accuracy:', accuracy_score(y_test, y_hat))\n",
    "print('f1:', f1_score(y_test, y_hat))\n",
    "setting8_time1 = NN_time\n",
    "setting8_acc1 = accuracy_score(y_test, y_hat)\n",
    "setting8_f1 = f1_score(y_test, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 3.37509473\n",
      "Iteration 2, loss = inf\n",
      "Iteration 3, loss = 1.06223897\n",
      "Iteration 4, loss = 4.56345918\n",
      "Iteration 5, loss = 3.75067508\n",
      "Iteration 6, loss = 2.09237178\n",
      "Iteration 7, loss = 0.58541858\n",
      "Iteration 8, loss = 0.57202823\n",
      "Iteration 9, loss = 0.40006155\n",
      "Iteration 10, loss = 0.37500265\n",
      "Iteration 11, loss = 0.37338811\n",
      "Iteration 12, loss = 0.36557775\n",
      "Iteration 13, loss = 0.35502810\n",
      "Iteration 14, loss = 0.34747155\n",
      "Iteration 15, loss = 0.34101753\n",
      "Iteration 16, loss = 0.33671519\n",
      "Iteration 17, loss = 0.33394253\n",
      "Iteration 18, loss = 0.33088486\n",
      "Iteration 19, loss = 0.32943512\n",
      "Iteration 20, loss = 0.32764251\n",
      "Iteration 21, loss = 0.32619279\n",
      "Iteration 22, loss = 0.32632494\n",
      "Iteration 23, loss = 0.32572769\n",
      "Iteration 24, loss = 0.32330685\n",
      "Iteration 25, loss = 0.32327442\n",
      "Iteration 26, loss = 0.32090970\n",
      "Iteration 27, loss = 0.32107374\n",
      "Iteration 28, loss = 0.32093964\n",
      "Iteration 29, loss = 0.32067009\n",
      "Iteration 30, loss = 0.31752379\n",
      "Iteration 31, loss = 0.31762338\n",
      "Iteration 32, loss = 0.31846389\n",
      "Iteration 33, loss = 0.31505355\n",
      "Iteration 34, loss = 0.31518200\n",
      "Iteration 35, loss = 0.31371639\n",
      "Iteration 36, loss = 0.31288520\n",
      "Iteration 37, loss = 0.31456107\n",
      "Iteration 38, loss = 0.31258497\n",
      "Iteration 39, loss = 0.31080150\n",
      "Iteration 40, loss = 0.31150690\n",
      "Iteration 41, loss = 0.31076156\n",
      "Iteration 42, loss = 0.30988715\n",
      "Iteration 43, loss = 0.31066796\n",
      "Iteration 44, loss = 0.31112935\n",
      "Iteration 45, loss = 0.30931225\n",
      "Iteration 46, loss = 0.30814775\n",
      "Iteration 47, loss = 0.30758206\n",
      "Iteration 48, loss = 0.30732968\n",
      "Iteration 49, loss = 0.30663703\n",
      "Iteration 50, loss = 0.30638669\n",
      "Iteration 51, loss = 0.30606703\n",
      "Iteration 52, loss = 0.30594690\n",
      "Iteration 53, loss = 0.30586151\n",
      "Iteration 54, loss = 0.30616316\n",
      "Iteration 55, loss = 0.30507433\n",
      "Iteration 56, loss = 0.30506898\n",
      "Iteration 57, loss = 0.30382512\n",
      "Iteration 58, loss = 0.30884658\n",
      "Iteration 59, loss = 0.30420071\n",
      "Iteration 60, loss = 0.30342348\n",
      "Iteration 61, loss = 0.30509514\n",
      "Iteration 62, loss = 0.30430488\n",
      "Iteration 63, loss = 0.30412236\n",
      "Iteration 64, loss = 0.30316024\n",
      "Iteration 65, loss = 0.30179054\n",
      "Iteration 66, loss = 0.30152854\n",
      "Iteration 67, loss = 0.30298003\n",
      "Iteration 68, loss = 0.30255867\n",
      "Iteration 69, loss = 0.30107409\n",
      "Iteration 70, loss = 0.30050723\n",
      "Iteration 71, loss = 0.30020237\n",
      "Iteration 72, loss = 0.30020147\n",
      "Iteration 73, loss = 0.29945203\n",
      "Iteration 74, loss = 0.30208246\n",
      "Iteration 75, loss = 0.29952158\n",
      "Iteration 76, loss = 0.29933960\n",
      "Iteration 77, loss = 0.29893225\n",
      "Iteration 78, loss = 0.29848884\n",
      "Iteration 79, loss = 0.29708539\n",
      "Iteration 80, loss = 0.29965898\n",
      "Iteration 81, loss = 0.29963929\n",
      "Iteration 82, loss = 0.29820098\n",
      "Iteration 83, loss = 0.29668853\n",
      "Iteration 84, loss = 0.29644498\n",
      "Iteration 85, loss = 0.29639793\n",
      "Iteration 86, loss = 0.29745237\n",
      "Iteration 87, loss = 0.29702885\n",
      "Iteration 88, loss = 0.29659354\n",
      "Iteration 89, loss = 0.29578678\n",
      "Iteration 90, loss = 0.29711868\n",
      "Iteration 91, loss = 0.29591319\n",
      "Iteration 92, loss = 0.29574628\n",
      "Iteration 93, loss = 0.29652318\n",
      "Iteration 94, loss = 0.29588471\n",
      "Iteration 95, loss = 0.29461299\n",
      "Iteration 96, loss = 0.29414826\n",
      "Iteration 97, loss = 0.29369610\n",
      "Iteration 98, loss = 0.29294836\n",
      "Iteration 99, loss = 0.29370066\n",
      "Iteration 100, loss = 0.29271606\n",
      "Iteration 101, loss = 0.29598691\n",
      "Iteration 102, loss = 0.29384595\n",
      "Iteration 103, loss = 0.29212742\n",
      "Iteration 104, loss = 0.29208865\n",
      "Iteration 105, loss = 0.29151856\n",
      "Iteration 106, loss = 0.29256076\n",
      "Iteration 107, loss = 0.29127828\n",
      "Iteration 108, loss = 0.29139381\n",
      "Iteration 109, loss = 0.29148740\n",
      "Iteration 110, loss = 0.29199044\n",
      "Iteration 111, loss = 0.29099551\n",
      "Iteration 112, loss = 0.29117352\n",
      "Iteration 113, loss = 0.28977657\n",
      "Iteration 114, loss = 0.29027773\n",
      "Iteration 115, loss = 0.29014509\n",
      "Iteration 116, loss = 0.29084073\n",
      "Iteration 117, loss = 0.29020112\n",
      "Iteration 118, loss = 0.29011816\n",
      "Iteration 119, loss = 0.28981112\n",
      "Iteration 120, loss = 0.29053102\n",
      "Iteration 121, loss = 0.28917783\n",
      "Iteration 122, loss = 0.29101506\n",
      "Iteration 123, loss = 0.29040502\n",
      "Iteration 124, loss = 0.28903064\n",
      "Iteration 125, loss = 0.28873087\n",
      "Iteration 126, loss = 0.28902248\n",
      "Iteration 127, loss = 0.28984251\n",
      "Iteration 128, loss = 0.28803939\n",
      "Iteration 129, loss = 0.28926264\n",
      "Iteration 130, loss = 0.28983660\n",
      "Iteration 131, loss = 0.28884693\n",
      "Iteration 132, loss = 0.28847741\n",
      "Iteration 133, loss = 0.28880641\n",
      "Iteration 134, loss = 0.28881958\n",
      "Iteration 135, loss = 0.28911995\n",
      "Iteration 136, loss = 0.28845573\n",
      "Iteration 137, loss = 0.28855555\n",
      "Iteration 138, loss = 0.28754651\n",
      "Iteration 139, loss = 0.29599834\n",
      "Iteration 140, loss = 0.29364316\n",
      "Iteration 141, loss = 0.28866977\n",
      "Iteration 142, loss = 0.28651083\n",
      "Iteration 143, loss = 0.28710719\n",
      "Iteration 144, loss = 0.28858268\n",
      "Iteration 145, loss = 0.28719586\n",
      "Iteration 146, loss = 0.28712827\n",
      "Iteration 147, loss = 0.28937072\n",
      "Iteration 148, loss = 0.28786821\n",
      "Iteration 149, loss = 0.29443352\n",
      "Iteration 150, loss = 0.29124807\n",
      "Iteration 151, loss = 0.28648244\n",
      "Iteration 152, loss = 0.28611897\n",
      "Iteration 153, loss = 0.28658841\n",
      "Iteration 154, loss = 0.28602158\n",
      "Iteration 155, loss = 0.28603211\n",
      "Iteration 156, loss = 0.28522903\n",
      "Iteration 157, loss = 0.28615048\n",
      "Iteration 158, loss = 0.28567674\n",
      "Iteration 159, loss = 0.28719327\n",
      "Iteration 160, loss = 0.28720427\n",
      "Iteration 161, loss = 0.28643138\n",
      "Iteration 162, loss = 0.28577329\n",
      "Iteration 163, loss = 0.28615846\n",
      "Iteration 164, loss = 0.28529037\n",
      "Iteration 165, loss = 0.28510158\n",
      "Iteration 166, loss = 0.28568404\n",
      "Iteration 167, loss = 0.28534036\n",
      "Iteration 168, loss = 0.28435450\n",
      "Iteration 169, loss = 0.28413367\n",
      "Iteration 170, loss = 0.28386228\n",
      "Iteration 171, loss = 0.28406362\n",
      "Iteration 172, loss = 0.28373034\n",
      "Iteration 173, loss = 0.28337257\n",
      "Iteration 174, loss = 0.28325595\n",
      "Iteration 175, loss = 0.28348012\n",
      "Iteration 176, loss = 0.28325199\n",
      "Iteration 177, loss = 0.28504105\n",
      "Iteration 178, loss = 0.28823435\n",
      "Iteration 179, loss = 0.28712334\n",
      "Iteration 180, loss = 0.28399464\n",
      "Iteration 181, loss = 0.28363591\n",
      "Iteration 182, loss = 0.28391872\n",
      "Iteration 183, loss = 0.28300022\n",
      "Iteration 184, loss = 0.28244440\n",
      "Iteration 185, loss = 0.28328487\n",
      "Iteration 186, loss = 0.28306065\n",
      "Iteration 187, loss = 0.28270165\n",
      "Iteration 188, loss = 0.28310837\n",
      "Iteration 189, loss = 0.28242447\n",
      "Iteration 190, loss = 0.28287397\n",
      "Iteration 191, loss = 0.28203201\n",
      "Iteration 192, loss = 0.28123033\n",
      "Iteration 193, loss = 0.28316502\n",
      "Iteration 194, loss = 0.28259150\n",
      "Iteration 195, loss = 0.28241556\n",
      "Iteration 196, loss = 0.28219680\n",
      "Iteration 197, loss = 0.28256168\n",
      "Iteration 198, loss = 0.28408572\n",
      "Iteration 199, loss = 0.28223159\n",
      "Iteration 200, loss = 0.28061153\n",
      "Iteration 201, loss = 0.28081708\n",
      "Iteration 202, loss = 0.28123711\n",
      "Iteration 203, loss = 0.28138134\n",
      "Iteration 204, loss = 0.28054300\n",
      "Iteration 205, loss = 0.28069003\n",
      "Iteration 206, loss = 0.28041038\n",
      "Iteration 207, loss = 0.28050680\n",
      "Iteration 208, loss = 0.28014627\n",
      "Iteration 209, loss = 0.27915263\n",
      "Iteration 210, loss = 0.27956300\n",
      "Iteration 211, loss = 0.27976775\n",
      "Iteration 212, loss = 0.27990332\n",
      "Iteration 213, loss = 0.27947950\n",
      "Iteration 214, loss = 0.27932531\n",
      "Iteration 215, loss = 0.27935479\n",
      "Iteration 216, loss = 0.27811934\n",
      "Iteration 217, loss = 0.27769669\n",
      "Iteration 218, loss = 0.27852424\n",
      "Iteration 219, loss = 0.27848732\n",
      "Iteration 220, loss = 0.27774989\n",
      "Iteration 221, loss = 0.27796556\n",
      "Iteration 222, loss = 0.27818053\n",
      "Iteration 223, loss = 0.27914664\n",
      "Iteration 224, loss = 0.27848098\n",
      "Iteration 225, loss = 0.27860843\n",
      "Iteration 226, loss = 0.27772454\n",
      "Iteration 227, loss = 0.27755722\n",
      "Iteration 228, loss = 0.27736298\n",
      "Iteration 229, loss = 0.27704459\n",
      "Iteration 230, loss = 0.27625484\n",
      "Iteration 231, loss = 0.27643768\n",
      "Iteration 232, loss = 0.27669647\n",
      "Iteration 233, loss = 0.27658903\n",
      "Iteration 234, loss = 0.27734598\n",
      "Iteration 235, loss = 0.27744755\n",
      "Iteration 236, loss = 0.27608450\n",
      "Iteration 237, loss = 0.27912686\n",
      "Iteration 238, loss = 0.27768475\n",
      "Iteration 239, loss = 0.27741355\n",
      "Iteration 240, loss = 0.27753369\n",
      "Iteration 241, loss = 0.27572378\n",
      "Iteration 242, loss = 0.27706787\n",
      "Iteration 243, loss = 0.27763860\n",
      "Iteration 244, loss = 0.27550948\n",
      "Iteration 245, loss = 0.27623259\n",
      "Iteration 246, loss = 0.27480059\n",
      "Iteration 247, loss = 0.27488415\n",
      "Iteration 248, loss = 0.27545012\n",
      "Iteration 249, loss = 0.27491327\n",
      "Iteration 250, loss = 0.27567306\n",
      "Iteration 251, loss = 0.27506267\n",
      "Iteration 252, loss = 0.27588874\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 253, loss = 0.27559155\n",
      "Iteration 254, loss = 0.27382964\n",
      "Iteration 255, loss = 0.27471663\n",
      "Iteration 256, loss = 0.27422831\n",
      "Iteration 257, loss = 0.27409901\n",
      "Iteration 258, loss = 0.27387486\n",
      "Iteration 259, loss = 0.27422977\n",
      "Iteration 260, loss = 0.27477330\n",
      "Iteration 261, loss = 0.27369443\n",
      "Iteration 262, loss = 0.27231294\n",
      "Iteration 263, loss = 0.27313734\n",
      "Iteration 264, loss = 0.27213793\n",
      "Iteration 265, loss = 0.27176937\n",
      "Iteration 266, loss = 0.27202409\n",
      "Iteration 267, loss = 0.27126997\n",
      "Iteration 268, loss = 0.27192601\n",
      "Iteration 269, loss = 0.27124958\n",
      "Iteration 270, loss = 0.27016843\n",
      "Iteration 271, loss = 0.27047660\n",
      "Iteration 272, loss = 0.27092712\n",
      "Iteration 273, loss = 0.27059933\n",
      "Iteration 274, loss = 0.27143257\n",
      "Iteration 275, loss = 0.27099063\n",
      "Iteration 276, loss = 0.27001876\n",
      "Iteration 277, loss = 0.26992250\n",
      "Iteration 278, loss = 0.26961371\n",
      "Iteration 279, loss = 0.26985038\n",
      "Iteration 280, loss = 0.27013897\n",
      "Iteration 281, loss = 0.26970601\n",
      "Iteration 282, loss = 0.26979242\n",
      "Iteration 283, loss = 0.27006126\n",
      "Iteration 284, loss = 0.26945855\n",
      "Iteration 285, loss = 0.26882803\n",
      "Iteration 286, loss = 0.26994372\n",
      "Iteration 287, loss = 0.27194883\n",
      "Iteration 288, loss = 0.26778957\n",
      "Iteration 289, loss = 0.27167678\n",
      "Iteration 290, loss = 0.27732853\n",
      "Iteration 291, loss = 0.27318971\n",
      "Iteration 292, loss = 0.26928024\n",
      "Iteration 293, loss = 0.26839683\n",
      "Iteration 294, loss = 0.26992373\n",
      "Iteration 295, loss = 0.26806334\n",
      "Iteration 296, loss = 0.26843928\n",
      "Iteration 297, loss = 0.26949317\n",
      "Iteration 298, loss = 0.26978892\n",
      "Iteration 299, loss = 0.26888223\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "The training time of the model: 0.62 .\n",
      "accuracy: 0.8875\n",
      "f1: 0.0\n"
     ]
    }
   ],
   "source": [
    "#repeat model H = 8 multiple times\n",
    "mlp8 = MLPClassifier(hidden_layer_sizes=(8, ), solver='sgd', \n",
    "                     max_iter=500, random_state=1, verbose=True)\n",
    "start = time()\n",
    "mlp8.fit(X_train, y_train)\n",
    "NN_time = time() - start\n",
    "y_hat = mlp8.predict(X_test)\n",
    "print(\"The training time of the model: %.2f .\"\n",
    "      % (NN_time))\n",
    "print('accuracy:', accuracy_score(y_test, y_hat))\n",
    "print('f1:', f1_score(y_test, y_hat))\n",
    "setting8_time2 = NN_time\n",
    "setting8_acc2 = accuracy_score(y_test, y_hat)\n",
    "setting8_f12 = f1_score(y_test, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = inf\n",
      "Iteration 2, loss = 2.24644564\n",
      "Iteration 3, loss = 0.75559819\n",
      "Iteration 4, loss = 0.49643464\n",
      "Iteration 5, loss = 0.52821546\n",
      "Iteration 6, loss = 0.52752435\n",
      "Iteration 7, loss = 0.52688450\n",
      "Iteration 8, loss = 0.52482572\n",
      "Iteration 9, loss = 0.52128440\n",
      "Iteration 10, loss = 0.51761662\n",
      "Iteration 11, loss = 0.51334746\n",
      "Iteration 12, loss = 0.50835013\n",
      "Iteration 13, loss = 0.50394469\n",
      "Iteration 14, loss = 0.50130744\n",
      "Iteration 15, loss = 0.49735612\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "The training time of the model: 0.04 .\n",
      "accuracy: 0.89375\n",
      "f1: 0.0\n",
      "The mean training time of the model H = 8: 0.29620814323425293 (std: 0.24259683852103708) .\n",
      "The mean accuracy time of the model H = 8: 0.89791666666666659 (std: 0.01062295731998497) .\n",
      "The mean F1 score time of the model H = 8: 0.12121212121212120 (std: 0.17141982574219330) .\n"
     ]
    }
   ],
   "source": [
    "#repeat model H = 8 multiple times\n",
    "#report the mean and standard deviation of the training time, accuracy, and F1 score for the setting.\n",
    "mlp8 = MLPClassifier(hidden_layer_sizes=(8, ), solver='sgd', \n",
    "                     max_iter=500, random_state=2, verbose=True)\n",
    "start = time()\n",
    "mlp8.fit(X_train, y_train)\n",
    "NN_time = time() - start\n",
    "y_hat = mlp8.predict(X_test)\n",
    "print(\"The training time of the model: %.2f .\"\n",
    "      % (NN_time))\n",
    "print('accuracy:', accuracy_score(y_test, y_hat))\n",
    "print('f1:', f1_score(y_test, y_hat))\n",
    "setting8_time3 = NN_time\n",
    "setting8_acc3 = accuracy_score(y_test, y_hat)\n",
    "setting8_f13 = f1_score(y_test, y_hat)\n",
    "set8_time = np.array([setting8_time1, setting8_time2, setting8_time3])\n",
    "mean_time = np.mean(set8_time)\n",
    "std_time = np.std(set8_time)\n",
    "set8_acc = np.array([setting8_acc1, setting8_acc2, setting8_acc3])\n",
    "mean8_acc = np.mean(set8_acc)\n",
    "std_acc = np.std(set8_acc)\n",
    "set8_f1 = np.array([setting8_f1, setting8_f12, setting8_f13])\n",
    "mean8_f1 = np.mean(set8_f1)\n",
    "std_f1 = np.std(set8_f1)\n",
    "print(\"The mean training time of the model H = 8: %.17f (std: %.17f) .\"\n",
    "      % (mean_time, std_time))\n",
    "print(\"The mean accuracy time of the model H = 8: %.17f (std: %.17f) .\"\n",
    "      % (mean8_acc, std_acc))\n",
    "print(\"The mean F1 score time of the model H = 8: %.17f (std: %.17f) .\"\n",
    "      % (mean8_f1, std_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 4.56597631\n",
      "Iteration 2, loss = 1.37426764\n",
      "Iteration 3, loss = 0.88773477\n",
      "Iteration 4, loss = 0.68562937\n",
      "Iteration 5, loss = 0.60373660\n",
      "Iteration 6, loss = 0.50973550\n",
      "Iteration 7, loss = 0.49240667\n",
      "Iteration 8, loss = 0.40585654\n",
      "Iteration 9, loss = 0.42354248\n",
      "Iteration 10, loss = 0.42007752\n",
      "Iteration 11, loss = 0.37208619\n",
      "Iteration 12, loss = 0.35739493\n",
      "Iteration 13, loss = 0.41206020\n",
      "Iteration 14, loss = 0.34280975\n",
      "Iteration 15, loss = 0.58984566\n",
      "Iteration 16, loss = 0.32920583\n",
      "Iteration 17, loss = 0.33004187\n",
      "Iteration 18, loss = 0.32152302\n",
      "Iteration 19, loss = 0.32027298\n",
      "Iteration 20, loss = 0.31070991\n",
      "Iteration 21, loss = 0.32500935\n",
      "Iteration 22, loss = 0.32911789\n",
      "Iteration 23, loss = 0.34025060\n",
      "Iteration 24, loss = 0.34434560\n",
      "Iteration 25, loss = 0.31997751\n",
      "Iteration 26, loss = 0.32156488\n",
      "Iteration 27, loss = 0.32235706\n",
      "Iteration 28, loss = 0.31376155\n",
      "Iteration 29, loss = 0.30924283\n",
      "Iteration 30, loss = 0.34268890\n",
      "Iteration 31, loss = 0.29730822\n",
      "Iteration 32, loss = 0.31463326\n",
      "Iteration 33, loss = 0.30189969\n",
      "Iteration 34, loss = 0.31674203\n",
      "Iteration 35, loss = 0.32176498\n",
      "Iteration 36, loss = 0.29551765\n",
      "Iteration 37, loss = 0.29071407\n",
      "Iteration 38, loss = 0.35693007\n",
      "Iteration 39, loss = 0.29951236\n",
      "Iteration 40, loss = 0.31441042\n",
      "Iteration 41, loss = 0.35658451\n",
      "Iteration 42, loss = 0.29184545\n",
      "Iteration 43, loss = 0.30530924\n",
      "Iteration 44, loss = 0.32522592\n",
      "Iteration 45, loss = 0.28873572\n",
      "Iteration 46, loss = 0.30583599\n",
      "Iteration 47, loss = 0.28870814\n",
      "Iteration 48, loss = 0.29074919\n",
      "Iteration 49, loss = 0.28685458\n",
      "Iteration 50, loss = 0.31602273\n",
      "Iteration 51, loss = 0.28562267\n",
      "Iteration 52, loss = 0.28748883\n",
      "Iteration 53, loss = 0.28487469\n",
      "Iteration 54, loss = 0.29486300\n",
      "Iteration 55, loss = 0.28329074\n",
      "Iteration 56, loss = 0.28593277\n",
      "Iteration 57, loss = 0.28585046\n",
      "Iteration 58, loss = 0.30725147\n",
      "Iteration 59, loss = 0.38818607\n",
      "Iteration 60, loss = 0.28033559\n",
      "Iteration 61, loss = 0.29905457\n",
      "Iteration 62, loss = 0.31063996\n",
      "Iteration 63, loss = 0.42463446\n",
      "Iteration 64, loss = 0.29637516\n",
      "Iteration 65, loss = 0.28968349\n",
      "Iteration 66, loss = 0.28366747\n",
      "Iteration 67, loss = 0.27859196\n",
      "Iteration 68, loss = 0.28807831\n",
      "Iteration 69, loss = 0.29374626\n",
      "Iteration 70, loss = 0.28855038\n",
      "Iteration 71, loss = 0.28193034\n",
      "Iteration 72, loss = 0.27948783\n",
      "Iteration 73, loss = 0.27543926\n",
      "Iteration 74, loss = 0.27556158\n",
      "Iteration 75, loss = 0.27291435\n",
      "Iteration 76, loss = 0.28159046\n",
      "Iteration 77, loss = 0.27852612\n",
      "Iteration 78, loss = 0.27174828\n",
      "Iteration 79, loss = 0.28549306\n",
      "Iteration 80, loss = 0.29659056\n",
      "Iteration 81, loss = 0.27217705\n",
      "Iteration 82, loss = 0.27325024\n",
      "Iteration 83, loss = 0.26984443\n",
      "Iteration 84, loss = 0.26784749\n",
      "Iteration 85, loss = 0.29758682\n",
      "Iteration 86, loss = 0.27239235\n",
      "Iteration 87, loss = 0.29766772\n",
      "Iteration 88, loss = 0.26930123\n",
      "Iteration 89, loss = 0.27221033\n",
      "Iteration 90, loss = 0.27609204\n",
      "Iteration 91, loss = 0.27417109\n",
      "Iteration 92, loss = 0.26517644\n",
      "Iteration 93, loss = 0.27404472\n",
      "Iteration 94, loss = 0.26784893\n",
      "Iteration 95, loss = 0.29744060\n",
      "Iteration 96, loss = 0.28712533\n",
      "Iteration 97, loss = 0.29208481\n",
      "Iteration 98, loss = 0.27748555\n",
      "Iteration 99, loss = 0.27676791\n",
      "Iteration 100, loss = 0.27592095\n",
      "Iteration 101, loss = 0.29770193\n",
      "Iteration 102, loss = 0.28047529\n",
      "Iteration 103, loss = 0.36441978\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "The training time of the model: 0.35 .\n",
      "accuracy: 0.63125\n",
      "f1: 0.3218390804597701\n"
     ]
    }
   ],
   "source": [
    "#model H = 16\n",
    "mlp16 = MLPClassifier(hidden_layer_sizes=(16, ), solver='sgd', \n",
    "                     max_iter=500, random_state=0, verbose=True)\n",
    "start = time()\n",
    "mlp16.fit(X_train, y_train)\n",
    "NN_time = time() - start\n",
    "y_hat = mlp16.predict(X_test)\n",
    "print(\"The training time of the model: %.2f .\"\n",
    "      % (NN_time))\n",
    "print('accuracy:', accuracy_score(y_test, y_hat))\n",
    "print('f1:', f1_score(y_test, y_hat))\n",
    "setting16_time1 = NN_time\n",
    "setting16_acc1 = accuracy_score(y_test, y_hat)\n",
    "setting16_f1 = f1_score(y_test, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.11884793\n",
      "Iteration 2, loss = 1.27694343\n",
      "Iteration 3, loss = 0.53484922\n",
      "Iteration 4, loss = 0.44276268\n",
      "Iteration 5, loss = 0.37155650\n",
      "Iteration 6, loss = 0.34637293\n",
      "Iteration 7, loss = 0.33541942\n",
      "Iteration 8, loss = 0.32691674\n",
      "Iteration 9, loss = 0.32493730\n",
      "Iteration 10, loss = 0.32119727\n",
      "Iteration 11, loss = 0.31308385\n",
      "Iteration 12, loss = 0.31054336\n",
      "Iteration 13, loss = 0.31402861\n",
      "Iteration 14, loss = 0.31158838\n",
      "Iteration 15, loss = 0.31066244\n",
      "Iteration 16, loss = 0.32302275\n",
      "Iteration 17, loss = 0.31585911\n",
      "Iteration 18, loss = 0.30626778\n",
      "Iteration 19, loss = 0.30675900\n",
      "Iteration 20, loss = 0.31706300\n",
      "Iteration 21, loss = 0.30696332\n",
      "Iteration 22, loss = 0.31026071\n",
      "Iteration 23, loss = 0.30441125\n",
      "Iteration 24, loss = 0.30098541\n",
      "Iteration 25, loss = 0.30224042\n",
      "Iteration 26, loss = 0.29981838\n",
      "Iteration 27, loss = 0.30046320\n",
      "Iteration 28, loss = 0.29882711\n",
      "Iteration 29, loss = 0.31069048\n",
      "Iteration 30, loss = 0.30747715\n",
      "Iteration 31, loss = 0.30078629\n",
      "Iteration 32, loss = 0.30645123\n",
      "Iteration 33, loss = 0.30426672\n",
      "Iteration 34, loss = 0.30236093\n",
      "Iteration 35, loss = 0.29708443\n",
      "Iteration 36, loss = 0.29526842\n",
      "Iteration 37, loss = 0.29254914\n",
      "Iteration 38, loss = 0.29310726\n",
      "Iteration 39, loss = 0.29404845\n",
      "Iteration 40, loss = 0.30578404\n",
      "Iteration 41, loss = 0.29568035\n",
      "Iteration 42, loss = 0.30365648\n",
      "Iteration 43, loss = 0.29908020\n",
      "Iteration 44, loss = 0.29322514\n",
      "Iteration 45, loss = 0.29702836\n",
      "Iteration 46, loss = 0.29289703\n",
      "Iteration 47, loss = 0.30823084\n",
      "Iteration 48, loss = 0.29112842\n",
      "Iteration 49, loss = 0.28815311\n",
      "Iteration 50, loss = 0.28794409\n",
      "Iteration 51, loss = 0.28723220\n",
      "Iteration 52, loss = 0.28833471\n",
      "Iteration 53, loss = 0.29192811\n",
      "Iteration 54, loss = 0.29418215\n",
      "Iteration 55, loss = 0.30095988\n",
      "Iteration 56, loss = 0.29552695\n",
      "Iteration 57, loss = 0.28594453\n",
      "Iteration 58, loss = 0.28867493\n",
      "Iteration 59, loss = 0.28947859\n",
      "Iteration 60, loss = 0.29703131\n",
      "Iteration 61, loss = 0.28439639\n",
      "Iteration 62, loss = 0.29964321\n",
      "Iteration 63, loss = 0.28984278\n",
      "Iteration 64, loss = 0.28430789\n",
      "Iteration 65, loss = 0.29508842\n",
      "Iteration 66, loss = 0.28613737\n",
      "Iteration 67, loss = 0.28288980\n",
      "Iteration 68, loss = 0.28907875\n",
      "Iteration 69, loss = 0.29469906\n",
      "Iteration 70, loss = 0.29469941\n",
      "Iteration 71, loss = 0.28094847\n",
      "Iteration 72, loss = 0.28137271\n",
      "Iteration 73, loss = 0.28183451\n",
      "Iteration 74, loss = 0.27888880\n",
      "Iteration 75, loss = 0.27903657\n",
      "Iteration 76, loss = 0.28220880\n",
      "Iteration 77, loss = 0.27878682\n",
      "Iteration 78, loss = 0.28170963\n",
      "Iteration 79, loss = 0.27777549\n",
      "Iteration 80, loss = 0.27972194\n",
      "Iteration 81, loss = 0.28164735\n",
      "Iteration 82, loss = 0.28126523\n",
      "Iteration 83, loss = 0.28056056\n",
      "Iteration 84, loss = 0.27635072\n",
      "Iteration 85, loss = 0.27822386\n",
      "Iteration 86, loss = 0.29536352\n",
      "Iteration 87, loss = 0.27967191\n",
      "Iteration 88, loss = 0.27809451\n",
      "Iteration 89, loss = 0.27451662\n",
      "Iteration 90, loss = 0.27467680\n",
      "Iteration 91, loss = 0.27404709\n",
      "Iteration 92, loss = 0.28915992\n",
      "Iteration 93, loss = 0.29456283\n",
      "Iteration 94, loss = 0.29139627\n",
      "Iteration 95, loss = 0.27429840\n",
      "Iteration 96, loss = 0.27251459\n",
      "Iteration 97, loss = 0.28656647\n",
      "Iteration 98, loss = 0.26970722\n",
      "Iteration 99, loss = 0.27390502\n",
      "Iteration 100, loss = 0.28369566\n",
      "Iteration 101, loss = 0.26996230\n",
      "Iteration 102, loss = 0.29606463\n",
      "Iteration 103, loss = 0.26651061\n",
      "Iteration 104, loss = 0.26726536\n",
      "Iteration 105, loss = 0.26742252\n",
      "Iteration 106, loss = 0.27152400\n",
      "Iteration 107, loss = 0.27757193\n",
      "Iteration 108, loss = 0.26533024\n",
      "Iteration 109, loss = 0.26626987\n",
      "Iteration 110, loss = 0.26682402\n",
      "Iteration 111, loss = 0.27568434\n",
      "Iteration 112, loss = 0.28381458\n",
      "Iteration 113, loss = 0.27313453\n",
      "Iteration 114, loss = 0.27111248\n",
      "Iteration 115, loss = 0.26699645\n",
      "Iteration 116, loss = 0.26939962\n",
      "Iteration 117, loss = 0.27570909\n",
      "Iteration 118, loss = 0.26459563\n",
      "Iteration 119, loss = 0.26188759\n",
      "Iteration 120, loss = 0.26045941\n",
      "Iteration 121, loss = 0.26231005\n",
      "Iteration 122, loss = 0.26673273\n",
      "Iteration 123, loss = 0.25910860\n",
      "Iteration 124, loss = 0.26418695\n",
      "Iteration 125, loss = 0.26173045\n",
      "Iteration 126, loss = 0.25875667\n",
      "Iteration 127, loss = 0.25801004\n",
      "Iteration 128, loss = 0.25829232\n",
      "Iteration 129, loss = 0.26036291\n",
      "Iteration 130, loss = 0.25621023\n",
      "Iteration 131, loss = 0.25848067\n",
      "Iteration 132, loss = 0.25926383\n",
      "Iteration 133, loss = 0.25780811\n",
      "Iteration 134, loss = 0.28573352\n",
      "Iteration 135, loss = 0.26875001\n",
      "Iteration 136, loss = 0.25850695\n",
      "Iteration 137, loss = 0.25509779\n",
      "Iteration 138, loss = 0.26709987\n",
      "Iteration 139, loss = 0.25832101\n",
      "Iteration 140, loss = 0.25761217\n",
      "Iteration 141, loss = 0.25318355\n",
      "Iteration 142, loss = 0.26058559\n",
      "Iteration 143, loss = 0.25354950\n",
      "Iteration 144, loss = 0.25442955\n",
      "Iteration 145, loss = 0.25883617\n",
      "Iteration 146, loss = 0.25395586\n",
      "Iteration 147, loss = 0.25955284\n",
      "Iteration 148, loss = 0.25778154\n",
      "Iteration 149, loss = 0.25796427\n",
      "Iteration 150, loss = 0.25883362\n",
      "Iteration 151, loss = 0.25498571\n",
      "Iteration 152, loss = 0.26363415\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "The training time of the model: 0.49 .\n",
      "accuracy: 0.8875\n",
      "f1: 0.4000000000000001\n"
     ]
    }
   ],
   "source": [
    "#repeat model H = 16 multiple times\n",
    "mlp16 = MLPClassifier(hidden_layer_sizes=(16, ), solver='sgd', \n",
    "                     max_iter=500, random_state=1, verbose=True)\n",
    "start = time()\n",
    "mlp16.fit(X_train, y_train)\n",
    "NN_time = time() - start\n",
    "y_hat = mlp16.predict(X_test)\n",
    "print(\"The training time of the model: %.2f .\"\n",
    "      % (NN_time))\n",
    "print('accuracy:', accuracy_score(y_test, y_hat))\n",
    "print('f1:', f1_score(y_test, y_hat))\n",
    "setting16_time2 = NN_time\n",
    "setting16_acc2 = accuracy_score(y_test, y_hat)\n",
    "setting16_f12 = f1_score(y_test, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 3.20044666\n",
      "Iteration 2, loss = 4.39528444\n",
      "Iteration 3, loss = 2.38711631\n",
      "Iteration 4, loss = 0.88840801\n",
      "Iteration 5, loss = 0.37437064\n",
      "Iteration 6, loss = 0.38311635\n",
      "Iteration 7, loss = 0.37619649\n",
      "Iteration 8, loss = 0.33927524\n",
      "Iteration 9, loss = 0.31380103\n",
      "Iteration 10, loss = 0.30918853\n",
      "Iteration 11, loss = 0.30845053\n",
      "Iteration 12, loss = 0.30653429\n",
      "Iteration 13, loss = 0.31963163\n",
      "Iteration 14, loss = 0.29532832\n",
      "Iteration 15, loss = 0.29694501\n",
      "Iteration 16, loss = 0.29532942\n",
      "Iteration 17, loss = 0.29006805\n",
      "Iteration 18, loss = 0.28545347\n",
      "Iteration 19, loss = 0.28432831\n",
      "Iteration 20, loss = 0.28276435\n",
      "Iteration 21, loss = 0.28884391\n",
      "Iteration 22, loss = 0.27962335\n",
      "Iteration 23, loss = 0.28918171\n",
      "Iteration 24, loss = 0.28330730\n",
      "Iteration 25, loss = 0.27915927\n",
      "Iteration 26, loss = 0.27873542\n",
      "Iteration 27, loss = 0.27873433\n",
      "Iteration 28, loss = 0.28212387\n",
      "Iteration 29, loss = 0.27508361\n",
      "Iteration 30, loss = 0.27970315\n",
      "Iteration 31, loss = 0.27728218\n",
      "Iteration 32, loss = 0.27554955\n",
      "Iteration 33, loss = 0.27372206\n",
      "Iteration 34, loss = 0.27346761\n",
      "Iteration 35, loss = 0.27299306\n",
      "Iteration 36, loss = 0.27331479\n",
      "Iteration 37, loss = 0.27635815\n",
      "Iteration 38, loss = 0.27360622\n",
      "Iteration 39, loss = 0.27108161\n",
      "Iteration 40, loss = 0.27558035\n",
      "Iteration 41, loss = 0.27373198\n",
      "Iteration 42, loss = 0.27263118\n",
      "Iteration 43, loss = 0.27403711\n",
      "Iteration 44, loss = 0.26978948\n",
      "Iteration 45, loss = 0.27074713\n",
      "Iteration 46, loss = 0.26848019\n",
      "Iteration 47, loss = 0.27215760\n",
      "Iteration 48, loss = 0.26987886\n",
      "Iteration 49, loss = 0.27190727\n",
      "Iteration 50, loss = 0.26811114\n",
      "Iteration 51, loss = 0.26722462\n",
      "Iteration 52, loss = 0.27509925\n",
      "Iteration 53, loss = 0.27533988\n",
      "Iteration 54, loss = 0.26699864\n",
      "Iteration 55, loss = 0.26662129\n",
      "Iteration 56, loss = 0.26489432\n",
      "Iteration 57, loss = 0.26544832\n",
      "Iteration 58, loss = 0.26901393\n",
      "Iteration 59, loss = 0.26522873\n",
      "Iteration 60, loss = 0.26734643\n",
      "Iteration 61, loss = 0.28315791\n",
      "Iteration 62, loss = 0.26406527\n",
      "Iteration 63, loss = 0.26701927\n",
      "Iteration 64, loss = 0.26970193\n",
      "Iteration 65, loss = 0.26913577\n",
      "Iteration 66, loss = 0.26304062\n",
      "Iteration 67, loss = 0.26432236\n",
      "Iteration 68, loss = 0.26548831\n",
      "Iteration 69, loss = 0.26755248\n",
      "Iteration 70, loss = 0.26727229\n",
      "Iteration 71, loss = 0.26967529\n",
      "Iteration 72, loss = 0.26932423\n",
      "Iteration 73, loss = 0.26528280\n",
      "Iteration 74, loss = 0.26236863\n",
      "Iteration 75, loss = 0.26540596\n",
      "Iteration 76, loss = 0.26318176\n",
      "Iteration 77, loss = 0.26519608\n",
      "Iteration 78, loss = 0.26527490\n",
      "Iteration 79, loss = 0.26449500\n",
      "Iteration 80, loss = 0.27575122\n",
      "Iteration 81, loss = 0.27355036\n",
      "Iteration 82, loss = 0.26889448\n",
      "Iteration 83, loss = 0.26595473\n",
      "Iteration 84, loss = 0.26664984\n",
      "Iteration 85, loss = 0.26636499\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "The training time of the model: 0.19 .\n",
      "accuracy: 0.8875\n",
      "f1: 0.1\n",
      "The mean training time of the model H = 16: 0.34109187126159668 (std: 0.12183933096614349) .\n",
      "The mean accuracy time of the model H = 16: 0.80208333333333337 (std: 0.12079740845270186) .\n",
      "The mean F1 score time of the model H = 16: 0.27394636015325674 (std: 0.12707028106201296) .\n"
     ]
    }
   ],
   "source": [
    "#repeat model H = 16 multiple times\n",
    "#report the mean and standard deviation of the training time, accuracy, and F1 score for the setting.\n",
    "mlp16 = MLPClassifier(hidden_layer_sizes=(16, ), solver='sgd', \n",
    "                     max_iter=500, random_state=2, verbose=True)\n",
    "start = time()\n",
    "mlp16.fit(X_train, y_train)\n",
    "NN_time = time() - start\n",
    "y_hat = mlp16.predict(X_test)\n",
    "print(\"The training time of the model: %.2f .\"\n",
    "      % (NN_time))\n",
    "print('accuracy:', accuracy_score(y_test, y_hat))\n",
    "print('f1:', f1_score(y_test, y_hat))\n",
    "setting16_time3 = NN_time\n",
    "setting16_acc3 = accuracy_score(y_test, y_hat)\n",
    "setting16_f13 = f1_score(y_test, y_hat)\n",
    "set16_time = np.array([setting16_time1, setting16_time2, setting16_time3])\n",
    "mean_time = np.mean(set16_time)\n",
    "std_time = np.std(set16_time)\n",
    "set16_acc = np.array([setting16_acc1, setting16_acc2, setting16_acc3])\n",
    "mean16_acc = np.mean(set16_acc)\n",
    "std_acc = np.std(set16_acc)\n",
    "set16_f1 = np.array([setting16_f1, setting16_f12, setting16_f13])\n",
    "mean16_f1 = np.mean(set16_f1)\n",
    "std_f1 = np.std(set16_f1)\n",
    "print(\"The mean training time of the model H = 16: %.17f (std: %.17f) .\"\n",
    "      % (mean_time, std_time))\n",
    "print(\"The mean accuracy time of the model H = 16: %.17f (std: %.17f) .\"\n",
    "      % (mean16_acc, std_acc))\n",
    "print(\"The mean F1 score time of the model H = 16: %.17f (std: %.17f) .\"\n",
    "      % (mean16_f1, std_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.79728957\n",
      "Iteration 2, loss = 5.44176823\n",
      "Iteration 3, loss = 3.52183514\n",
      "Iteration 4, loss = 1.30236482\n",
      "Iteration 5, loss = 0.81981907\n",
      "Iteration 6, loss = 0.46690190\n",
      "Iteration 7, loss = 0.37622435\n",
      "Iteration 8, loss = 0.36169471\n",
      "Iteration 9, loss = 0.32892116\n",
      "Iteration 10, loss = 0.32339331\n",
      "Iteration 11, loss = 0.32383255\n",
      "Iteration 12, loss = 0.30833217\n",
      "Iteration 13, loss = 0.30396915\n",
      "Iteration 14, loss = 0.30092192\n",
      "Iteration 15, loss = 0.30408483\n",
      "Iteration 16, loss = 0.30258237\n",
      "Iteration 17, loss = 0.29485532\n",
      "Iteration 18, loss = 0.29872840\n",
      "Iteration 19, loss = 0.29294922\n",
      "Iteration 20, loss = 0.29110952\n",
      "Iteration 21, loss = 0.29219483\n",
      "Iteration 22, loss = 0.28822497\n",
      "Iteration 23, loss = 0.28210707\n",
      "Iteration 24, loss = 0.28242531\n",
      "Iteration 25, loss = 0.28722410\n",
      "Iteration 26, loss = 0.27943087\n",
      "Iteration 27, loss = 0.28146171\n",
      "Iteration 28, loss = 0.27903288\n",
      "Iteration 29, loss = 0.28634421\n",
      "Iteration 30, loss = 0.27799334\n",
      "Iteration 31, loss = 0.27407585\n",
      "Iteration 32, loss = 0.27548410\n",
      "Iteration 33, loss = 0.28420854\n",
      "Iteration 34, loss = 0.27323134\n",
      "Iteration 35, loss = 0.27898853\n",
      "Iteration 36, loss = 0.28302368\n",
      "Iteration 37, loss = 0.27100240\n",
      "Iteration 38, loss = 0.28243359\n",
      "Iteration 39, loss = 0.27666307\n",
      "Iteration 40, loss = 0.27747480\n",
      "Iteration 41, loss = 0.26992865\n",
      "Iteration 42, loss = 0.26460944\n",
      "Iteration 43, loss = 0.26845733\n",
      "Iteration 44, loss = 0.26570183\n",
      "Iteration 45, loss = 0.26535404\n",
      "Iteration 46, loss = 0.26823640\n",
      "Iteration 47, loss = 0.27014619\n",
      "Iteration 48, loss = 0.26303248\n",
      "Iteration 49, loss = 0.26544297\n",
      "Iteration 50, loss = 0.26769235\n",
      "Iteration 51, loss = 0.26363538\n",
      "Iteration 52, loss = 0.26531965\n",
      "Iteration 53, loss = 0.25906190\n",
      "Iteration 54, loss = 0.26217983\n",
      "Iteration 55, loss = 0.25890523\n",
      "Iteration 56, loss = 0.26893431\n",
      "Iteration 57, loss = 0.26926023\n",
      "Iteration 58, loss = 0.26945522\n",
      "Iteration 59, loss = 0.25949918\n",
      "Iteration 60, loss = 0.26075892\n",
      "Iteration 61, loss = 0.26139694\n",
      "Iteration 62, loss = 0.26145741\n",
      "Iteration 63, loss = 0.25601302\n",
      "Iteration 64, loss = 0.25493690\n",
      "Iteration 65, loss = 0.25521842\n",
      "Iteration 66, loss = 0.25962828\n",
      "Iteration 67, loss = 0.25528104\n",
      "Iteration 68, loss = 0.25454452\n",
      "Iteration 69, loss = 0.25869766\n",
      "Iteration 70, loss = 0.25586098\n",
      "Iteration 71, loss = 0.25257370\n",
      "Iteration 72, loss = 0.25363348\n",
      "Iteration 73, loss = 0.25652222\n",
      "Iteration 74, loss = 0.25626188\n",
      "Iteration 75, loss = 0.25146095\n",
      "Iteration 76, loss = 0.25572098\n",
      "Iteration 77, loss = 0.25423515\n",
      "Iteration 78, loss = 0.25227641\n",
      "Iteration 79, loss = 0.25408584\n",
      "Iteration 80, loss = 0.24900216\n",
      "Iteration 81, loss = 0.25131090\n",
      "Iteration 82, loss = 0.25090806\n",
      "Iteration 83, loss = 0.24778328\n",
      "Iteration 84, loss = 0.25506634\n",
      "Iteration 85, loss = 0.25389537\n",
      "Iteration 86, loss = 0.26709577\n",
      "Iteration 87, loss = 0.24668335\n",
      "Iteration 88, loss = 0.24568289\n",
      "Iteration 89, loss = 0.24616066\n",
      "Iteration 90, loss = 0.24481440\n",
      "Iteration 91, loss = 0.24904127\n",
      "Iteration 92, loss = 0.25051356\n",
      "Iteration 93, loss = 0.25283890\n",
      "Iteration 94, loss = 0.24574819\n",
      "Iteration 95, loss = 0.24476989\n",
      "Iteration 96, loss = 0.24404240\n",
      "Iteration 97, loss = 0.24973522\n",
      "Iteration 98, loss = 0.24144191\n",
      "Iteration 99, loss = 0.24827073\n",
      "Iteration 100, loss = 0.25276314\n",
      "Iteration 101, loss = 0.24355756\n",
      "Iteration 102, loss = 0.24240365\n",
      "Iteration 103, loss = 0.24646405\n",
      "Iteration 104, loss = 0.24374864\n",
      "Iteration 105, loss = 0.25049153\n",
      "Iteration 106, loss = 0.25290250\n",
      "Iteration 107, loss = 0.24409756\n",
      "Iteration 108, loss = 0.24529133\n",
      "Iteration 109, loss = 0.24502275\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "The training time of the model: 0.34 .\n",
      "accuracy: 0.8875\n",
      "f1: 0.1\n"
     ]
    }
   ],
   "source": [
    "#model H = 32\n",
    "mlp32 = MLPClassifier(hidden_layer_sizes=(32, ), solver='sgd', \n",
    "                     max_iter=500, random_state=0, verbose=True)\n",
    "start = time()\n",
    "mlp32.fit(X_train, y_train)\n",
    "NN_time = time() - start\n",
    "y_hat = mlp32.predict(X_test)\n",
    "print(\"The training time of the model: %.2f .\"\n",
    "      % (NN_time))\n",
    "print('accuracy:', accuracy_score(y_test, y_hat))\n",
    "print('f1:', f1_score(y_test, y_hat))\n",
    "setting32_time1 = NN_time\n",
    "setting32_acc1 = accuracy_score(y_test, y_hat)\n",
    "setting32_f1 = f1_score(y_test, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.64725234\n",
      "Iteration 2, loss = 0.71707505\n",
      "Iteration 3, loss = 4.94415777\n",
      "Iteration 4, loss = 2.85144335\n",
      "Iteration 5, loss = 0.68201420\n",
      "Iteration 6, loss = 0.50568421\n",
      "Iteration 7, loss = 0.46810483\n",
      "Iteration 8, loss = 0.38435373\n",
      "Iteration 9, loss = 0.35384403\n",
      "Iteration 10, loss = 0.35885546\n",
      "Iteration 11, loss = 0.33043200\n",
      "Iteration 12, loss = 0.34394556\n",
      "Iteration 13, loss = 0.30521511\n",
      "Iteration 14, loss = 0.34126971\n",
      "Iteration 15, loss = 0.31611057\n",
      "Iteration 16, loss = 0.29959574\n",
      "Iteration 17, loss = 0.31355754\n",
      "Iteration 18, loss = 0.30704738\n",
      "Iteration 19, loss = 0.30914135\n",
      "Iteration 20, loss = 0.30308355\n",
      "Iteration 21, loss = 0.28883654\n",
      "Iteration 22, loss = 0.30638012\n",
      "Iteration 23, loss = 0.28304712\n",
      "Iteration 24, loss = 0.28896146\n",
      "Iteration 25, loss = 0.28667700\n",
      "Iteration 26, loss = 0.28413337\n",
      "Iteration 27, loss = 0.28928698\n",
      "Iteration 28, loss = 0.27065920\n",
      "Iteration 29, loss = 0.30676029\n",
      "Iteration 30, loss = 0.28110023\n",
      "Iteration 31, loss = 0.27074302\n",
      "Iteration 32, loss = 0.27691835\n",
      "Iteration 33, loss = 0.27920340\n",
      "Iteration 34, loss = 0.27107356\n",
      "Iteration 35, loss = 0.27141540\n",
      "Iteration 36, loss = 0.27059148\n",
      "Iteration 37, loss = 0.27117038\n",
      "Iteration 38, loss = 0.29145107\n",
      "Iteration 39, loss = 0.26443067\n",
      "Iteration 40, loss = 0.28876139\n",
      "Iteration 41, loss = 0.28812863\n",
      "Iteration 42, loss = 0.26223659\n",
      "Iteration 43, loss = 0.31123856\n",
      "Iteration 44, loss = 0.26207979\n",
      "Iteration 45, loss = 0.26137145\n",
      "Iteration 46, loss = 0.29897057\n",
      "Iteration 47, loss = 0.26937663\n",
      "Iteration 48, loss = 0.29458733\n",
      "Iteration 49, loss = 0.26702009\n",
      "Iteration 50, loss = 0.27380469\n",
      "Iteration 51, loss = 0.26280715\n",
      "Iteration 52, loss = 0.26396401\n",
      "Iteration 53, loss = 0.25848156\n",
      "Iteration 54, loss = 0.27797762\n",
      "Iteration 55, loss = 0.25955561\n",
      "Iteration 56, loss = 0.25794405\n",
      "Iteration 57, loss = 0.26764945\n",
      "Iteration 58, loss = 0.26596393\n",
      "Iteration 59, loss = 0.27752111\n",
      "Iteration 60, loss = 0.27922213\n",
      "Iteration 61, loss = 0.26560635\n",
      "Iteration 62, loss = 0.26583997\n",
      "Iteration 63, loss = 0.26420665\n",
      "Iteration 64, loss = 0.25064698\n",
      "Iteration 65, loss = 0.25006944\n",
      "Iteration 66, loss = 0.25693737\n",
      "Iteration 67, loss = 0.25584056\n",
      "Iteration 68, loss = 0.25159896\n",
      "Iteration 69, loss = 0.25049785\n",
      "Iteration 70, loss = 0.24954197\n",
      "Iteration 71, loss = 0.25367650\n",
      "Iteration 72, loss = 0.24424833\n",
      "Iteration 73, loss = 0.25123008\n",
      "Iteration 74, loss = 0.25042481\n",
      "Iteration 75, loss = 0.24907159\n",
      "Iteration 76, loss = 0.24756371\n",
      "Iteration 77, loss = 0.24043554\n",
      "Iteration 78, loss = 0.25219442\n",
      "Iteration 79, loss = 0.24007141\n",
      "Iteration 80, loss = 0.24591574\n",
      "Iteration 81, loss = 0.24728655\n",
      "Iteration 82, loss = 0.28041104\n",
      "Iteration 83, loss = 0.24514909\n",
      "Iteration 84, loss = 0.24800036\n",
      "Iteration 85, loss = 0.26197851\n",
      "Iteration 86, loss = 0.25755552\n",
      "Iteration 87, loss = 0.24338687\n",
      "Iteration 88, loss = 0.24012675\n",
      "Iteration 89, loss = 0.26438784\n",
      "Iteration 90, loss = 0.23711321\n",
      "Iteration 91, loss = 0.25039818\n",
      "Iteration 92, loss = 0.24493857\n",
      "Iteration 93, loss = 0.23810585\n",
      "Iteration 94, loss = 0.25642424\n",
      "Iteration 95, loss = 0.23477407\n",
      "Iteration 96, loss = 0.23566789\n",
      "Iteration 97, loss = 0.23152856\n",
      "Iteration 98, loss = 0.25469443\n",
      "Iteration 99, loss = 0.23329931\n",
      "Iteration 100, loss = 0.24338342\n",
      "Iteration 101, loss = 0.23134821\n",
      "Iteration 102, loss = 0.22945012\n",
      "Iteration 103, loss = 0.22828759\n",
      "Iteration 104, loss = 0.23973897\n",
      "Iteration 105, loss = 0.23220994\n",
      "Iteration 106, loss = 0.23229923\n",
      "Iteration 107, loss = 0.23912538\n",
      "Iteration 108, loss = 0.27304428\n",
      "Iteration 109, loss = 0.23559652\n",
      "Iteration 110, loss = 0.23086016\n",
      "Iteration 111, loss = 0.22713981\n",
      "Iteration 112, loss = 0.22593834\n",
      "Iteration 113, loss = 0.22784491\n",
      "Iteration 114, loss = 0.25598062\n",
      "Iteration 115, loss = 0.23690533\n",
      "Iteration 116, loss = 0.23918510\n",
      "Iteration 117, loss = 0.22782023\n",
      "Iteration 118, loss = 0.22225665\n",
      "Iteration 119, loss = 0.23285466\n",
      "Iteration 120, loss = 0.22006258\n",
      "Iteration 121, loss = 0.34786262\n",
      "Iteration 122, loss = 0.22418564\n",
      "Iteration 123, loss = 0.22567500\n",
      "Iteration 124, loss = 0.22093791\n",
      "Iteration 125, loss = 0.22006421\n",
      "Iteration 126, loss = 0.22236686\n",
      "Iteration 127, loss = 0.22187735\n",
      "Iteration 128, loss = 0.25889979\n",
      "Iteration 129, loss = 0.24907525\n",
      "Iteration 130, loss = 0.22178340\n",
      "Iteration 131, loss = 0.22280552\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "The training time of the model: 0.45 .\n",
      "accuracy: 0.9\n",
      "f1: 0.2\n"
     ]
    }
   ],
   "source": [
    "#repeat model H = 32 multiple times\n",
    "mlp32 = MLPClassifier(hidden_layer_sizes=(32, ), solver='sgd', \n",
    "                     max_iter=500, random_state=1, verbose=True)\n",
    "start = time()\n",
    "mlp32.fit(X_train, y_train)\n",
    "NN_time = time() - start\n",
    "y_hat = mlp32.predict(X_test)\n",
    "print(\"The training time of the model: %.2f .\" \n",
    "      % (NN_time))\n",
    "print('accuracy:', accuracy_score(y_test, y_hat))\n",
    "print('f1:', f1_score(y_test, y_hat))\n",
    "setting32_time2 = NN_time\n",
    "setting32_acc2 = accuracy_score(y_test, y_hat)\n",
    "setting32_f12 = f1_score(y_test, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.23246860\n",
      "Iteration 2, loss = 1.42361584\n",
      "Iteration 3, loss = 0.89053081\n",
      "Iteration 4, loss = 1.55015569\n",
      "Iteration 5, loss = 0.58815990\n",
      "Iteration 6, loss = 0.46793642\n",
      "Iteration 7, loss = 0.36171189\n",
      "Iteration 8, loss = 0.44346745\n",
      "Iteration 9, loss = 0.32034177\n",
      "Iteration 10, loss = 0.33224086\n",
      "Iteration 11, loss = 0.32050168\n",
      "Iteration 12, loss = 0.33878729\n",
      "Iteration 13, loss = 0.35919741\n",
      "Iteration 14, loss = 0.30476669\n",
      "Iteration 15, loss = 0.29306443\n",
      "Iteration 16, loss = 0.28638133\n",
      "Iteration 17, loss = 0.30040127\n",
      "Iteration 18, loss = 0.31775297\n",
      "Iteration 19, loss = 0.36014415\n",
      "Iteration 20, loss = 0.26489736\n",
      "Iteration 21, loss = 0.27352212\n",
      "Iteration 22, loss = 0.26240044\n",
      "Iteration 23, loss = 0.30304271\n",
      "Iteration 24, loss = 0.26231786\n",
      "Iteration 25, loss = 0.29115993\n",
      "Iteration 26, loss = 0.31300191\n",
      "Iteration 27, loss = 0.25335039\n",
      "Iteration 28, loss = 0.25629672\n",
      "Iteration 29, loss = 0.25410193\n",
      "Iteration 30, loss = 0.28046609\n",
      "Iteration 31, loss = 0.24732249\n",
      "Iteration 32, loss = 0.24455485\n",
      "Iteration 33, loss = 0.27206677\n",
      "Iteration 34, loss = 0.24761339\n",
      "Iteration 35, loss = 0.24822328\n",
      "Iteration 36, loss = 0.24687172\n",
      "Iteration 37, loss = 0.24965366\n",
      "Iteration 38, loss = 0.25471412\n",
      "Iteration 39, loss = 0.28886548\n",
      "Iteration 40, loss = 0.27504505\n",
      "Iteration 41, loss = 0.23664260\n",
      "Iteration 42, loss = 0.25716286\n",
      "Iteration 43, loss = 0.24177782\n",
      "Iteration 44, loss = 0.35685581\n",
      "Iteration 45, loss = 0.23454418\n",
      "Iteration 46, loss = 0.23289753\n",
      "Iteration 47, loss = 0.26891889\n",
      "Iteration 48, loss = 0.23456158\n",
      "Iteration 49, loss = 0.25288168\n",
      "Iteration 50, loss = 0.25956837\n",
      "Iteration 51, loss = 0.24192207\n",
      "Iteration 52, loss = 0.50914544\n",
      "Iteration 53, loss = 0.24250556\n",
      "Iteration 54, loss = 0.25068640\n",
      "Iteration 55, loss = 0.24088176\n",
      "Iteration 56, loss = 0.24510113\n",
      "Iteration 57, loss = 0.23092404\n",
      "Iteration 58, loss = 0.24545764\n",
      "Iteration 59, loss = 0.24056868\n",
      "Iteration 60, loss = 0.23678771\n",
      "Iteration 61, loss = 0.23935872\n",
      "Iteration 62, loss = 0.23139760\n",
      "Iteration 63, loss = 0.22881731\n",
      "Iteration 64, loss = 0.23243636\n",
      "Iteration 65, loss = 0.22609181\n",
      "Iteration 66, loss = 0.23431033\n",
      "Iteration 67, loss = 0.23909376\n",
      "Iteration 68, loss = 0.23166157\n",
      "Iteration 69, loss = 0.23873160\n",
      "Iteration 70, loss = 0.22399652\n",
      "Iteration 71, loss = 0.25588205\n",
      "Iteration 72, loss = 0.22392692\n",
      "Iteration 73, loss = 0.22991162\n",
      "Iteration 74, loss = 0.25500110\n",
      "Iteration 75, loss = 0.24851325\n",
      "Iteration 76, loss = 0.22091237\n",
      "Iteration 77, loss = 0.22655120\n",
      "Iteration 78, loss = 0.23149831\n",
      "Iteration 79, loss = 0.22489112\n",
      "Iteration 80, loss = 0.23320083\n",
      "Iteration 81, loss = 0.22716676\n",
      "Iteration 82, loss = 0.21926926\n",
      "Iteration 83, loss = 0.22608948\n",
      "Iteration 84, loss = 0.22416539\n",
      "Iteration 85, loss = 0.22204137\n",
      "Iteration 86, loss = 0.21869230\n",
      "Iteration 87, loss = 0.21878358\n",
      "Iteration 88, loss = 0.21756805\n",
      "Iteration 89, loss = 0.21753691\n",
      "Iteration 90, loss = 0.24833251\n",
      "Iteration 91, loss = 0.21605327\n",
      "Iteration 92, loss = 0.21733610\n",
      "Iteration 93, loss = 0.22680466\n",
      "Iteration 94, loss = 0.21793934\n",
      "Iteration 95, loss = 0.21943330\n",
      "Iteration 96, loss = 0.21477504\n",
      "Iteration 97, loss = 0.21618234\n",
      "Iteration 98, loss = 0.21733234\n",
      "Iteration 99, loss = 0.23509115\n",
      "Iteration 100, loss = 0.21672385\n",
      "Iteration 101, loss = 0.26696990\n",
      "Iteration 102, loss = 0.21802556\n",
      "Iteration 103, loss = 0.22190412\n",
      "Iteration 104, loss = 0.22735147\n",
      "Iteration 105, loss = 0.22965031\n",
      "Iteration 106, loss = 0.21806007\n",
      "Iteration 107, loss = 0.22160542\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "The training time of the model: 0.33 .\n",
      "accuracy: 0.90625\n",
      "f1: 0.21052631578947367\n",
      "The mean training time of the model H = 32: 0.37566288312276203 (std: 0.05463445344447040) .\n",
      "The mean accuracy time of the model H = 32: 0.89791666666666670 (std: 0.00779511955577907) .\n",
      "The mean F1 score time of the model H = 32: 0.17017543859649123 (std: 0.04980726163711716) .\n"
     ]
    }
   ],
   "source": [
    "#repeat model H = 32 multiple times\n",
    "#report the mean and standard deviation of the training time, accuracy, and F1 score for the setting.\n",
    "mlp32 = MLPClassifier(hidden_layer_sizes=(32, ), solver='sgd', \n",
    "                     max_iter=500, random_state=2, verbose=True)\n",
    "start = time()\n",
    "mlp32.fit(X_train, y_train)\n",
    "NN_time = time() - start\n",
    "y_hat = mlp32.predict(X_test)\n",
    "print(\"The training time of the model: %.2f .\"\n",
    "      % (NN_time))\n",
    "print('accuracy:', accuracy_score(y_test, y_hat))\n",
    "print('f1:', f1_score(y_test, y_hat))\n",
    "setting32_time3 = NN_time\n",
    "setting32_acc3 = accuracy_score(y_test, y_hat)\n",
    "setting32_f13 = f1_score(y_test, y_hat)\n",
    "set32_time = np.array([setting32_time1, setting32_time2, setting32_time3])\n",
    "mean_time = np.mean(set32_time)\n",
    "std_time = np.std(set32_time)\n",
    "set32_acc = np.array([setting32_acc1, setting32_acc2, setting32_acc3])\n",
    "mean32_acc = np.mean(set32_acc)\n",
    "std_acc = np.std(set32_acc)\n",
    "set32_f1 = np.array([setting32_f1, setting32_f12, setting32_f13])\n",
    "mean32_f1 = np.mean(set32_f1)\n",
    "std_f1 = np.std(set32_f1)\n",
    "print(\"The mean training time of the model H = 32: %.17f (std: %.17f) .\"\n",
    "      % (mean_time, std_time))\n",
    "print(\"The mean accuracy time of the model H = 32: %.17f (std: %.17f) .\"\n",
    "      % (mean32_acc, std_acc))\n",
    "print(\"The mean F1 score time of the model H = 32: %.17f (std: %.17f) .\"\n",
    "      % (mean32_f1, std_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = inf\n",
      "Iteration 2, loss = 7.53744670\n",
      "Iteration 3, loss = 4.82316029\n",
      "Iteration 4, loss = 1.23634182\n",
      "Iteration 5, loss = 0.50993920\n",
      "Iteration 6, loss = 0.32912933\n",
      "Iteration 7, loss = 0.31343145\n",
      "Iteration 8, loss = 0.30834560\n",
      "Iteration 9, loss = 0.30092573\n",
      "Iteration 10, loss = 0.29508203\n",
      "Iteration 11, loss = 0.29171004\n",
      "Iteration 12, loss = 0.28829141\n",
      "Iteration 13, loss = 0.28433477\n",
      "Iteration 14, loss = 0.28110853\n",
      "Iteration 15, loss = 0.28180017\n",
      "Iteration 16, loss = 0.27868285\n",
      "Iteration 17, loss = 0.27756483\n",
      "Iteration 18, loss = 0.27646167\n",
      "Iteration 19, loss = 0.27190681\n",
      "Iteration 20, loss = 0.27276582\n",
      "Iteration 21, loss = 0.26825249\n",
      "Iteration 22, loss = 0.27491414\n",
      "Iteration 23, loss = 0.27405948\n",
      "Iteration 24, loss = 0.27028032\n",
      "Iteration 25, loss = 0.26752888\n",
      "Iteration 26, loss = 0.26873564\n",
      "Iteration 27, loss = 0.27187552\n",
      "Iteration 28, loss = 0.26982334\n",
      "Iteration 29, loss = 0.26189668\n",
      "Iteration 30, loss = 0.26188764\n",
      "Iteration 31, loss = 0.25841240\n",
      "Iteration 32, loss = 0.26067138\n",
      "Iteration 33, loss = 0.26110891\n",
      "Iteration 34, loss = 0.25759304\n",
      "Iteration 35, loss = 0.26200970\n",
      "Iteration 36, loss = 0.25736973\n",
      "Iteration 37, loss = 0.25973132\n",
      "Iteration 38, loss = 0.25742358\n",
      "Iteration 39, loss = 0.25725185\n",
      "Iteration 40, loss = 0.25604222\n",
      "Iteration 41, loss = 0.25395112\n",
      "Iteration 42, loss = 0.25367647\n",
      "Iteration 43, loss = 0.25465448\n",
      "Iteration 44, loss = 0.25462516\n",
      "Iteration 45, loss = 0.25179742\n",
      "Iteration 46, loss = 0.25023031\n",
      "Iteration 47, loss = 0.25090966\n",
      "Iteration 48, loss = 0.25083349\n",
      "Iteration 49, loss = 0.24852672\n",
      "Iteration 50, loss = 0.25360321\n",
      "Iteration 51, loss = 0.25043605\n",
      "Iteration 52, loss = 0.25720839\n",
      "Iteration 53, loss = 0.25051343\n",
      "Iteration 54, loss = 0.25405335\n",
      "Iteration 55, loss = 0.24828045\n",
      "Iteration 56, loss = 0.24767238\n",
      "Iteration 57, loss = 0.24861258\n",
      "Iteration 58, loss = 0.24504504\n",
      "Iteration 59, loss = 0.24494309\n",
      "Iteration 60, loss = 0.24411680\n",
      "Iteration 61, loss = 0.24381831\n",
      "Iteration 62, loss = 0.24801233\n",
      "Iteration 63, loss = 0.24242005\n",
      "Iteration 64, loss = 0.24713682\n",
      "Iteration 65, loss = 0.24452486\n",
      "Iteration 66, loss = 0.24399595\n",
      "Iteration 67, loss = 0.24246998\n",
      "Iteration 68, loss = 0.24344706\n",
      "Iteration 69, loss = 0.24248109\n",
      "Iteration 70, loss = 0.24105915\n",
      "Iteration 71, loss = 0.24401745\n",
      "Iteration 72, loss = 0.24563007\n",
      "Iteration 73, loss = 0.24937667\n",
      "Iteration 74, loss = 0.24562314\n",
      "Iteration 75, loss = 0.24447531\n",
      "Iteration 76, loss = 0.24127483\n",
      "Iteration 77, loss = 0.24094281\n",
      "Iteration 78, loss = 0.24562306\n",
      "Iteration 79, loss = 0.24294802\n",
      "Iteration 80, loss = 0.23814098\n",
      "Iteration 81, loss = 0.23774690\n",
      "Iteration 82, loss = 0.23776839\n",
      "Iteration 83, loss = 0.23940166\n",
      "Iteration 84, loss = 0.23684249\n",
      "Iteration 85, loss = 0.24040450\n",
      "Iteration 86, loss = 0.23498555\n",
      "Iteration 87, loss = 0.23594859\n",
      "Iteration 88, loss = 0.23593080\n",
      "Iteration 89, loss = 0.23511445\n",
      "Iteration 90, loss = 0.23602606\n",
      "Iteration 91, loss = 0.23754056\n",
      "Iteration 92, loss = 0.23499788\n",
      "Iteration 93, loss = 0.23337956\n",
      "Iteration 94, loss = 0.23562794\n",
      "Iteration 95, loss = 0.23754894\n",
      "Iteration 96, loss = 0.23828867\n",
      "Iteration 97, loss = 0.23165677\n",
      "Iteration 98, loss = 0.23364285\n",
      "Iteration 99, loss = 0.23898932\n",
      "Iteration 100, loss = 0.23325743\n",
      "Iteration 101, loss = 0.23242467\n",
      "Iteration 102, loss = 0.23808574\n",
      "Iteration 103, loss = 0.23485681\n",
      "Iteration 104, loss = 0.23287852\n",
      "Iteration 105, loss = 0.23401159\n",
      "Iteration 106, loss = 0.23258733\n",
      "Iteration 107, loss = 0.23392841\n",
      "Iteration 108, loss = 0.23009922\n",
      "Iteration 109, loss = 0.23002026\n",
      "Iteration 110, loss = 0.23140475\n",
      "Iteration 111, loss = 0.23314247\n",
      "Iteration 112, loss = 0.22865583\n",
      "Iteration 113, loss = 0.22995938\n",
      "Iteration 114, loss = 0.22715998\n",
      "Iteration 115, loss = 0.23602756\n",
      "Iteration 116, loss = 0.23288380\n",
      "Iteration 117, loss = 0.22606618\n",
      "Iteration 118, loss = 0.22533507\n",
      "Iteration 119, loss = 0.22659964\n",
      "Iteration 120, loss = 0.23127874\n",
      "Iteration 121, loss = 0.22497205\n",
      "Iteration 122, loss = 0.22609490\n",
      "Iteration 123, loss = 0.23455656\n",
      "Iteration 124, loss = 0.22672340\n",
      "Iteration 125, loss = 0.22655364\n",
      "Iteration 126, loss = 0.22650554\n",
      "Iteration 127, loss = 0.22579608\n",
      "Iteration 128, loss = 0.22586535\n",
      "Iteration 129, loss = 0.22381453\n",
      "Iteration 130, loss = 0.22919025\n",
      "Iteration 131, loss = 0.22396809\n",
      "Iteration 132, loss = 0.23024828\n",
      "Iteration 133, loss = 0.22381518\n",
      "Iteration 134, loss = 0.22429466\n",
      "Iteration 135, loss = 0.22354977\n",
      "Iteration 136, loss = 0.23180066\n",
      "Iteration 137, loss = 0.22301297\n",
      "Iteration 138, loss = 0.22475463\n",
      "Iteration 139, loss = 0.22229934\n",
      "Iteration 140, loss = 0.22196172\n",
      "Iteration 141, loss = 0.22144367\n",
      "Iteration 142, loss = 0.22003945\n",
      "Iteration 143, loss = 0.22035408\n",
      "Iteration 144, loss = 0.22097290\n",
      "Iteration 145, loss = 0.22003132\n",
      "Iteration 146, loss = 0.21887094\n",
      "Iteration 147, loss = 0.21900344\n",
      "Iteration 148, loss = 0.21739102\n",
      "Iteration 149, loss = 0.22576771\n",
      "Iteration 150, loss = 0.21878259\n",
      "Iteration 151, loss = 0.21780042\n",
      "Iteration 152, loss = 0.21666681\n",
      "Iteration 153, loss = 0.21979567\n",
      "Iteration 154, loss = 0.21619553\n",
      "Iteration 155, loss = 0.21878026\n",
      "Iteration 156, loss = 0.21593255\n",
      "Iteration 157, loss = 0.21466644\n",
      "Iteration 158, loss = 0.21466203\n",
      "Iteration 159, loss = 0.21610777\n",
      "Iteration 160, loss = 0.21479896\n",
      "Iteration 161, loss = 0.22094296\n",
      "Iteration 162, loss = 0.23257955\n",
      "Iteration 163, loss = 0.21892908\n",
      "Iteration 164, loss = 0.21323968\n",
      "Iteration 165, loss = 0.21791005\n",
      "Iteration 166, loss = 0.21316067\n",
      "Iteration 167, loss = 0.21239811\n",
      "Iteration 168, loss = 0.21490161\n",
      "Iteration 169, loss = 0.21541415\n",
      "Iteration 170, loss = 0.21206033\n",
      "Iteration 171, loss = 0.21177194\n",
      "Iteration 172, loss = 0.24036304\n",
      "Iteration 173, loss = 0.21239206\n",
      "Iteration 174, loss = 0.21236041\n",
      "Iteration 175, loss = 0.21310252\n",
      "Iteration 176, loss = 0.21072999\n",
      "Iteration 177, loss = 0.21285589\n",
      "Iteration 178, loss = 0.21068939\n",
      "Iteration 179, loss = 0.21158996\n",
      "Iteration 180, loss = 0.21038020\n",
      "Iteration 181, loss = 0.21871312\n",
      "Iteration 182, loss = 0.21093288\n",
      "Iteration 183, loss = 0.21134272\n",
      "Iteration 184, loss = 0.20974466\n",
      "Iteration 185, loss = 0.20759987\n",
      "Iteration 186, loss = 0.21108972\n",
      "Iteration 187, loss = 0.20810443\n",
      "Iteration 188, loss = 0.21929705\n",
      "Iteration 189, loss = 0.20825889\n",
      "Iteration 190, loss = 0.22041519\n",
      "Iteration 191, loss = 0.20757825\n",
      "Iteration 192, loss = 0.20871796\n",
      "Iteration 193, loss = 0.20577209\n",
      "Iteration 194, loss = 0.21368972\n",
      "Iteration 195, loss = 0.21861128\n",
      "Iteration 196, loss = 0.21454170\n",
      "Iteration 197, loss = 0.20633535\n",
      "Iteration 198, loss = 0.21618178\n",
      "Iteration 199, loss = 0.20729145\n",
      "Iteration 200, loss = 0.20991372\n",
      "Iteration 201, loss = 0.20920313\n",
      "Iteration 202, loss = 0.20487691\n",
      "Iteration 203, loss = 0.20372596\n",
      "Iteration 204, loss = 0.21557570\n",
      "Iteration 205, loss = 0.20404031\n",
      "Iteration 206, loss = 0.20424073\n",
      "Iteration 207, loss = 0.20909215\n",
      "Iteration 208, loss = 0.20370675\n",
      "Iteration 209, loss = 0.20450227\n",
      "Iteration 210, loss = 0.20333574\n",
      "Iteration 211, loss = 0.20148354\n",
      "Iteration 212, loss = 0.22098226\n",
      "Iteration 213, loss = 0.20006204\n",
      "Iteration 214, loss = 0.20205093\n",
      "Iteration 215, loss = 0.20006410\n",
      "Iteration 216, loss = 0.19989909\n",
      "Iteration 217, loss = 0.20379157\n",
      "Iteration 218, loss = 0.21277926\n",
      "Iteration 219, loss = 0.20093346\n",
      "Iteration 220, loss = 0.22342463\n",
      "Iteration 221, loss = 0.20648836\n",
      "Iteration 222, loss = 0.20203257\n",
      "Iteration 223, loss = 0.19964286\n",
      "Iteration 224, loss = 0.21143798\n",
      "Iteration 225, loss = 0.20366940\n",
      "Iteration 226, loss = 0.19795294\n",
      "Iteration 227, loss = 0.20359072\n",
      "Iteration 228, loss = 0.19614678\n",
      "Iteration 229, loss = 0.21770910\n",
      "Iteration 230, loss = 0.20877832\n",
      "Iteration 231, loss = 0.20151657\n",
      "Iteration 232, loss = 0.19839005\n",
      "Iteration 233, loss = 0.21691170\n",
      "Iteration 234, loss = 0.20470746\n",
      "Iteration 235, loss = 0.19712660\n",
      "Iteration 236, loss = 0.19863744\n",
      "Iteration 237, loss = 0.21395043\n",
      "Iteration 238, loss = 0.19774935\n",
      "Iteration 239, loss = 0.19489071\n",
      "Iteration 240, loss = 0.19553378\n",
      "Iteration 241, loss = 0.19717147\n",
      "Iteration 242, loss = 0.19386959\n",
      "Iteration 243, loss = 0.21284720\n",
      "Iteration 244, loss = 0.19971205\n",
      "Iteration 245, loss = 0.19756762\n",
      "Iteration 246, loss = 0.20470435\n",
      "Iteration 247, loss = 0.19926679\n",
      "Iteration 248, loss = 0.19815471\n",
      "Iteration 249, loss = 0.19601713\n",
      "Iteration 250, loss = 0.19312109\n",
      "Iteration 251, loss = 0.19067813\n",
      "Iteration 252, loss = 0.19894931\n",
      "Iteration 253, loss = 0.19396354\n",
      "Iteration 254, loss = 0.19726461\n",
      "Iteration 255, loss = 0.20410105\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 256, loss = 0.19654022\n",
      "Iteration 257, loss = 0.19355724\n",
      "Iteration 258, loss = 0.19180633\n",
      "Iteration 259, loss = 0.19485845\n",
      "Iteration 260, loss = 0.18875571\n",
      "Iteration 261, loss = 0.19787584\n",
      "Iteration 262, loss = 0.20101309\n",
      "Iteration 263, loss = 0.21353824\n",
      "Iteration 264, loss = 0.20943076\n",
      "Iteration 265, loss = 0.19104752\n",
      "Iteration 266, loss = 0.18843822\n",
      "Iteration 267, loss = 0.19594904\n",
      "Iteration 268, loss = 0.19364366\n",
      "Iteration 269, loss = 0.18802369\n",
      "Iteration 270, loss = 0.19633776\n",
      "Iteration 271, loss = 0.20961441\n",
      "Iteration 272, loss = 0.18725113\n",
      "Iteration 273, loss = 0.19193197\n",
      "Iteration 274, loss = 0.18913627\n",
      "Iteration 275, loss = 0.19279995\n",
      "Iteration 276, loss = 0.19872667\n",
      "Iteration 277, loss = 0.18939884\n",
      "Iteration 278, loss = 0.21754057\n",
      "Iteration 279, loss = 0.18884795\n",
      "Iteration 280, loss = 0.18426191\n",
      "Iteration 281, loss = 0.24621322\n",
      "Iteration 282, loss = 0.20130037\n",
      "Iteration 283, loss = 0.22528712\n",
      "Iteration 284, loss = 0.19377343\n",
      "Iteration 285, loss = 0.20462903\n",
      "Iteration 286, loss = 0.18408533\n",
      "Iteration 287, loss = 0.20822052\n",
      "Iteration 288, loss = 0.18408632\n",
      "Iteration 289, loss = 0.18955293\n",
      "Iteration 290, loss = 0.19874732\n",
      "Iteration 291, loss = 0.18548296\n",
      "Iteration 292, loss = 0.18481109\n",
      "Iteration 293, loss = 0.18114494\n",
      "Iteration 294, loss = 0.18272022\n",
      "Iteration 295, loss = 0.18668385\n",
      "Iteration 296, loss = 0.21052177\n",
      "Iteration 297, loss = 0.19537378\n",
      "Iteration 298, loss = 0.18548988\n",
      "Iteration 299, loss = 0.18667825\n",
      "Iteration 300, loss = 0.18167880\n",
      "Iteration 301, loss = 0.19819805\n",
      "Iteration 302, loss = 0.18541010\n",
      "Iteration 303, loss = 0.18984698\n",
      "Iteration 304, loss = 0.17978427\n",
      "Iteration 305, loss = 0.18669759\n",
      "Iteration 306, loss = 0.18457025\n",
      "Iteration 307, loss = 0.20231842\n",
      "Iteration 308, loss = 0.20814698\n",
      "Iteration 309, loss = 0.18922791\n",
      "Iteration 310, loss = 0.19667759\n",
      "Iteration 311, loss = 0.18849477\n",
      "Iteration 312, loss = 0.17882349\n",
      "Iteration 313, loss = 0.17957339\n",
      "Iteration 314, loss = 0.21229750\n",
      "Iteration 315, loss = 0.17576118\n",
      "Iteration 316, loss = 0.20497082\n",
      "Iteration 317, loss = 0.18105751\n",
      "Iteration 318, loss = 0.19111193\n",
      "Iteration 319, loss = 0.18558190\n",
      "Iteration 320, loss = 0.18790359\n",
      "Iteration 321, loss = 0.21090184\n",
      "Iteration 322, loss = 0.17986484\n",
      "Iteration 323, loss = 0.18554403\n",
      "Iteration 324, loss = 0.18846934\n",
      "Iteration 325, loss = 0.18261667\n",
      "Iteration 326, loss = 0.18187949\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "The training time of the model: 1.08 .\n",
      "accuracy: 0.84375\n",
      "f1: 0.358974358974359\n"
     ]
    }
   ],
   "source": [
    "#model H = 64\n",
    "mlp64 = MLPClassifier(hidden_layer_sizes=(64, ), solver='sgd', \n",
    "                     max_iter=500, random_state=0, verbose=True)\n",
    "start = time()\n",
    "mlp64.fit(X_train, y_train)\n",
    "NN_time = time() - start\n",
    "y_hat = mlp64.predict(X_test)\n",
    "print(\"The training time of the model: %.2f .\"\n",
    "      % (NN_time))\n",
    "print('accuracy:', accuracy_score(y_test, y_hat))\n",
    "print('f1:', f1_score(y_test, y_hat))\n",
    "setting64_time1 = NN_time\n",
    "setting64_acc1 = accuracy_score(y_test, y_hat)\n",
    "setting64_f1 = f1_score(y_test, y_hat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 6.07467860\n",
      "Iteration 2, loss = 8.60882057\n",
      "Iteration 3, loss = 3.82847109\n",
      "Iteration 4, loss = 0.74617196\n",
      "Iteration 5, loss = 0.41307904\n",
      "Iteration 6, loss = 0.42966253\n",
      "Iteration 7, loss = 0.38170748\n",
      "Iteration 8, loss = 0.41638408\n",
      "Iteration 9, loss = 0.33103860\n",
      "Iteration 10, loss = 0.45328738\n",
      "Iteration 11, loss = 0.44078913\n",
      "Iteration 12, loss = 0.32472837\n",
      "Iteration 13, loss = 0.30309348\n",
      "Iteration 14, loss = 0.30977058\n",
      "Iteration 15, loss = 0.30106894\n",
      "Iteration 16, loss = 0.30488332\n",
      "Iteration 17, loss = 0.31006580\n",
      "Iteration 18, loss = 0.28425685\n",
      "Iteration 19, loss = 0.30588155\n",
      "Iteration 20, loss = 0.32744239\n",
      "Iteration 21, loss = 0.28071539\n",
      "Iteration 22, loss = 0.30789060\n",
      "Iteration 23, loss = 0.65132741\n",
      "Iteration 24, loss = 0.30133481\n",
      "Iteration 25, loss = 0.27897796\n",
      "Iteration 26, loss = 0.27309062\n",
      "Iteration 27, loss = 0.29217132\n",
      "Iteration 28, loss = 0.26942390\n",
      "Iteration 29, loss = 0.26730990\n",
      "Iteration 30, loss = 0.28028846\n",
      "Iteration 31, loss = 0.26995410\n",
      "Iteration 32, loss = 0.26726398\n",
      "Iteration 33, loss = 0.26436587\n",
      "Iteration 34, loss = 0.26615884\n",
      "Iteration 35, loss = 0.26516465\n",
      "Iteration 36, loss = 0.26930336\n",
      "Iteration 37, loss = 0.26767959\n",
      "Iteration 38, loss = 0.26695550\n",
      "Iteration 39, loss = 0.26364001\n",
      "Iteration 40, loss = 0.26865560\n",
      "Iteration 41, loss = 0.27566948\n",
      "Iteration 42, loss = 0.27903783\n",
      "Iteration 43, loss = 0.26748925\n",
      "Iteration 44, loss = 0.26279316\n",
      "Iteration 45, loss = 0.25631822\n",
      "Iteration 46, loss = 0.26508123\n",
      "Iteration 47, loss = 0.27110740\n",
      "Iteration 48, loss = 0.27964321\n",
      "Iteration 49, loss = 0.26286910\n",
      "Iteration 50, loss = 0.24741354\n",
      "Iteration 51, loss = 0.29900532\n",
      "Iteration 52, loss = 0.25704119\n",
      "Iteration 53, loss = 0.25120544\n",
      "Iteration 54, loss = 0.24918732\n",
      "Iteration 55, loss = 0.25252185\n",
      "Iteration 56, loss = 0.24163995\n",
      "Iteration 57, loss = 0.28183477\n",
      "Iteration 58, loss = 0.24658069\n",
      "Iteration 59, loss = 0.24572145\n",
      "Iteration 60, loss = 0.24948738\n",
      "Iteration 61, loss = 0.25320580\n",
      "Iteration 62, loss = 0.23842572\n",
      "Iteration 63, loss = 0.25380902\n",
      "Iteration 64, loss = 0.24225430\n",
      "Iteration 65, loss = 0.25231220\n",
      "Iteration 66, loss = 0.23822074\n",
      "Iteration 67, loss = 0.25146805\n",
      "Iteration 68, loss = 0.23596477\n",
      "Iteration 69, loss = 0.27383394\n",
      "Iteration 70, loss = 0.27421766\n",
      "Iteration 71, loss = 0.25072865\n",
      "Iteration 72, loss = 0.24907656\n",
      "Iteration 73, loss = 0.26876693\n",
      "Iteration 74, loss = 0.24297987\n",
      "Iteration 75, loss = 0.23931472\n",
      "Iteration 76, loss = 0.22993424\n",
      "Iteration 77, loss = 0.27600675\n",
      "Iteration 78, loss = 0.23657378\n",
      "Iteration 79, loss = 0.24506893\n",
      "Iteration 80, loss = 0.24304824\n",
      "Iteration 81, loss = 0.27150408\n",
      "Iteration 82, loss = 0.22844756\n",
      "Iteration 83, loss = 0.33831822\n",
      "Iteration 84, loss = 0.22487803\n",
      "Iteration 85, loss = 0.24872675\n",
      "Iteration 86, loss = 0.24801091\n",
      "Iteration 87, loss = 0.24110412\n",
      "Iteration 88, loss = 0.22791615\n",
      "Iteration 89, loss = 0.22795749\n",
      "Iteration 90, loss = 0.22186108\n",
      "Iteration 91, loss = 0.22925815\n",
      "Iteration 92, loss = 0.23958479\n",
      "Iteration 93, loss = 0.22101318\n",
      "Iteration 94, loss = 0.22095671\n",
      "Iteration 95, loss = 0.21739248\n",
      "Iteration 96, loss = 0.21873808\n",
      "Iteration 97, loss = 0.21845123\n",
      "Iteration 98, loss = 0.28702339\n",
      "Iteration 99, loss = 0.24020691\n",
      "Iteration 100, loss = 0.22780471\n",
      "Iteration 101, loss = 0.24070790\n",
      "Iteration 102, loss = 0.21542770\n",
      "Iteration 103, loss = 0.21503883\n",
      "Iteration 104, loss = 0.23369938\n",
      "Iteration 105, loss = 0.21893394\n",
      "Iteration 106, loss = 0.22099864\n",
      "Iteration 107, loss = 0.22536254\n",
      "Iteration 108, loss = 0.22939040\n",
      "Iteration 109, loss = 0.21259123\n",
      "Iteration 110, loss = 0.23772773\n",
      "Iteration 111, loss = 0.21369137\n",
      "Iteration 112, loss = 0.23007986\n",
      "Iteration 113, loss = 0.21429485\n",
      "Iteration 114, loss = 0.21295535\n",
      "Iteration 115, loss = 0.21202965\n",
      "Iteration 116, loss = 0.20921326\n",
      "Iteration 117, loss = 0.21514072\n",
      "Iteration 118, loss = 0.47018500\n",
      "Iteration 119, loss = 0.21194006\n",
      "Iteration 120, loss = 0.21537971\n",
      "Iteration 121, loss = 0.23808773\n",
      "Iteration 122, loss = 0.21065416\n",
      "Iteration 123, loss = 0.20514392\n",
      "Iteration 124, loss = 0.21129299\n",
      "Iteration 125, loss = 0.20637083\n",
      "Iteration 126, loss = 0.21090257\n",
      "Iteration 127, loss = 0.20779997\n",
      "Iteration 128, loss = 0.20981407\n",
      "Iteration 129, loss = 0.27244479\n",
      "Iteration 130, loss = 0.21898173\n",
      "Iteration 131, loss = 0.20959000\n",
      "Iteration 132, loss = 0.21622699\n",
      "Iteration 133, loss = 0.20914468\n",
      "Iteration 134, loss = 0.20493063\n",
      "Iteration 135, loss = 0.20002945\n",
      "Iteration 136, loss = 0.20904043\n",
      "Iteration 137, loss = 0.19999885\n",
      "Iteration 138, loss = 0.21154173\n",
      "Iteration 139, loss = 0.20729562\n",
      "Iteration 140, loss = 0.20012621\n",
      "Iteration 141, loss = 0.19793065\n",
      "Iteration 142, loss = 0.20361639\n",
      "Iteration 143, loss = 0.28350855\n",
      "Iteration 144, loss = 0.20223363\n",
      "Iteration 145, loss = 0.20550872\n",
      "Iteration 146, loss = 0.22233101\n",
      "Iteration 147, loss = 0.19838843\n",
      "Iteration 148, loss = 0.21880623\n",
      "Iteration 149, loss = 0.19313724\n",
      "Iteration 150, loss = 0.19151347\n",
      "Iteration 151, loss = 0.19336718\n",
      "Iteration 152, loss = 0.19839635\n",
      "Iteration 153, loss = 0.19707664\n",
      "Iteration 154, loss = 0.19434109\n",
      "Iteration 155, loss = 0.22552785\n",
      "Iteration 156, loss = 0.19368381\n",
      "Iteration 157, loss = 0.33360452\n",
      "Iteration 158, loss = 0.21988210\n",
      "Iteration 159, loss = 0.19563111\n",
      "Iteration 160, loss = 0.20339130\n",
      "Iteration 161, loss = 0.19628418\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "The training time of the model: 0.54 .\n",
      "accuracy: 0.90625\n",
      "f1: 0.21052631578947367\n"
     ]
    }
   ],
   "source": [
    "#repeat model H = 64 multiple times\n",
    "mlp64 = MLPClassifier(hidden_layer_sizes=(64, ), solver='sgd', \n",
    "                     max_iter=500, random_state=1, verbose=True)\n",
    "start = time()\n",
    "mlp64.fit(X_train, y_train)\n",
    "NN_time = time() - start\n",
    "y_hat = mlp64.predict(X_test)\n",
    "print(\"The training time of the model: %.2f .\"\n",
    "      % (NN_time))\n",
    "print('accuracy:', accuracy_score(y_test, y_hat))\n",
    "print('f1:', f1_score(y_test, y_hat))\n",
    "setting64_time2 = NN_time\n",
    "setting64_acc2 = accuracy_score(y_test, y_hat)\n",
    "setting64_f12 = f1_score(y_test, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.10129733\n",
      "Iteration 2, loss = 0.77692529\n",
      "Iteration 3, loss = 1.44188703\n",
      "Iteration 4, loss = 3.07007134\n",
      "Iteration 5, loss = 0.85724640\n",
      "Iteration 6, loss = 0.59781545\n",
      "Iteration 7, loss = 0.45544059\n",
      "Iteration 8, loss = 0.37235871\n",
      "Iteration 9, loss = 0.34978574\n",
      "Iteration 10, loss = 0.35726552\n",
      "Iteration 11, loss = 0.37120981\n",
      "Iteration 12, loss = 0.34381209\n",
      "Iteration 13, loss = 0.39370170\n",
      "Iteration 14, loss = 0.34729339\n",
      "Iteration 15, loss = 0.32171415\n",
      "Iteration 16, loss = 0.32617181\n",
      "Iteration 17, loss = 0.34988561\n",
      "Iteration 18, loss = 1.36277522\n",
      "Iteration 19, loss = 0.61902575\n",
      "Iteration 20, loss = 0.32622557\n",
      "Iteration 21, loss = 0.30764348\n",
      "Iteration 22, loss = 0.31736124\n",
      "Iteration 23, loss = 0.30018452\n",
      "Iteration 24, loss = 0.31690199\n",
      "Iteration 25, loss = 0.30715318\n",
      "Iteration 26, loss = 0.29611331\n",
      "Iteration 27, loss = 0.31081445\n",
      "Iteration 28, loss = 0.28626177\n",
      "Iteration 29, loss = 0.28826100\n",
      "Iteration 30, loss = 0.30059463\n",
      "Iteration 31, loss = 0.31927122\n",
      "Iteration 32, loss = 0.28272021\n",
      "Iteration 33, loss = 0.28014145\n",
      "Iteration 34, loss = 0.28100635\n",
      "Iteration 35, loss = 0.29970220\n",
      "Iteration 36, loss = 0.29095489\n",
      "Iteration 37, loss = 0.27995330\n",
      "Iteration 38, loss = 0.28486466\n",
      "Iteration 39, loss = 0.28247213\n",
      "Iteration 40, loss = 0.27299140\n",
      "Iteration 41, loss = 0.27576722\n",
      "Iteration 42, loss = 0.27437198\n",
      "Iteration 43, loss = 0.27355759\n",
      "Iteration 44, loss = 0.26868311\n",
      "Iteration 45, loss = 0.26928546\n",
      "Iteration 46, loss = 0.27878999\n",
      "Iteration 47, loss = 0.30075412\n",
      "Iteration 48, loss = 0.27352371\n",
      "Iteration 49, loss = 0.26762303\n",
      "Iteration 50, loss = 0.27411967\n",
      "Iteration 51, loss = 0.31933644\n",
      "Iteration 52, loss = 0.26885936\n",
      "Iteration 53, loss = 0.26559691\n",
      "Iteration 54, loss = 0.27004824\n",
      "Iteration 55, loss = 0.26322434\n",
      "Iteration 56, loss = 0.26570661\n",
      "Iteration 57, loss = 0.27453557\n",
      "Iteration 58, loss = 0.27568117\n",
      "Iteration 59, loss = 0.26161162\n",
      "Iteration 60, loss = 0.25859531\n",
      "Iteration 61, loss = 0.26568024\n",
      "Iteration 62, loss = 0.25550042\n",
      "Iteration 63, loss = 0.26237512\n",
      "Iteration 64, loss = 0.26767951\n",
      "Iteration 65, loss = 0.25800343\n",
      "Iteration 66, loss = 0.25888527\n",
      "Iteration 67, loss = 0.26299091\n",
      "Iteration 68, loss = 0.27585957\n",
      "Iteration 69, loss = 0.25974395\n",
      "Iteration 70, loss = 0.25365115\n",
      "Iteration 71, loss = 0.26307244\n",
      "Iteration 72, loss = 0.25069728\n",
      "Iteration 73, loss = 0.25702117\n",
      "Iteration 74, loss = 0.25081296\n",
      "Iteration 75, loss = 0.27280186\n",
      "Iteration 76, loss = 0.25165760\n",
      "Iteration 77, loss = 0.26587098\n",
      "Iteration 78, loss = 0.25567237\n",
      "Iteration 79, loss = 0.25042319\n",
      "Iteration 80, loss = 0.25246971\n",
      "Iteration 81, loss = 0.25885903\n",
      "Iteration 82, loss = 0.24735673\n",
      "Iteration 83, loss = 0.25058894\n",
      "Iteration 84, loss = 0.24963896\n",
      "Iteration 85, loss = 0.32840359\n",
      "Iteration 86, loss = 0.24810191\n",
      "Iteration 87, loss = 0.24956844\n",
      "Iteration 88, loss = 0.24863170\n",
      "Iteration 89, loss = 0.24840108\n",
      "Iteration 90, loss = 0.24045679\n",
      "Iteration 91, loss = 0.25896113\n",
      "Iteration 92, loss = 0.25263275\n",
      "Iteration 93, loss = 0.24492118\n",
      "Iteration 94, loss = 0.25720546\n",
      "Iteration 95, loss = 0.23902605\n",
      "Iteration 96, loss = 0.28176944\n",
      "Iteration 97, loss = 0.24141870\n",
      "Iteration 98, loss = 0.25979452\n",
      "Iteration 99, loss = 0.25372102\n",
      "Iteration 100, loss = 0.24743060\n",
      "Iteration 101, loss = 0.23627196\n",
      "Iteration 102, loss = 0.24966863\n",
      "Iteration 103, loss = 0.23786977\n",
      "Iteration 104, loss = 0.24162318\n",
      "Iteration 105, loss = 0.23561492\n",
      "Iteration 106, loss = 0.26125614\n",
      "Iteration 107, loss = 0.24356450\n",
      "Iteration 108, loss = 0.23830287\n",
      "Iteration 109, loss = 0.23339720\n",
      "Iteration 110, loss = 0.25349787\n",
      "Iteration 111, loss = 0.24098106\n",
      "Iteration 112, loss = 0.24204528\n",
      "Iteration 113, loss = 0.24722097\n",
      "Iteration 114, loss = 0.25744192\n",
      "Iteration 115, loss = 0.23404233\n",
      "Iteration 116, loss = 0.30264438\n",
      "Iteration 117, loss = 0.23577389\n",
      "Iteration 118, loss = 0.23989206\n",
      "Iteration 119, loss = 0.23265646\n",
      "Iteration 120, loss = 0.24069379\n",
      "Iteration 121, loss = 0.23171207\n",
      "Iteration 122, loss = 0.23359903\n",
      "Iteration 123, loss = 0.23135128\n",
      "Iteration 124, loss = 0.23732006\n",
      "Iteration 125, loss = 0.23276056\n",
      "Iteration 126, loss = 0.23118481\n",
      "Iteration 127, loss = 0.23645879\n",
      "Iteration 128, loss = 0.22991543\n",
      "Iteration 129, loss = 0.25506579\n",
      "Iteration 130, loss = 0.23068587\n",
      "Iteration 131, loss = 0.22882182\n",
      "Iteration 132, loss = 0.22779606\n",
      "Iteration 133, loss = 0.22878230\n",
      "Iteration 134, loss = 0.22772527\n",
      "Iteration 135, loss = 0.22824584\n",
      "Iteration 136, loss = 0.22727635\n",
      "Iteration 137, loss = 0.26380606\n",
      "Iteration 138, loss = 0.22687868\n",
      "Iteration 139, loss = 0.23215877\n",
      "Iteration 140, loss = 0.23236125\n",
      "Iteration 141, loss = 0.23773427\n",
      "Iteration 142, loss = 0.23454859\n",
      "Iteration 143, loss = 0.22750648\n",
      "Iteration 144, loss = 0.23882677\n",
      "Iteration 145, loss = 0.22835349\n",
      "Iteration 146, loss = 0.22375194\n",
      "Iteration 147, loss = 0.22498357\n",
      "Iteration 148, loss = 0.23870072\n",
      "Iteration 149, loss = 0.23466765\n",
      "Iteration 150, loss = 0.23055951\n",
      "Iteration 151, loss = 0.22244952\n",
      "Iteration 152, loss = 0.22301206\n",
      "Iteration 153, loss = 0.22715719\n",
      "Iteration 154, loss = 0.23082857\n",
      "Iteration 155, loss = 0.22702501\n",
      "Iteration 156, loss = 0.23237594\n",
      "Iteration 157, loss = 0.22057290\n",
      "Iteration 158, loss = 0.22111358\n",
      "Iteration 159, loss = 0.22122836\n",
      "Iteration 160, loss = 0.23574643\n",
      "Iteration 161, loss = 0.22688016\n",
      "Iteration 162, loss = 0.22289337\n",
      "Iteration 163, loss = 0.21640874\n",
      "Iteration 164, loss = 0.22804205\n",
      "Iteration 165, loss = 0.22488928\n",
      "Iteration 166, loss = 0.21942858\n",
      "Iteration 167, loss = 0.21813213\n",
      "Iteration 168, loss = 0.21996806\n",
      "Iteration 169, loss = 0.23702206\n",
      "Iteration 170, loss = 0.21927216\n",
      "Iteration 171, loss = 0.25029505\n",
      "Iteration 172, loss = 0.22064307\n",
      "Iteration 173, loss = 0.22269876\n",
      "Iteration 174, loss = 0.21387382\n",
      "Iteration 175, loss = 0.21885428\n",
      "Iteration 176, loss = 0.21582974\n",
      "Iteration 177, loss = 0.22101540\n",
      "Iteration 178, loss = 0.21934070\n",
      "Iteration 179, loss = 0.24441438\n",
      "Iteration 180, loss = 0.21493488\n",
      "Iteration 181, loss = 0.27204337\n",
      "Iteration 182, loss = 0.21360686\n",
      "Iteration 183, loss = 0.21454509\n",
      "Iteration 184, loss = 0.21638128\n",
      "Iteration 185, loss = 0.22137210\n",
      "Iteration 186, loss = 0.21563271\n",
      "Iteration 187, loss = 0.23802191\n",
      "Iteration 188, loss = 0.21313051\n",
      "Iteration 189, loss = 0.21188492\n",
      "Iteration 190, loss = 0.21108452\n",
      "Iteration 191, loss = 0.27575433\n",
      "Iteration 192, loss = 0.21352532\n",
      "Iteration 193, loss = 0.20855168\n",
      "Iteration 194, loss = 0.21838066\n",
      "Iteration 195, loss = 0.21237220\n",
      "Iteration 196, loss = 0.21115041\n",
      "Iteration 197, loss = 0.20882188\n",
      "Iteration 198, loss = 0.20978968\n",
      "Iteration 199, loss = 0.22081045\n",
      "Iteration 200, loss = 0.21089369\n",
      "Iteration 201, loss = 0.21145420\n",
      "Iteration 202, loss = 0.21388573\n",
      "Iteration 203, loss = 0.21894082\n",
      "Iteration 204, loss = 0.20951479\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "The training time of the model: 0.67 .\n",
      "accuracy: 0.9\n",
      "f1: 0.2727272727272727\n",
      "The mean training time of the model H = 64: 0.76295924186706543 (std: 0.22730650925725335) .\n",
      "The mean accuracy time of the model H = 64: 0.88333333333333330 (std: 0.02810570325673343) .\n",
      "The mean F1 score time of the model H = 64: 0.28074264916370179 (std: 0.06086810919750254) .\n"
     ]
    }
   ],
   "source": [
    "#repeat model H = 64 multiple times\n",
    "#report the mean and standard deviation of the training time, accuracy, and F1 score for the setting.\n",
    "mlp64 = MLPClassifier(hidden_layer_sizes=(64, ), solver='sgd', \n",
    "                     max_iter=500, random_state=2, verbose=True)\n",
    "start = time()\n",
    "mlp64.fit(X_train, y_train)\n",
    "NN_time = time() - start\n",
    "y_hat = mlp64.predict(X_test)\n",
    "print(\"The training time of the model: %.2f .\"\n",
    "      % (NN_time))\n",
    "print('accuracy:', accuracy_score(y_test, y_hat))\n",
    "print('f1:', f1_score(y_test, y_hat))\n",
    "setting64_time3 = NN_time\n",
    "setting64_acc3 = accuracy_score(y_test, y_hat)\n",
    "setting64_f13 = f1_score(y_test, y_hat)\n",
    "set64_time = np.array([setting64_time1, setting64_time2, setting64_time3])\n",
    "mean_time = np.mean(set64_time)\n",
    "std_time = np.std(set64_time)\n",
    "set64_acc = np.array([setting64_acc1, setting64_acc2, setting64_acc3])\n",
    "mean64_acc = np.mean(set64_acc)\n",
    "std_acc = np.std(set64_acc)\n",
    "set64_f1 = np.array([setting64_f1, setting64_f12, setting64_f13])\n",
    "mean64_f1 = np.mean(set64_f1)\n",
    "std_f1 = np.std(set64_f1)\n",
    "print(\"The mean training time of the model H = 64: %.17f (std: %.17f) .\"\n",
    "      % (mean_time, std_time))\n",
    "print(\"The mean accuracy time of the model H = 64: %.17f (std: %.17f) .\"\n",
    "      % (mean64_acc, std_acc))\n",
    "print(\"The mean F1 score time of the model H = 64: %.17f (std: %.17f) .\"\n",
    "      % (mean64_f1, std_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the accuracy and the F1 score for diï¬€erent values of H.\n",
    "X = np.array([1, 2, 4, 8, 16, 32, 64])\n",
    "y_acc = np.array([mean1_acc, mean2_acc, mean4_acc, mean8_acc, mean16_acc, mean32_acc, mean64_acc ])\n",
    "y_f1 = np.array([mean1_f1, mean2_f1, mean4_f1, mean8_f1, mean16_f1, mean32_f1, mean64_f1])\n",
    "plt.scatter(X,y_acc)\n",
    "plt.scatter(X,y_f1)\n",
    "plt.title('Accuracy and F1 score VS values of H')\n",
    "plt.xlabel('values of H')\n",
    "plt.ylabel('Accuracy(blue)/F1 score(orange)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4  Predicting the Winners in the Pokemon Battles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "q4_test = pd.read_csv('./q4_test.csv', sep =',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>First_pokemon</th>\n",
       "      <th>Second_pokemon</th>\n",
       "      <th>Winner</th>\n",
       "      <th>First_Type1</th>\n",
       "      <th>First_Type2</th>\n",
       "      <th>First_HP</th>\n",
       "      <th>First_Atk</th>\n",
       "      <th>First_Def</th>\n",
       "      <th>First_SpAtk</th>\n",
       "      <th>First_SpDef</th>\n",
       "      <th>First_Speed</th>\n",
       "      <th>First_generation</th>\n",
       "      <th>First_hasGender</th>\n",
       "      <th>First_legendary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>173</td>\n",
       "      <td>463</td>\n",
       "      <td>463</td>\n",
       "      <td>Water</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>174</td>\n",
       "      <td>307</td>\n",
       "      <td>307</td>\n",
       "      <td>Water</td>\n",
       "      <td>NaN</td>\n",
       "      <td>65.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>772</td>\n",
       "      <td>181</td>\n",
       "      <td>772</td>\n",
       "      <td>Fighting</td>\n",
       "      <td>Flying</td>\n",
       "      <td>78.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>430</td>\n",
       "      <td>356</td>\n",
       "      <td>430</td>\n",
       "      <td>Psychic</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>519</td>\n",
       "      <td>695</td>\n",
       "      <td>519</td>\n",
       "      <td>Fire</td>\n",
       "      <td>NaN</td>\n",
       "      <td>75.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39995</th>\n",
       "      <td>416</td>\n",
       "      <td>778</td>\n",
       "      <td>778</td>\n",
       "      <td>Ice</td>\n",
       "      <td>NaN</td>\n",
       "      <td>80.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39996</th>\n",
       "      <td>221</td>\n",
       "      <td>265</td>\n",
       "      <td>265</td>\n",
       "      <td>Bug</td>\n",
       "      <td>Steel</td>\n",
       "      <td>75.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39997</th>\n",
       "      <td>35</td>\n",
       "      <td>448</td>\n",
       "      <td>448</td>\n",
       "      <td>Poison</td>\n",
       "      <td>NaN</td>\n",
       "      <td>55.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39998</th>\n",
       "      <td>791</td>\n",
       "      <td>713</td>\n",
       "      <td>713</td>\n",
       "      <td>Flying</td>\n",
       "      <td>Dragon</td>\n",
       "      <td>40.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39999</th>\n",
       "      <td>312</td>\n",
       "      <td>486</td>\n",
       "      <td>486</td>\n",
       "      <td>Normal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>60.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40000 rows Ã— 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       First_pokemon  Second_pokemon  Winner First_Type1 First_Type2  \\\n",
       "0                173             463     463       Water         NaN   \n",
       "1                174             307     307       Water         NaN   \n",
       "2                772             181     772    Fighting      Flying   \n",
       "3                430             356     430     Psychic         NaN   \n",
       "4                519             695     519        Fire         NaN   \n",
       "...              ...             ...     ...         ...         ...   \n",
       "39995            416             778     778         Ice         NaN   \n",
       "39996            221             265     265         Bug       Steel   \n",
       "39997             35             448     448      Poison         NaN   \n",
       "39998            791             713     713      Flying      Dragon   \n",
       "39999            312             486     486      Normal         NaN   \n",
       "\n",
       "       First_HP  First_Atk  First_Def  First_SpAtk  First_SpDef  First_Speed  \\\n",
       "0          50.0       65.0       64.0         44.0         48.0         43.0   \n",
       "1          65.0       80.0       80.0         59.0         63.0         58.0   \n",
       "2          78.0       92.0       75.0         74.0         63.0        118.0   \n",
       "3          50.0      180.0       20.0        180.0         20.0        150.0   \n",
       "4          75.0       95.0       67.0        125.0         95.0         83.0   \n",
       "...         ...        ...        ...          ...          ...          ...   \n",
       "39995      80.0       50.0      100.0        100.0        200.0         50.0   \n",
       "39996      75.0       90.0      140.0         60.0         60.0         40.0   \n",
       "39997      55.0       47.0       52.0         40.0         40.0         41.0   \n",
       "39998      40.0       30.0       35.0         45.0         40.0         55.0   \n",
       "39999      60.0       60.0       60.0         35.0         35.0         30.0   \n",
       "\n",
       "       First_generation First_hasGender First_legendary  \n",
       "0                   2.0            True           False  \n",
       "1                   2.0            True           False  \n",
       "2                   6.0            True           False  \n",
       "3                   3.0           False            True  \n",
       "4                   4.0            True           False  \n",
       "...                 ...             ...             ...  \n",
       "39995               3.0           False            True  \n",
       "39996               2.0            True           False  \n",
       "39997               1.0            True           False  \n",
       "39998               6.0            True           False  \n",
       "39999               3.0            True           False  \n",
       "\n",
       "[40000 rows x 14 columns]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#organize data from pokemon to battles\n",
    "index = 0\n",
    "for first in battles['First_pokemon']:\n",
    "    battles.loc[index,'First_Type1'] = pokemon.loc[first-1, 'Type 1']\n",
    "    battles.loc[index,'First_Type2'] = pokemon.loc[first-1, 'Type 2']\n",
    "    battles.loc[index,'First_HP'] = pokemon.loc[first-1, 'HP']\n",
    "    battles.loc[index,'First_Atk'] = pokemon.loc[first-1, 'Attack']\n",
    "    battles.loc[index,'First_Def'] = pokemon.loc[first-1, 'Defense']\n",
    "    battles.loc[index,'First_SpAtk'] = pokemon.loc[first-1, 'Sp. Atk']\n",
    "    battles.loc[index,'First_SpDef'] = pokemon.loc[first-1, 'Sp. Def']\n",
    "    battles.loc[index,'First_Speed'] = pokemon.loc[first-1, 'Speed']\n",
    "    battles.loc[index,'First_generation'] = pokemon.loc[first-1, 'Generation']\n",
    "    battles.loc[index,'First_hasGender'] = pokemon.loc[first-1, 'Has Gender']\n",
    "    battles.loc[index,'First_legendary'] = pokemon.loc[first-1, 'Legendary']\n",
    "    index += 1\n",
    "battles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>First_pokemon</th>\n",
       "      <th>Second_pokemon</th>\n",
       "      <th>Winner</th>\n",
       "      <th>First_Type1</th>\n",
       "      <th>First_Type2</th>\n",
       "      <th>First_HP</th>\n",
       "      <th>First_Atk</th>\n",
       "      <th>First_Def</th>\n",
       "      <th>First_SpAtk</th>\n",
       "      <th>First_SpDef</th>\n",
       "      <th>...</th>\n",
       "      <th>Second_Type2</th>\n",
       "      <th>Second_HP</th>\n",
       "      <th>Second_Atk</th>\n",
       "      <th>Second_Def</th>\n",
       "      <th>Second_SpAtk</th>\n",
       "      <th>Second_SpDef</th>\n",
       "      <th>Second_Speed</th>\n",
       "      <th>Second_generation</th>\n",
       "      <th>Second_hasGender</th>\n",
       "      <th>Second_legendary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>173</td>\n",
       "      <td>463</td>\n",
       "      <td>463</td>\n",
       "      <td>Water</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Flying</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>174</td>\n",
       "      <td>307</td>\n",
       "      <td>307</td>\n",
       "      <td>Water</td>\n",
       "      <td>NaN</td>\n",
       "      <td>65.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Fairy</td>\n",
       "      <td>68.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>772</td>\n",
       "      <td>181</td>\n",
       "      <td>772</td>\n",
       "      <td>Fighting</td>\n",
       "      <td>Flying</td>\n",
       "      <td>78.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Flying</td>\n",
       "      <td>55.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>430</td>\n",
       "      <td>356</td>\n",
       "      <td>430</td>\n",
       "      <td>Psychic</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>70.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>519</td>\n",
       "      <td>695</td>\n",
       "      <td>519</td>\n",
       "      <td>Fire</td>\n",
       "      <td>NaN</td>\n",
       "      <td>75.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Dragon</td>\n",
       "      <td>52.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39995</th>\n",
       "      <td>416</td>\n",
       "      <td>778</td>\n",
       "      <td>778</td>\n",
       "      <td>Ice</td>\n",
       "      <td>NaN</td>\n",
       "      <td>80.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Fairy</td>\n",
       "      <td>57.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39996</th>\n",
       "      <td>221</td>\n",
       "      <td>265</td>\n",
       "      <td>265</td>\n",
       "      <td>Bug</td>\n",
       "      <td>Steel</td>\n",
       "      <td>75.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39997</th>\n",
       "      <td>35</td>\n",
       "      <td>448</td>\n",
       "      <td>448</td>\n",
       "      <td>Poison</td>\n",
       "      <td>NaN</td>\n",
       "      <td>55.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>77.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39998</th>\n",
       "      <td>791</td>\n",
       "      <td>713</td>\n",
       "      <td>713</td>\n",
       "      <td>Flying</td>\n",
       "      <td>Dragon</td>\n",
       "      <td>40.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Ice</td>\n",
       "      <td>125.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39999</th>\n",
       "      <td>312</td>\n",
       "      <td>486</td>\n",
       "      <td>486</td>\n",
       "      <td>Normal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>60.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Psychic</td>\n",
       "      <td>67.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40000 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       First_pokemon  Second_pokemon  Winner First_Type1 First_Type2  \\\n",
       "0                173             463     463       Water         NaN   \n",
       "1                174             307     307       Water         NaN   \n",
       "2                772             181     772    Fighting      Flying   \n",
       "3                430             356     430     Psychic         NaN   \n",
       "4                519             695     519        Fire         NaN   \n",
       "...              ...             ...     ...         ...         ...   \n",
       "39995            416             778     778         Ice         NaN   \n",
       "39996            221             265     265         Bug       Steel   \n",
       "39997             35             448     448      Poison         NaN   \n",
       "39998            791             713     713      Flying      Dragon   \n",
       "39999            312             486     486      Normal         NaN   \n",
       "\n",
       "       First_HP  First_Atk  First_Def  First_SpAtk  First_SpDef  ...  \\\n",
       "0          50.0       65.0       64.0         44.0         48.0  ...   \n",
       "1          65.0       80.0       80.0         59.0         63.0  ...   \n",
       "2          78.0       92.0       75.0         74.0         63.0  ...   \n",
       "3          50.0      180.0       20.0        180.0         20.0  ...   \n",
       "4          75.0       95.0       67.0        125.0         95.0  ...   \n",
       "...         ...        ...        ...          ...          ...  ...   \n",
       "39995      80.0       50.0      100.0        100.0        200.0  ...   \n",
       "39996      75.0       90.0      140.0         60.0         60.0  ...   \n",
       "39997      55.0       47.0       52.0         40.0         40.0  ...   \n",
       "39998      40.0       30.0       35.0         45.0         40.0  ...   \n",
       "39999      60.0       60.0       60.0         35.0         35.0  ...   \n",
       "\n",
       "       Second_Type2  Second_HP Second_Atk Second_Def Second_SpAtk  \\\n",
       "0            Flying       30.0       30.0       42.0         30.0   \n",
       "1             Fairy       68.0       85.0       65.0        165.0   \n",
       "2            Flying       55.0       35.0       50.0         55.0   \n",
       "3               NaN       70.0       85.0      140.0         85.0   \n",
       "4            Dragon       52.0       65.0       50.0         45.0   \n",
       "...             ...        ...        ...        ...          ...   \n",
       "39995         Fairy       57.0       80.0       91.0         80.0   \n",
       "39996           NaN      100.0       75.0      115.0         90.0   \n",
       "39997           NaN       77.0       85.0       51.0         55.0   \n",
       "39998           Ice      125.0      120.0       90.0        170.0   \n",
       "39999       Psychic       67.0       89.0      116.0         79.0   \n",
       "\n",
       "      Second_SpDef  Second_Speed  Second_generation  Second_hasGender  \\\n",
       "0             42.0          70.0                4.0              True   \n",
       "1            135.0         100.0                3.0              True   \n",
       "2            110.0          85.0                2.0              True   \n",
       "3             70.0          20.0                3.0              True   \n",
       "4             50.0          38.0                5.0              True   \n",
       "...            ...           ...                ...               ...   \n",
       "39995         87.0          75.0                6.0              True   \n",
       "39996        115.0          85.0                2.0             False   \n",
       "39997         51.0          65.0                4.0              True   \n",
       "39998        100.0          95.0                5.0             False   \n",
       "39999        116.0          33.0                4.0             False   \n",
       "\n",
       "       Second_legendary  \n",
       "0                 False  \n",
       "1                 False  \n",
       "2                 False  \n",
       "3                 False  \n",
       "4                 False  \n",
       "...                 ...  \n",
       "39995             False  \n",
       "39996              True  \n",
       "39997             False  \n",
       "39998              True  \n",
       "39999             False  \n",
       "\n",
       "[40000 rows x 25 columns]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#organize data from pokemon to battles\n",
    "index = 0\n",
    "for second in battles['Second_pokemon']:\n",
    "    battles.loc[index,'Second_Type1'] = pokemon.loc[second-1, 'Type 1']\n",
    "    battles.loc[index,'Second_Type2'] = pokemon.loc[second-1, 'Type 2']\n",
    "    battles.loc[index,'Second_HP'] = pokemon.loc[second-1, 'HP']\n",
    "    battles.loc[index,'Second_Atk'] = pokemon.loc[second-1, 'Attack']\n",
    "    battles.loc[index,'Second_Def'] = pokemon.loc[second-1, 'Defense']\n",
    "    battles.loc[index,'Second_SpAtk'] = pokemon.loc[second-1, 'Sp. Atk']\n",
    "    battles.loc[index,'Second_SpDef'] = pokemon.loc[second-1, 'Sp. Def']\n",
    "    battles.loc[index,'Second_Speed'] = pokemon.loc[second-1, 'Speed']\n",
    "    battles.loc[index,'Second_generation'] = pokemon.loc[second-1, 'Generation']\n",
    "    battles.loc[index,'Second_hasGender'] = pokemon.loc[second-1, 'Has Gender']\n",
    "    battles.loc[index,'Second_legendary'] = pokemon.loc[second-1, 'Legendary']\n",
    "    index += 1\n",
    "battles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>First_pokemon</th>\n",
       "      <th>Second_pokemon</th>\n",
       "      <th>Winner</th>\n",
       "      <th>First_Type1</th>\n",
       "      <th>First_Type2</th>\n",
       "      <th>First_HP</th>\n",
       "      <th>First_Atk</th>\n",
       "      <th>First_Def</th>\n",
       "      <th>First_SpAtk</th>\n",
       "      <th>First_SpDef</th>\n",
       "      <th>...</th>\n",
       "      <th>Second_Type2_Ghost</th>\n",
       "      <th>Second_Type2_Grass</th>\n",
       "      <th>Second_Type2_Ground</th>\n",
       "      <th>Second_Type2_Ice</th>\n",
       "      <th>Second_Type2_Normal</th>\n",
       "      <th>Second_Type2_Poison</th>\n",
       "      <th>Second_Type2_Psychic</th>\n",
       "      <th>Second_Type2_Rock</th>\n",
       "      <th>Second_Type2_Steel</th>\n",
       "      <th>Second_Type2_Water</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>173</td>\n",
       "      <td>463</td>\n",
       "      <td>463</td>\n",
       "      <td>Water</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>174</td>\n",
       "      <td>307</td>\n",
       "      <td>307</td>\n",
       "      <td>Water</td>\n",
       "      <td>NaN</td>\n",
       "      <td>65.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>772</td>\n",
       "      <td>181</td>\n",
       "      <td>772</td>\n",
       "      <td>Fighting</td>\n",
       "      <td>Flying</td>\n",
       "      <td>78.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>430</td>\n",
       "      <td>356</td>\n",
       "      <td>430</td>\n",
       "      <td>Psychic</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>519</td>\n",
       "      <td>695</td>\n",
       "      <td>519</td>\n",
       "      <td>Fire</td>\n",
       "      <td>NaN</td>\n",
       "      <td>75.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39995</th>\n",
       "      <td>416</td>\n",
       "      <td>778</td>\n",
       "      <td>778</td>\n",
       "      <td>Ice</td>\n",
       "      <td>NaN</td>\n",
       "      <td>80.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39996</th>\n",
       "      <td>221</td>\n",
       "      <td>265</td>\n",
       "      <td>265</td>\n",
       "      <td>Bug</td>\n",
       "      <td>Steel</td>\n",
       "      <td>75.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39997</th>\n",
       "      <td>35</td>\n",
       "      <td>448</td>\n",
       "      <td>448</td>\n",
       "      <td>Poison</td>\n",
       "      <td>NaN</td>\n",
       "      <td>55.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39998</th>\n",
       "      <td>791</td>\n",
       "      <td>713</td>\n",
       "      <td>713</td>\n",
       "      <td>Flying</td>\n",
       "      <td>Dragon</td>\n",
       "      <td>40.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39999</th>\n",
       "      <td>312</td>\n",
       "      <td>486</td>\n",
       "      <td>486</td>\n",
       "      <td>Normal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>60.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40000 rows Ã— 97 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       First_pokemon  Second_pokemon  Winner First_Type1 First_Type2  \\\n",
       "0                173             463     463       Water         NaN   \n",
       "1                174             307     307       Water         NaN   \n",
       "2                772             181     772    Fighting      Flying   \n",
       "3                430             356     430     Psychic         NaN   \n",
       "4                519             695     519        Fire         NaN   \n",
       "...              ...             ...     ...         ...         ...   \n",
       "39995            416             778     778         Ice         NaN   \n",
       "39996            221             265     265         Bug       Steel   \n",
       "39997             35             448     448      Poison         NaN   \n",
       "39998            791             713     713      Flying      Dragon   \n",
       "39999            312             486     486      Normal         NaN   \n",
       "\n",
       "       First_HP  First_Atk  First_Def  First_SpAtk  First_SpDef  ...  \\\n",
       "0          50.0       65.0       64.0         44.0         48.0  ...   \n",
       "1          65.0       80.0       80.0         59.0         63.0  ...   \n",
       "2          78.0       92.0       75.0         74.0         63.0  ...   \n",
       "3          50.0      180.0       20.0        180.0         20.0  ...   \n",
       "4          75.0       95.0       67.0        125.0         95.0  ...   \n",
       "...         ...        ...        ...          ...          ...  ...   \n",
       "39995      80.0       50.0      100.0        100.0        200.0  ...   \n",
       "39996      75.0       90.0      140.0         60.0         60.0  ...   \n",
       "39997      55.0       47.0       52.0         40.0         40.0  ...   \n",
       "39998      40.0       30.0       35.0         45.0         40.0  ...   \n",
       "39999      60.0       60.0       60.0         35.0         35.0  ...   \n",
       "\n",
       "       Second_Type2_Ghost  Second_Type2_Grass Second_Type2_Ground  \\\n",
       "0                       0                   0                   0   \n",
       "1                       0                   0                   0   \n",
       "2                       0                   0                   0   \n",
       "3                       0                   0                   0   \n",
       "4                       0                   0                   0   \n",
       "...                   ...                 ...                 ...   \n",
       "39995                   0                   0                   0   \n",
       "39996                   0                   0                   0   \n",
       "39997                   0                   0                   0   \n",
       "39998                   0                   0                   0   \n",
       "39999                   0                   0                   0   \n",
       "\n",
       "      Second_Type2_Ice Second_Type2_Normal Second_Type2_Poison  \\\n",
       "0                    0                   0                   0   \n",
       "1                    0                   0                   0   \n",
       "2                    0                   0                   0   \n",
       "3                    0                   0                   0   \n",
       "4                    0                   0                   0   \n",
       "...                ...                 ...                 ...   \n",
       "39995                0                   0                   0   \n",
       "39996                0                   0                   0   \n",
       "39997                0                   0                   0   \n",
       "39998                1                   0                   0   \n",
       "39999                0                   0                   0   \n",
       "\n",
       "       Second_Type2_Psychic  Second_Type2_Rock  Second_Type2_Steel  \\\n",
       "0                         0                  0                   0   \n",
       "1                         0                  0                   0   \n",
       "2                         0                  0                   0   \n",
       "3                         0                  0                   0   \n",
       "4                         0                  0                   0   \n",
       "...                     ...                ...                 ...   \n",
       "39995                     0                  0                   0   \n",
       "39996                     0                  0                   0   \n",
       "39997                     0                  0                   0   \n",
       "39998                     0                  0                   0   \n",
       "39999                     1                  0                   0   \n",
       "\n",
       "       Second_Type2_Water  \n",
       "0                       0  \n",
       "1                       0  \n",
       "2                       0  \n",
       "3                       0  \n",
       "4                       0  \n",
       "...                   ...  \n",
       "39995                   0  \n",
       "39996                   0  \n",
       "39997                   0  \n",
       "39998                   0  \n",
       "39999                   0  \n",
       "\n",
       "[40000 rows x 97 columns]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_type = pd.get_dummies(battles[['First_Type1','First_Type2','Second_Type1','Second_Type2']])\n",
    "battles = pd.concat([battles, one_hot_type], axis=1)\n",
    "battles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set bool label of whether the first pokemon wins\n",
    "index = 0\n",
    "for winner in battles['Winner']:\n",
    "    if winner == battles.loc[index, 'First_pokemon']:\n",
    "        battles.loc[index,'First_win'] = 1\n",
    "    else:\n",
    "        battles.loc[index,'First_win'] = 0\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "battles['First_hasGender']= battles['First_hasGender'].astype(bool)\n",
    "battles['Second_hasGender']= battles['Second_hasGender'].astype(bool)\n",
    "battles['First_legendary']= battles['First_hasGender'].astype(bool)\n",
    "battles['Second_legendary']= battles['Second_hasGender'].astype(bool)\n",
    "battles['First_win']= battles['First_win'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# report best 3 hyperparameters\n",
    "def report(results, n_top=3):\n",
    "    for i in range(1, n_top + 1):\n",
    "        candidates = np.flatnonzero(results['rank_test_score'] == i)\n",
    "        for candidate in candidates:\n",
    "            print(\"Model with rank: {0}\".format(i))\n",
    "            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\"\n",
    "                  .format(results['mean_test_score'][candidate],\n",
    "                          results['std_test_score'][candidate]))\n",
    "            print(\"Parameters: {0}\".format(results['params'][candidate]))\n",
    "            print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "[CV] activation=relu, hidden_layer_sizes=(20,), learning_rate=constant, learning_rate_init=0.0014345289151653472, random_state=4211, solver=sgd \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  activation=relu, hidden_layer_sizes=(20,), learning_rate=constant, learning_rate_init=0.0014345289151653472, random_state=4211, solver=sgd, score=0.941, total=   9.2s\n",
      "[CV] activation=relu, hidden_layer_sizes=(20,), learning_rate=constant, learning_rate_init=0.0014345289151653472, random_state=4211, solver=sgd \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    9.1s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  activation=relu, hidden_layer_sizes=(20,), learning_rate=constant, learning_rate_init=0.0014345289151653472, random_state=4211, solver=sgd, score=0.946, total=   7.8s\n",
      "[CV] activation=relu, hidden_layer_sizes=(20,), learning_rate=constant, learning_rate_init=0.0014345289151653472, random_state=4211, solver=sgd \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:   16.9s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  activation=relu, hidden_layer_sizes=(20,), learning_rate=constant, learning_rate_init=0.0014345289151653472, random_state=4211, solver=sgd, score=0.937, total=   6.8s\n",
      "[CV] activation=relu, hidden_layer_sizes=(20,), learning_rate=constant, learning_rate_init=0.0014345289151653472, random_state=4211, solver=sgd \n",
      "[CV]  activation=relu, hidden_layer_sizes=(20,), learning_rate=constant, learning_rate_init=0.0014345289151653472, random_state=4211, solver=sgd, score=0.934, total=   7.8s\n",
      "[CV] activation=relu, hidden_layer_sizes=(20,), learning_rate=constant, learning_rate_init=0.0014345289151653472, random_state=4211, solver=sgd \n",
      "[CV]  activation=relu, hidden_layer_sizes=(20,), learning_rate=constant, learning_rate_init=0.0014345289151653472, random_state=4211, solver=sgd, score=0.932, total=   9.2s\n",
      "[CV] activation=relu, hidden_layer_sizes=(10,), learning_rate=constant, learning_rate_init=0.0037248347469766643, random_state=4211, solver=sgd \n",
      "[CV]  activation=relu, hidden_layer_sizes=(10,), learning_rate=constant, learning_rate_init=0.0037248347469766643, random_state=4211, solver=sgd, score=0.528, total=   1.1s\n",
      "[CV] activation=relu, hidden_layer_sizes=(10,), learning_rate=constant, learning_rate_init=0.0037248347469766643, random_state=4211, solver=sgd \n",
      "[CV]  activation=relu, hidden_layer_sizes=(10,), learning_rate=constant, learning_rate_init=0.0037248347469766643, random_state=4211, solver=sgd, score=0.528, total=   1.1s\n",
      "[CV] activation=relu, hidden_layer_sizes=(10,), learning_rate=constant, learning_rate_init=0.0037248347469766643, random_state=4211, solver=sgd \n",
      "[CV]  activation=relu, hidden_layer_sizes=(10,), learning_rate=constant, learning_rate_init=0.0037248347469766643, random_state=4211, solver=sgd, score=0.528, total=   1.1s\n",
      "[CV] activation=relu, hidden_layer_sizes=(10,), learning_rate=constant, learning_rate_init=0.0037248347469766643, random_state=4211, solver=sgd \n",
      "[CV]  activation=relu, hidden_layer_sizes=(10,), learning_rate=constant, learning_rate_init=0.0037248347469766643, random_state=4211, solver=sgd, score=0.528, total=   1.1s\n",
      "[CV] activation=relu, hidden_layer_sizes=(10,), learning_rate=constant, learning_rate_init=0.0037248347469766643, random_state=4211, solver=sgd \n",
      "[CV]  activation=relu, hidden_layer_sizes=(10,), learning_rate=constant, learning_rate_init=0.0037248347469766643, random_state=4211, solver=sgd, score=0.528, total=   1.2s\n",
      "[CV] activation=relu, hidden_layer_sizes=(15,), learning_rate=invscaling, learning_rate_init=0.8852296611737694, random_state=4211, solver=sgd \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  activation=relu, hidden_layer_sizes=(15,), learning_rate=invscaling, learning_rate_init=0.8852296611737694, random_state=4211, solver=sgd, score=0.528, total=  14.8s\n",
      "[CV] activation=relu, hidden_layer_sizes=(15,), learning_rate=invscaling, learning_rate_init=0.8852296611737694, random_state=4211, solver=sgd \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  activation=relu, hidden_layer_sizes=(15,), learning_rate=invscaling, learning_rate_init=0.8852296611737694, random_state=4211, solver=sgd, score=0.528, total=  15.3s\n",
      "[CV] activation=relu, hidden_layer_sizes=(15,), learning_rate=invscaling, learning_rate_init=0.8852296611737694, random_state=4211, solver=sgd \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  activation=relu, hidden_layer_sizes=(15,), learning_rate=invscaling, learning_rate_init=0.8852296611737694, random_state=4211, solver=sgd, score=0.528, total=  18.1s\n",
      "[CV] activation=relu, hidden_layer_sizes=(15,), learning_rate=invscaling, learning_rate_init=0.8852296611737694, random_state=4211, solver=sgd \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  activation=relu, hidden_layer_sizes=(15,), learning_rate=invscaling, learning_rate_init=0.8852296611737694, random_state=4211, solver=sgd, score=0.528, total=  18.8s\n",
      "[CV] activation=relu, hidden_layer_sizes=(15,), learning_rate=invscaling, learning_rate_init=0.8852296611737694, random_state=4211, solver=sgd \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  activation=relu, hidden_layer_sizes=(15,), learning_rate=invscaling, learning_rate_init=0.8852296611737694, random_state=4211, solver=sgd, score=0.528, total=  16.2s\n",
      "[CV] activation=relu, hidden_layer_sizes=(15,), learning_rate=adaptive, learning_rate_init=0.03763198853223679, random_state=4211, solver=adam \n",
      "[CV]  activation=relu, hidden_layer_sizes=(15,), learning_rate=adaptive, learning_rate_init=0.03763198853223679, random_state=4211, solver=adam, score=0.899, total=   2.1s\n",
      "[CV] activation=relu, hidden_layer_sizes=(15,), learning_rate=adaptive, learning_rate_init=0.03763198853223679, random_state=4211, solver=adam \n",
      "[CV]  activation=relu, hidden_layer_sizes=(15,), learning_rate=adaptive, learning_rate_init=0.03763198853223679, random_state=4211, solver=adam, score=0.888, total=   2.2s\n",
      "[CV] activation=relu, hidden_layer_sizes=(15,), learning_rate=adaptive, learning_rate_init=0.03763198853223679, random_state=4211, solver=adam \n",
      "[CV]  activation=relu, hidden_layer_sizes=(15,), learning_rate=adaptive, learning_rate_init=0.03763198853223679, random_state=4211, solver=adam, score=0.889, total=   2.5s\n",
      "[CV] activation=relu, hidden_layer_sizes=(15,), learning_rate=adaptive, learning_rate_init=0.03763198853223679, random_state=4211, solver=adam \n",
      "[CV]  activation=relu, hidden_layer_sizes=(15,), learning_rate=adaptive, learning_rate_init=0.03763198853223679, random_state=4211, solver=adam, score=0.899, total=   2.6s\n",
      "[CV] activation=relu, hidden_layer_sizes=(15,), learning_rate=adaptive, learning_rate_init=0.03763198853223679, random_state=4211, solver=adam \n",
      "[CV]  activation=relu, hidden_layer_sizes=(15,), learning_rate=adaptive, learning_rate_init=0.03763198853223679, random_state=4211, solver=adam, score=0.888, total=   2.7s\n",
      "[CV] activation=tanh, hidden_layer_sizes=(10,), learning_rate=invscaling, learning_rate_init=0.0116849482224164, random_state=4211, solver=sgd \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  activation=tanh, hidden_layer_sizes=(10,), learning_rate=invscaling, learning_rate_init=0.0116849482224164, random_state=4211, solver=sgd, score=0.868, total=  13.2s\n",
      "[CV] activation=tanh, hidden_layer_sizes=(10,), learning_rate=invscaling, learning_rate_init=0.0116849482224164, random_state=4211, solver=sgd \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  activation=tanh, hidden_layer_sizes=(10,), learning_rate=invscaling, learning_rate_init=0.0116849482224164, random_state=4211, solver=sgd, score=0.850, total=  12.6s\n",
      "[CV] activation=tanh, hidden_layer_sizes=(10,), learning_rate=invscaling, learning_rate_init=0.0116849482224164, random_state=4211, solver=sgd \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  activation=tanh, hidden_layer_sizes=(10,), learning_rate=invscaling, learning_rate_init=0.0116849482224164, random_state=4211, solver=sgd, score=0.851, total=  12.5s\n",
      "[CV] activation=tanh, hidden_layer_sizes=(10,), learning_rate=invscaling, learning_rate_init=0.0116849482224164, random_state=4211, solver=sgd \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  activation=tanh, hidden_layer_sizes=(10,), learning_rate=invscaling, learning_rate_init=0.0116849482224164, random_state=4211, solver=sgd, score=0.905, total=  12.7s\n",
      "[CV] activation=tanh, hidden_layer_sizes=(10,), learning_rate=invscaling, learning_rate_init=0.0116849482224164, random_state=4211, solver=sgd \n",
      "[CV]  activation=tanh, hidden_layer_sizes=(10,), learning_rate=invscaling, learning_rate_init=0.0116849482224164, random_state=4211, solver=sgd, score=0.573, total=   1.0s\n",
      "[CV] activation=relu, hidden_layer_sizes=(15,), learning_rate=constant, learning_rate_init=0.03759318429083023, random_state=4211, solver=sgd \n",
      "[CV]  activation=relu, hidden_layer_sizes=(15,), learning_rate=constant, learning_rate_init=0.03759318429083023, random_state=4211, solver=sgd, score=0.528, total=   1.3s\n",
      "[CV] activation=relu, hidden_layer_sizes=(15,), learning_rate=constant, learning_rate_init=0.03759318429083023, random_state=4211, solver=sgd \n",
      "[CV]  activation=relu, hidden_layer_sizes=(15,), learning_rate=constant, learning_rate_init=0.03759318429083023, random_state=4211, solver=sgd, score=0.528, total=   1.3s\n",
      "[CV] activation=relu, hidden_layer_sizes=(15,), learning_rate=constant, learning_rate_init=0.03759318429083023, random_state=4211, solver=sgd \n",
      "[CV]  activation=relu, hidden_layer_sizes=(15,), learning_rate=constant, learning_rate_init=0.03759318429083023, random_state=4211, solver=sgd, score=0.528, total=   1.3s\n",
      "[CV] activation=relu, hidden_layer_sizes=(15,), learning_rate=constant, learning_rate_init=0.03759318429083023, random_state=4211, solver=sgd \n",
      "[CV]  activation=relu, hidden_layer_sizes=(15,), learning_rate=constant, learning_rate_init=0.03759318429083023, random_state=4211, solver=sgd, score=0.528, total=   1.9s\n",
      "[CV] activation=relu, hidden_layer_sizes=(15,), learning_rate=constant, learning_rate_init=0.03759318429083023, random_state=4211, solver=sgd \n",
      "[CV]  activation=relu, hidden_layer_sizes=(15,), learning_rate=constant, learning_rate_init=0.03759318429083023, random_state=4211, solver=sgd, score=0.528, total=   1.4s\n",
      "[CV] activation=relu, hidden_layer_sizes=(20,), learning_rate=adaptive, learning_rate_init=0.03788663583678653, random_state=4211, solver=adam \n",
      "[CV]  activation=relu, hidden_layer_sizes=(20,), learning_rate=adaptive, learning_rate_init=0.03788663583678653, random_state=4211, solver=adam, score=0.930, total=   6.6s\n",
      "[CV] activation=relu, hidden_layer_sizes=(20,), learning_rate=adaptive, learning_rate_init=0.03788663583678653, random_state=4211, solver=adam \n",
      "[CV]  activation=relu, hidden_layer_sizes=(20,), learning_rate=adaptive, learning_rate_init=0.03788663583678653, random_state=4211, solver=adam, score=0.884, total=   2.4s\n",
      "[CV] activation=relu, hidden_layer_sizes=(20,), learning_rate=adaptive, learning_rate_init=0.03788663583678653, random_state=4211, solver=adam \n",
      "[CV]  activation=relu, hidden_layer_sizes=(20,), learning_rate=adaptive, learning_rate_init=0.03788663583678653, random_state=4211, solver=adam, score=0.881, total=   2.3s\n",
      "[CV] activation=relu, hidden_layer_sizes=(20,), learning_rate=adaptive, learning_rate_init=0.03788663583678653, random_state=4211, solver=adam \n",
      "[CV]  activation=relu, hidden_layer_sizes=(20,), learning_rate=adaptive, learning_rate_init=0.03788663583678653, random_state=4211, solver=adam, score=0.829, total=   2.6s\n",
      "[CV] activation=relu, hidden_layer_sizes=(20,), learning_rate=adaptive, learning_rate_init=0.03788663583678653, random_state=4211, solver=adam \n",
      "[CV]  activation=relu, hidden_layer_sizes=(20,), learning_rate=adaptive, learning_rate_init=0.03788663583678653, random_state=4211, solver=adam, score=0.897, total=   4.1s\n",
      "[CV] activation=relu, hidden_layer_sizes=(15,), learning_rate=adaptive, learning_rate_init=0.017009281984112383, random_state=4211, solver=adam \n",
      "[CV]  activation=relu, hidden_layer_sizes=(15,), learning_rate=adaptive, learning_rate_init=0.017009281984112383, random_state=4211, solver=adam, score=0.866, total=   2.6s\n",
      "[CV] activation=relu, hidden_layer_sizes=(15,), learning_rate=adaptive, learning_rate_init=0.017009281984112383, random_state=4211, solver=adam \n",
      "[CV]  activation=relu, hidden_layer_sizes=(15,), learning_rate=adaptive, learning_rate_init=0.017009281984112383, random_state=4211, solver=adam, score=0.896, total=   3.4s\n",
      "[CV] activation=relu, hidden_layer_sizes=(15,), learning_rate=adaptive, learning_rate_init=0.017009281984112383, random_state=4211, solver=adam \n",
      "[CV]  activation=relu, hidden_layer_sizes=(15,), learning_rate=adaptive, learning_rate_init=0.017009281984112383, random_state=4211, solver=adam, score=0.908, total=   3.0s\n",
      "[CV] activation=relu, hidden_layer_sizes=(15,), learning_rate=adaptive, learning_rate_init=0.017009281984112383, random_state=4211, solver=adam \n",
      "[CV]  activation=relu, hidden_layer_sizes=(15,), learning_rate=adaptive, learning_rate_init=0.017009281984112383, random_state=4211, solver=adam, score=0.911, total=   5.2s\n",
      "[CV] activation=relu, hidden_layer_sizes=(15,), learning_rate=adaptive, learning_rate_init=0.017009281984112383, random_state=4211, solver=adam \n",
      "[CV]  activation=relu, hidden_layer_sizes=(15,), learning_rate=adaptive, learning_rate_init=0.017009281984112383, random_state=4211, solver=adam, score=0.874, total=   2.6s\n",
      "[CV] activation=tanh, hidden_layer_sizes=(20,), learning_rate=invscaling, learning_rate_init=0.0058235839931391415, random_state=4211, solver=adam \n",
      "[CV]  activation=tanh, hidden_layer_sizes=(20,), learning_rate=invscaling, learning_rate_init=0.0058235839931391415, random_state=4211, solver=adam, score=0.938, total=   5.3s\n",
      "[CV] activation=tanh, hidden_layer_sizes=(20,), learning_rate=invscaling, learning_rate_init=0.0058235839931391415, random_state=4211, solver=adam \n",
      "[CV]  activation=tanh, hidden_layer_sizes=(20,), learning_rate=invscaling, learning_rate_init=0.0058235839931391415, random_state=4211, solver=adam, score=0.941, total=   3.3s\n",
      "[CV] activation=tanh, hidden_layer_sizes=(20,), learning_rate=invscaling, learning_rate_init=0.0058235839931391415, random_state=4211, solver=adam \n",
      "[CV]  activation=tanh, hidden_layer_sizes=(20,), learning_rate=invscaling, learning_rate_init=0.0058235839931391415, random_state=4211, solver=adam, score=0.923, total=   4.6s\n",
      "[CV] activation=tanh, hidden_layer_sizes=(20,), learning_rate=invscaling, learning_rate_init=0.0058235839931391415, random_state=4211, solver=adam \n",
      "[CV]  activation=tanh, hidden_layer_sizes=(20,), learning_rate=invscaling, learning_rate_init=0.0058235839931391415, random_state=4211, solver=adam, score=0.941, total=   2.8s\n",
      "[CV] activation=tanh, hidden_layer_sizes=(20,), learning_rate=invscaling, learning_rate_init=0.0058235839931391415, random_state=4211, solver=adam \n",
      "[CV]  activation=tanh, hidden_layer_sizes=(20,), learning_rate=invscaling, learning_rate_init=0.0058235839931391415, random_state=4211, solver=adam, score=0.921, total=   6.6s\n",
      "[CV] activation=relu, hidden_layer_sizes=(15,), learning_rate=invscaling, learning_rate_init=0.006456100414107124, random_state=4211, solver=sgd \n",
      "[CV]  activation=relu, hidden_layer_sizes=(15,), learning_rate=invscaling, learning_rate_init=0.006456100414107124, random_state=4211, solver=sgd, score=0.528, total=   1.2s\n",
      "[CV] activation=relu, hidden_layer_sizes=(15,), learning_rate=invscaling, learning_rate_init=0.006456100414107124, random_state=4211, solver=sgd \n",
      "[CV]  activation=relu, hidden_layer_sizes=(15,), learning_rate=invscaling, learning_rate_init=0.006456100414107124, random_state=4211, solver=sgd, score=0.528, total=   1.3s\n",
      "[CV] activation=relu, hidden_layer_sizes=(15,), learning_rate=invscaling, learning_rate_init=0.006456100414107124, random_state=4211, solver=sgd \n",
      "[CV]  activation=relu, hidden_layer_sizes=(15,), learning_rate=invscaling, learning_rate_init=0.006456100414107124, random_state=4211, solver=sgd, score=0.528, total=   1.5s\n",
      "[CV] activation=relu, hidden_layer_sizes=(15,), learning_rate=invscaling, learning_rate_init=0.006456100414107124, random_state=4211, solver=sgd \n",
      "[CV]  activation=relu, hidden_layer_sizes=(15,), learning_rate=invscaling, learning_rate_init=0.006456100414107124, random_state=4211, solver=sgd, score=0.528, total=   1.3s\n",
      "[CV] activation=relu, hidden_layer_sizes=(15,), learning_rate=invscaling, learning_rate_init=0.006456100414107124, random_state=4211, solver=sgd \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  activation=relu, hidden_layer_sizes=(15,), learning_rate=invscaling, learning_rate_init=0.006456100414107124, random_state=4211, solver=sgd, score=0.528, total=   1.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  50 out of  50 | elapsed:  4.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with rank: 1\n",
      "Mean validation score: 0.938 (std: 0.005)\n",
      "Parameters: {'activation': 'relu', 'hidden_layer_sizes': (20,), 'learning_rate': 'constant', 'learning_rate_init': 0.0014345289151653472, 'random_state': 4211, 'solver': 'sgd'}\n",
      "\n",
      "Model with rank: 2\n",
      "Mean validation score: 0.933 (std: 0.009)\n",
      "Parameters: {'activation': 'tanh', 'hidden_layer_sizes': (20,), 'learning_rate': 'invscaling', 'learning_rate_init': 0.0058235839931391415, 'random_state': 4211, 'solver': 'adam'}\n",
      "\n",
      "Model with rank: 3\n",
      "Mean validation score: 0.893 (std: 0.005)\n",
      "Parameters: {'activation': 'relu', 'hidden_layer_sizes': (15,), 'learning_rate': 'adaptive', 'learning_rate_init': 0.03763198853223679, 'random_state': 4211, 'solver': 'adam'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.fixes import loguniform\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "random_dist = {'hidden_layer_sizes': [(20, ), (10,), (15,)],\n",
    "               'solver': ['sgd', 'adam'],\n",
    "               'learning_rate': ['constant', 'invscaling', 'adaptive'],\n",
    "               'learning_rate_init': loguniform(1e-3, 1e0),\n",
    "               'activation': ['tanh', 'relu'],\n",
    "               'random_state':[4211]}\n",
    "\n",
    "mlp = MLPClassifier()\n",
    "X_all = pd.concat([battles.iloc[:,0:1],battles.iloc[:,5:14]],axis=1)\n",
    "X_all = pd.concat([X_all,battles.iloc[:,17:96]],axis=1)\n",
    "y_all = battles['First_win']\n",
    "\n",
    "mlp = MLPClassifier()\n",
    "random_search = RandomizedSearchCV(\n",
    "    mlp, random_dist, n_iter=10, cv=5, verbose=3, random_state=4211)\n",
    "\n",
    "random_search.fit(X_all, y_all)\n",
    "\n",
    "report(random_search.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "#organize data from pokemon to q4_test\n",
    "index = 0\n",
    "for first in q4_test['First_pokemon']:\n",
    "    q4_test.loc[index,'First_Type1'] = pokemon.loc[first-1, 'Type 1']\n",
    "    q4_test.loc[index,'First_Type2'] = pokemon.loc[first-1, 'Type 2']\n",
    "    q4_test.loc[index,'First_HP'] = pokemon.loc[first-1, 'HP']\n",
    "    q4_test.loc[index,'First_Atk'] = pokemon.loc[first-1, 'Attack']\n",
    "    q4_test.loc[index,'First_Def'] = pokemon.loc[first-1, 'Defense']\n",
    "    q4_test.loc[index,'First_SpAtk'] = pokemon.loc[first-1, 'Sp. Atk']\n",
    "    q4_test.loc[index,'First_SpDef'] = pokemon.loc[first-1, 'Sp. Def']\n",
    "    q4_test.loc[index,'First_Speed'] = pokemon.loc[first-1, 'Speed']\n",
    "    q4_test.loc[index,'First_generation'] = pokemon.loc[first-1, 'Generation']\n",
    "    q4_test.loc[index,'First_hasGender'] = pokemon.loc[first-1, 'Has Gender']\n",
    "    q4_test.loc[index,'First_legendary'] = pokemon.loc[first-1, 'Legendary']\n",
    "    index += 1\n",
    "index = 0\n",
    "for second in q4_test['Second_pokemon']:\n",
    "    q4_test.loc[index,'Second_Type1'] = pokemon.loc[second-1, 'Type 1']\n",
    "    q4_test.loc[index,'Second_Type2'] = pokemon.loc[second-1, 'Type 2']\n",
    "    q4_test.loc[index,'Second_HP'] = pokemon.loc[second-1, 'HP']\n",
    "    q4_test.loc[index,'Second_Atk'] = pokemon.loc[second-1, 'Attack']\n",
    "    q4_test.loc[index,'Second_Def'] = pokemon.loc[second-1, 'Defense']\n",
    "    q4_test.loc[index,'Second_SpAtk'] = pokemon.loc[second-1, 'Sp. Atk']\n",
    "    q4_test.loc[index,'Second_SpDef'] = pokemon.loc[second-1, 'Sp. Def']\n",
    "    q4_test.loc[index,'Second_Speed'] = pokemon.loc[second-1, 'Speed']\n",
    "    q4_test.loc[index,'Second_generation'] = pokemon.loc[second-1, 'Generation']\n",
    "    q4_test.loc[index,'Second_hasGender'] = pokemon.loc[second-1, 'Has Gender']\n",
    "    q4_test.loc[index,'Second_legendary'] = pokemon.loc[second-1, 'Legendary']\n",
    "    index += 1\n",
    "one_hot_type = pd.get_dummies(q4_test[['First_Type1','First_Type2','Second_Type1','Second_Type2']])\n",
    "q4_test = pd.concat([q4_test, one_hot_type], axis=1)\n",
    "index = 0\n",
    "for winner in q4_test['Winner']:\n",
    "    if winner == q4_test.loc[index, 'First_pokemon']:\n",
    "        q4_test.loc[index,'First_win'] = 1\n",
    "    else:\n",
    "        q4_test.loc[index,'First_win'] = 0\n",
    "    index += 1\n",
    "q4_test['First_hasGender']= q4_test['First_hasGender'].astype(bool)\n",
    "q4_test['Second_hasGender']= q4_test['Second_hasGender'].astype(bool)\n",
    "q4_test['First_legendary']= q4_test['First_hasGender'].astype(bool)\n",
    "q4_test['Second_legendary']= q4_test['Second_hasGender'].astype(bool)\n",
    "q4_test['First_win']= q4_test['First_win'].astype(int)\n",
    "X_test = pd.concat([q4_test.iloc[:,0:1],q4_test.iloc[:,5:14]],axis=1)\n",
    "X_test = pd.concat([X_test,q4_test.iloc[:,17:96]],axis=1)\n",
    "y_test = q4_test['First_win']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy: 0.9318\n",
      "Classification report for classifier MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "              hidden_layer_sizes=(20,), learning_rate='constant',\n",
      "              learning_rate_init=0.0014345289151653472, max_fun=15000,\n",
      "              max_iter=200, momentum=0.9, n_iter_no_change=10,\n",
      "              nesterovs_momentum=True, power_t=0.5, random_state=4211,\n",
      "              shuffle=True, solver='sgd', tol=0.0001, validation_fraction=0.1,\n",
      "              verbose=False, warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.95      0.94      5282\n",
      "           1       0.94      0.92      0.93      4718\n",
      "\n",
      "    accuracy                           0.93     10000\n",
      "   macro avg       0.93      0.93      0.93     10000\n",
      "weighted avg       0.93      0.93      0.93     10000\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[5000  282]\n",
      " [ 400 4318]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAT4AAAEjCAYAAACrcG11AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3deZzVVf3H8ddnZtgGhnXYFwEBfy4lLiFq7gZollZiqCn68xdlapZZmVmUaWmbS+6J4ZZI5a5FuGu5Aa64MIgsI8g27APMzJ3P74/vGbzAzJ17YS4z937fTx7fB/d7vueec+4snznLdzF3R0QkTgqauwEiIruaAp+IxI4Cn4jEjgKfiMSOAp+IxI4Cn4jEjgJfHjOzdmb2qJmtMbO/7UQ5p5vZv5uybc3BzP5pZuObux3S/BT4WgAzO83MZpjZejNbEn5BP98ERZ8M9AS6ufvYHS3E3e9191FN0J6tmNmRZuZm9sA26fuG9GfTLOcXZnZPY/nc/Th3v3MHmyt5RIGvmZnZRcC1wK+JgtQA4CbgxCYofjdgjrvXNEFZ2bIcOMTMuiWljQfmNFUFFtHPunzK3bU10wZ0AtYDY1PkaUMUGBeH7VqgTTh2JFAO/ABYBiwBzg7HfglUAdWhjnOAXwD3JJU9EHCgKOyfBcwD1gEfAacnpb+Y9L5DgNeANeH/Q5KOPQv8CvhPKOffQGkDn62u/bcA54W0wpD2c+DZpLzXAYuAtcBM4LCQPmabz/lmUjuuDO3YCAwJaf8Xjt8M/D2p/KuBpwBr7p8Lbdnf9FeweR0MtAUeTJHnp8BIYDiwLzACuCzpeC+iANqXKLjdaGZd3H0iUS/yfnfv4O6TUjXEzNoD1wPHuXsJUXB7o558XYHHQ95uwB+Bx7fpsZ0GnA30AFoDF6eqG7gLODO8Hg3MJgryyV4j+hp0Bf4K/M3M2rr7v7b5nPsmvecMYAJQAizYprwfAJ81s7PM7DCir914d9c1nDGgwNe8ugErPPVQ9HTgcndf5u7LiXpyZyQdrw7Hq939CaJezx472J5aYB8za+fuS9x9dj15vgiUufvd7l7j7vcB7wNfSsrzF3ef4+4bgalEAatB7v5foKuZ7UEUAO+qJ8897r4y1PkHop5wY59zsrvPDu+p3qa8SuAbRIH7HuACdy9vpDzJEwp8zWslUGpmRSny9GHr3sqCkLaljG0CZyXQIdOGuPsG4OvAt4ElZva4mf1PGu2pa1PfpP1PdqA9dwPnA0dRTw/YzH5gZu+FFerVRL3c0kbKXJTqoLu/SjS0N6IALTGhwNe8XgI2ASelyLOYaJGizgC2HwamawNQnLTfK/mgu09z9y8AvYl6cX9Ooz11bfp4B9tU527gO8AToTe2RRiK/hg4Beji7p2J5hetrukNlJly2Gpm5xH1HBcDP9rxpkuuUeBrRu6+hmgS/0YzO8nMis2slZkdZ2a/DdnuAy4zs+5mVhryN3rqRgPeAA43swFm1gn4Sd0BM+tpZl8Oc32biYbMiXrKeAIYFk7BKTKzrwN7AY/tYJsAcPePgCOI5jS3VQLUEK0AF5nZz4GOSceXAgMzWbk1s2HAFUTD3TOAH5lZyiG55A8Fvmbm7n8ELiJasFhONDw7H3goZLkCmAG8BbwNzAppO1LXdOD+UNZMtg5WBUQT/ouBCqIg9J16ylgJnBDyriTqKZ3g7it2pE3blP2iu9fXm50G/JPoFJcFRL3k5GFs3cnZK81sVmP1hKmFe4Cr3f1Ndy8DLgXuNrM2O/MZJDeYFrFEJG7U4xOR2FHgE5HYUeATkdhR4BOR2FHgE5HYUeATkdhR4BOR2FHgE5HYUeATkdhR4BOR2FHgE5HYUeATkdhR4BOR2FHgE5HYUeATkWZjZvPN7G0ze8PMZoS0rmY23czKwv9dQrqZ2fVmNtfM3jKz/ZPKGR/yl6Xz0HgFPhFpbke5+3B3PzDsXwI85e5DiR75eUlIPw4YGrYJRI8IrXvy30TgIKKnEE6sC5YNUeATkZbmRODO8PpOPn0mzYnAXR55GehsZr2JHkk63d0r3H0VMJ3oecsNSvV0r12utGuhD+zfqrmbIRmY81Zx45mkxdjEBqp8szWes2Gjj2rvKyvqexzL9ma+tXk20aMC6tzm7rcl7TvwbzNz4NZwrKe7LwFw9yVm1iPk7cvWjxwoD2kNpTeoRQW+gf1b8eq0/s3dDMnA6D56Pk8uecWf2ukyVlYkeHXagLTyFvYu25Q0hK3Poe6+OAS36Wb2foq89QVsT5HeIA11RSQjDtSm+a/RssLDpdx9GdHzlEcAS8MQlvD/spC9HEjuGfUjejhWQ+kNUuATkYw4TrUn0tpSMbP2ZlZS9xoYBbwDPALUrcyOBx4Orx8BzgyruyOBNWFIPA0YZWZdwqLGqJDWoBY11BWR3JBOby4NPYEHzQyiWPRXd/+Xmb0GTDWzc4CFwNiQ/wngeGAuUAmcDeDuFWb2K+C1kO9yd69IVbECn4hkxHESTfBYWnefB+xbT/pK4Jh60h04r4Gy7gDuSLduBT4RyVht6rWDFk+BT0Qy4kBCgU9E4kY9PhGJFQeqm2COrzkp8IlIRhzXUFdEYsYhkdtxT4FPRDITXbmR2xT4RCRDRqLey2NzhwKfiGQkWtxQ4BORGInO41PgE5GYqVWPT0TiRD0+EYkdx0jk+B3tFPhEJGMa6opIrDhGlRc2dzN2igKfiGQkOoFZQ10RiRktbohIrLgbCVePT0RiplY9PhGJk2hxI7dDR263XkR2OS1uiEgsJXQen4jEia7cEJFYqtWqrojESXSTAgU+EYkRx6jWJWsiEifu6ARmEYkb0wnMIhIvjnp8IhJDWtwQkVhxTDciFZF4iR4vmduhI7dbLyLNQA8UF5GYcXTlhojEkHp8IhIr7qYen4jES7S4kduXrOV22BaRZhA9cyOdLa3SzArN7HUzeyzsDzKzV8yszMzuN7PWIb1N2J8bjg9MKuMnIf0DMxvdWJ0KfCKSkWhxw9La0nQh8F7S/tXANe4+FFgFnBPSzwFWufsQ4JqQDzPbCxgH7A2MAW4ys5RdUgU+EclYgoK0tsaYWT/gi8DtYd+Ao4G/hyx3AieF1yeGfcLxY0L+E4Ep7r7Z3T8C5gIjUtWrOT4RyUiGV26UmtmMpP3b3P22pP1rgR8BJWG/G7Da3WvCfjnQN7zuCywCcPcaM1sT8vcFXk4qM/k99VLgE5GMZfCwoRXufmB9B8zsBGCZu880syPrkuvJ6o0cS/WeeinwiUhG3KG6tklmyQ4FvmxmxwNtgY5EPcDOZlYUen39gMUhfznQHyg3syKgE1CRlF4n+T310hyfiGQkGuoWpLWlLMf9J+7ez90HEi1OPO3upwPPACeHbOOBh8PrR8I+4fjT7u4hfVxY9R0EDAVeTVW3enxpOnPEXrTrkKCgAAqLnBv+NWenyps+tQt/va4XAKdd+AlfOGUVAJeeNpiKZa1I1MA+B23g/F+XU5jbp0xlXfc+VfzwuoV06VGD18IT93TjoUndt8pTXJLgxzcspEefKgqLnL/f0oN/3991p+ot6VzDpbcsoGe/KpaWt+bKb+3G+jVFHPWVVZxy3jIANlUW8KdL+jHv3XY7VVdLk+UrN34MTDGzK4DXgUkhfRJwt5nNJerpjQNw99lmNhV4F6gBznP3RKoKshr4zGwMcB1QCNzu7ldls75s++3f5tKpW8qv53Z++LUh/ODahfTqX7Ulbe2qQu75Yy/+9M85mMH5Y4YxctRaSjon+Omt82lfUos7/OqbA3nh0c4cedLqpv4oeSVRY9x2eR/mvl1Mu/YJbvjXHGY9X8LCsrZb8nz5rBUsnNOGieMH0alrDZNeeJ+nH+hMTXXjg57PHryeL5xSwR++P2Cr9FPOX8brL3Zg6g09OeX8pXz9/GVMurIPSxe15odf2531a4o48Ki1XPjbci48YWiTf+7mUnc6S5OW6f4s8Gx4PY96VmXdfRMwtoH3XwlcmW59WRvqhvNobgSOA/YCTg3n2+SNxfNbc+lpgzlv9DAuOmkIC8vapPW+mc+WsP/h6+jYJUFJ5wT7H76OGc9Ei1rtS2oBSNRATZXVP20rW6lY1oq5bxcDsHFDIYvmtqW0d/VWedyhXftawGnbPsG61YUkaqIv7snnLuP6J+Zw85MfcMbFn6Rd78Gj1/Lk1KjX+OTUrhw8Zi0A785oz/o1UZ/i/VnFlPauarCM3NQ0Q93mlM2WjQDmuvs8d68CphCdb5ObzLn01N05b/QwnrinGwDX/ag/511Rzo3T5jDh54u54dJ+aRW14pNWdO/z6S9mae9qVnzSasv+pacO5uuf3Yd2HWo57AT19jLRs18Vu++zkfdnFW+V/shfShkwdBN/ff1dbn16Djf/vC/uxv5HrKPvoM189/ihfOcLwxj6mUr2OWh9WnV1Ka2mYln0fatY1orO3Wq2yzPm1Apee6bjzn+wFqY2PHejsa2lyuZQd8s5N0E5cFAW68uqax4uo1uvGlavKOKScbvTf8gm3p3RnismDNqSp7oq+kZPm9KVh26P5pgWz2/Nz74xmKJWTq8Bm5l4x/x6F9ot6Wfk1/fNo2qTcdX5u/HGix044Ij0fhHjrm1xgp/dPp9bft6HyvVbT4wecOQ6Ppzdjh+N3Z0+A6v4zZR5vPNKew44Yh37H7GOm6ZHc7btimvpO3gz77zSgeseK6NVm1raFddS0jnBTdM/AGDSFb2Z+VzjwWzfQ9Yz+tQKLjppSNN/2GYUrerm9sRzNgNfWufWmNkEYALAgL4td62lW6/or3nn0hoOHbOGN//bgQ4dE9z85Afb5R09roLR4yqA+uf4SntX89ZLHbbsr1jSis8evHVwa93WOXjUGl6a1kmBLw2FRc7Pbp/P0w904T//7Lzd8VFfr2DqDT0AY/H8NnyysDX9h2zGgPv/1HNLLz5Z3bxcQ3N8q1a0omuPqNfXtUc1q1d++vM7aM+NfO/3i7jsG4NZt6rl/lzviHy49Xw2h7ppnVvj7re5+4HufmD3bi3zr8imygIq1xdseT3zuRL22K+Snv2reP7RTkD0V/DD2W1TFbPFAUeuY+ZzJaxbXci61YXMfK6EA45cx8YNBaxcGv2SJGrg1ac60n/I5ux8qLziXPSHRSwqa8sDt3WvN8fyj1sz/LDoD0jn0mr67b6JJQtbM+O5EkaPq6BtcbRo1a1XNZ26VddbxrZe/ndHjj0l+gN37CkVvDQt6gV271vFz2+fz+++O4CP56U375trNNRt2GvA0HBezcdES8+nZbG+rFm1vIhfnhMNaRM1cNRXVvO5o9bRf/fNXH9JP/56XS8S1cYRJ65i9703NVpexy4JTv/eUi44fhgAp39/KR27JFi1vIhfnDWY6iojkYDhh67nhDNXZPWz5YO9R2zg2LGrmPdu2y3D0b/8pjc9+ka97MfvLuXea3ty8bULueWpDzCDSVf2YW1FEbOeK2HAkE1c++hcADZuKOC3FwxgzcrG673/hh789JYFjBlXwbKPo9NZIPp+lnRJcP5vyoFo1fmC44Zl4ZM3j2ys6u5qFp3/l6XCozOyryU6neWOsOTcoAP3beuvTuufKou0MKP7DG/uJkgGXvGnWOsVOxW1uu7Z3b9wx9fSyjv1kFtnNnTJWnPK6uSDuz8BPJHNOkRk13I3alrwqSrpyK9ZVxHZJXJ9qKvAJyIZyYc5PgU+EcmYAp+IxEo+nMenwCciGWvJ5+ilQ4FPRDLiDjVNcyPSZqPAJyIZ01BXRGJFc3wiEkuuwCcicaPFDRGJFXfN8YlI7BgJreqKSNxojk9EYkXX6opI/Hg0z5fLFPhEJGNa1RWRWHEtbohIHGmoKyKxo1VdEYkVdwU+EYkhnc4iIrGjOT4RiRXHqNWqrojETY53+BT4RCRDWtwQkVjK8S5fg4HPzDqmeqO7r2365ohILsjnHt9sorie/Anr9h0YkMV2iUgL5UBtbZ4GPnfvvysbIiI5woEc7/GltSZtZuPM7NLwup+ZHZDdZolIS+ae3paKmbU1s1fN7E0zm21mvwzpg8zsFTMrM7P7zax1SG8T9ueG4wOTyvpJSP/AzEY31v5GA5+Z3QAcBZwRkiqBWxp7n4jkMU9zS20zcLS77wsMB8aY2UjgauAadx8KrALOCfnPAVa5+xDgmpAPM9sLGAfsDYwBbjKzwlQVp9PjO8TdvwVsAnD3CqB1Gu8TkbxkuKe3peKR9WG3VdgcOBr4e0i/EzgpvD4x7BOOH2NmFtKnuPtmd/8ImAuMSFV3OoGv2swKQoMws25AbRrvE5F81TQ9Psys0MzeAJYB04EPgdXuXhOylAN9w+u+wCKAcHwN0C05vZ731CudwHcj8A+gexiDv0joYopIDDl4raW1AaVmNiNpm7BVUe4Jdx8O9CPqpe1Zf40A9d72edszT7Z9T70aPYHZ3e8ys5nAsSFprLu/09j7RCSfpb2qu8LdD2wsk7uvNrNngZFAZzMrCr26fsDikK0c6A+Um1kR0AmoSEqvk/yeeqV7pXEhUA1UZfAeEclXTTDUNbPuZtY5vG5H1Ll6D3gGODlkGw88HF4/EvYJx592dw/p48Kq7yBgKPBqqrob7fGZ2U+B04AHicL8X83sXnf/TWPvFZE81TSXrPUG7gwrsAXAVHd/zMzeBaaY2RXA68CkkH8ScLeZzSXq6Y0DcPfZZjYVeBeoAc5z90SqitO5VvcbwAHuXglgZlcCMwEFPpE4aqITmN39LWC/etLnUc+qrLtvAsY2UNaVwJXp1p1O4FuwTb4iYF66FYhI/snbG5Ga2TVEsb0SmG1m08L+KKKVXRGJq3y9VheoW7mdDTyelP5y9pojIrnA8rXH5+6TGjomIjGW5snJLVk6q7q7E00a7gW0rUt392FZbJeItFgWi7uzTAb+QnQqy3HAVGBKFtskIi1dE12y1lzSCXzF7j4NwN0/dPfLiO7WIiJxVZvm1kKlczrL5nAHhA/N7NvAx0CP7DZLRFqsPLgRaTqB7/tAB+C7RHN9nYD/zWajRKRly9tV3Tru/kp4uY5Pb0YqInGWr4HPzB4kxcdz969mpUUiIlmWqsd3wy5rRVD2dnuOGzxyV1crO+HU93X1Yi4p+2pVk5STt0Ndd39qVzZERHKEk9eXrImI1C9fe3wiIg3J9aFu2ndTNrM22WyIiOSQfL9yw8xGmNnbQFnY39fM/pT1lolIy5XvgQ+4HjgBWAng7m+iS9ZEYss8/a2lSmeOr8DdF0RXrW2R8n72IpLnYrCqu8jMRgAeHgpyATAnu80SkZasJffm0pFO4DuXaLg7AFgKPBnSRCSu8j3wufsywmPcRERo4fN36UjnDsx/pp747u4TstIiEWn58j3wEQ1t67QFvgIsyk5zRCQXWAu+yWg60hnq3p+8b2Z3A9Oz1iIRkSzbkUvWBgG7NXVDRCSH5PtQ18xW8enHLAAqgEuy2SgRacHyfXEjPGtjX6LnbADUunuOf2QR2Wk5HgVSXrIWgtyD7p4IW45/XBFpEjG4VvdVM9s/6y0RkZxgRKu66WwtVapnbhS5ew3weeCbZvYhsIHoc7u7KxiKxFGez/G9CuwPnLSL2iIiuSKPA58BuPuHu6gtIpIr8jjwdTezixo66O5/zEJ7RCQH5PNQtxDoQOj5iYhskceBb4m7X77LWiIiucFb9optOhqd4xMR2U6O9/hSncd3zC5rhYjklKZ45oaZ9TezZ8zsPTObbWYXhvSuZjbdzMrC/11CupnZ9WY218zeSj6/2MzGh/xlZja+sfY3GPjcvSLdL4KIxEzTXLlRA/zA3fcERgLnmdleRPcCeMrdhwJP8em9AY4DhoZtAnAzRIESmAgcBIwAJtYFy4ak/VxdEREg/aDXSOBz9yXuPiu8Xge8B/QFTgTuDNnu5NNziU8E7vLIy0BnM+sNjAamu3uFu68ium3emFR178htqUQkxoymP53FzAYC+wGvAD3dfQlEwdHMeoRsfdn6JsjlIa2h9AYp8IlIxjIIfKVmNiNp/zZ3v22rssw6AP8Avufua7d5lO1WWetJ8xTpDVLgE5HMpR/4Vrj7gQ0dNLNWREHvXnd/ICQvNbPeobfXG1gW0suB/klv7wcsDulHbpP+bKpGaY5PRDLXBHN84X6fk4D3trkS7BGgbmV2PPBwUvqZYXV3JLAmDImnAaPMrEtY1BgV0hqkHp+IZKbp7s5yKHAG8LaZvRHSLgWuAqaa2TnAQmBsOPYEcDwwF6gEzoboDBQz+xXwWsh3eWNnpSjwiUjmmiDwufuLNHyhxHbnEYcbIZ/XQFl3AHekW7cCn4hkLJ8vWRMRqVc+351FRGR7Lfx5GulQ4BORzCnwiUicZOPKjV1NgU9EMma1uR35FPhEJDOa4xORONJQV0TiR4FPROJGPT4RiR8FPhGJlTx/ypqIyHZ0Hp+IxJPnduRT4BORjKnHFyMFBc71D7/DiqWt+cX/7bFTZZ1y7seMHruc2lrj5l/uxqwXOlPaezMX//5DunSvxmuNf07pwcOTezVR6/NfbQKmndyd4h4Jjrh16/tQlk0ppuze9lghFBXXMuLyNXQaUrNT9a0vL+S/F3Vh85oCuu5VzcirV1HYOjt1tSh5cAJz1m49b2Z3mNkyM3snW3Xsaiee/QkLP2yX0XsmP//6dmkDhlRyxAkVfHvMZ7nsrD04//L5FBQ4iRrjz7/ejW+N2pfvf21vTjhjKQOGVDZV8/PenLva02lwdb3HBp6wkeMfXc5xDy1nz/9bz6yrOqZd7rwH2vH2n0q2S3/j9x3ZY/x6vjRtGa071jLvH8U7XVeusNr0tpYqm8/cmEwjz7bMJaW9NjPiqNVMu7/7lrQh+2zgt/e9y/UPv80Vk9+nS/eqtMoa+YVVPPdYV6qrClha3pbFC9oybN/1rFremg9ntwdg44ZCFs1tS7de9f8iy9YqPylg8XNtGTy2/j8UrTp82kWpqSyg7kFetQl4/bcdmXZyKU98uTtzpxSnVZ87LH25Nf1HbwJg0EmVlD/ZNmVd+STXA1/Whrru/nx4VmZe+NbPFjDpqgG0a58AoLColnMnzufybw1jTUUrDv/iSs66uJxrfjy40bK69azm/dc7bNlf8UlrSnttHTR79N3M7ntX8sEb7Zv2g+SpWb/uxPCL11K9oeEoM+feYj6Y3IHaauPoySsAmPf3YlqV1DL67ytIVMGTp5bS6/Ob6dAvkbK+qtUFtO7oFITfoHa9EmxcVpiyrrzhaHFjZ5nZBGACQFtrmb/kI45exeqVrZj7Tns+c9BaAPoN3sTAYZVcedf7ABQUOquWtQJg3Hc+5vPHR3NMXXtUc8NjbwPw7swO3DRxEFbPzLD7p7+wbYsTXHbTHG791W5Urm/2b1GL9/EzbWjTrZau+1Sz9JXWDeYbdnolw06vZP6j7Xjn5hIOvno1n/ynDas/aMWiadEURvU6Y938Ilq1r+Xps0sBqFpj1FYb5U9FPbqDr15F2+6puzP11ZVPtLixk8LDhW8D6FTQrUV+Ofc6YB0jj1nF545cTas2TnGHBN/4XjkLyoq56OS9t8s/5aa+TLkpepD75Odf5/wTPrPV8RWftKZ7n81b9kt7VbFyaRQ0C4tqueymMp55pJT/TuuaxU+VP5bPas3HT7dlyXNtSFQZ1euN//6wM4f8rv5gs9sXNzLjl52AqONywGVr6H3Y5u3yHffQciCa49vwcRGfuWDdlmPuULXWqK2BgiLY+Ekh7Xps30tMriuvtMjf1PTpubppmPy7AZxx6P6cdfh+XPXdIbz5UkeuvnAInbpW8z/7Rb8MhUW1DBia3kLEy0924YgTKmjVupae/TbRZ+Am5rzZAXC+d9VHLPqwHQ9O6p3FT5Rfhv9gHSc9t5QvP72MQ/6wip4HVW0X9NbN/3QYuvjZNpTsFq2y9v78ZsqmFFMbplLXflRITWXjk3Jm0POgKhZNi3qBHz1UTL9jNqWsK1/UncCcztZSNXuPL1fVVBdw5XlD+fbEBbQvSVBY6Dz0l14sLGt8cnxhWTEvPN6VW6e9RSJh3DRxILW1xt4HruPYr67go/fbbRke3/n7/rz2bOdsf5y89Nb1JXTdp4p+R29mzr3t+eSlNhQUQeuOtYy8KgqMu4+tZMPHhfzrq9GiVZsutRx2Y8pHsm4x/OK1/OeiLrx1XUe67FnN4JOjP3wN1ZU33HP+RqTmWZqkNLP7gCOBUmApMNHdJ6V6T6eCbj6y7fFZaY9kx9ffmNfcTZAMXP7VN5n/zvqdWmcu6dzP9zv8wrTyvvDoj2a6+4E7U182ZHNV99RslS0izaslD2PToaGuiGTGgRwf6irwiUjmcjvuKfCJSOY01BWR2Mn1VV0FPhHJTB7cnUWBT0QyEp3AnNuRT4FPRDLXgu+8kg4FPhHJmHp8IhIvmuMTkfjJ/Wt1FfhEJHMa6opIrOTBA8V1Pz4RyZx7elsj6nsomZl1NbPpZlYW/u8S0s3MrjezuWb2lpntn/Se8SF/mZmNb6xeBT4RyZynuTVuMts/lOwS4Cl3Hwo8FfYBjgOGhm0CcDNEgRKYCBwEjAAm1gXLhijwiUjGrLY2ra0x7v48sO2dX08E7gyv7wROSkq/yyMvA53NrDcwGpju7hXuvgqYTiNPeNQcn4hkxsn2Ccw93X0JgLsvMbMeIb0vsCgpX3lIayi9QQp8IpIRwzM5gbnUzGYk7d8WHjC2Y1Vvz1OkN0iBT0Qyl37gW7EDt55fama9Q2+vN7AspJcD/ZPy9QMWh/Qjt0l/NlUFmuMTkcw10apuAx4B6lZmxwMPJ6WfGVZ3RwJrwpB4GjDKzLqERY1RIa1B6vGJSGaacI4v+aFkZlZOtDp7FTDVzM4BFgJjQ/YngOOBuUAlcDaAu1eY2a+A10K+y9095aPyFPhEJGPprNimI8VDyY6pJ68D5zVQzh3AHenWq8AnIhnaqWFsi6DAJyKZcRT4RCSGcvxaXQU+EcmYbkQqIvGjwCciseIOidwe6yrwiUjm1OMTkdhR4BORWHFAz9wQkXhxcM3xiUicOFrcEJEY0hyfiMSOAp+IxItuUiAiceNAE92Wqrko8IlI5tTjE5F40SVrIhI3Dq7z+EQkdnTlhqwv1NoAAASkSURBVIjEjub4RCRW3LWqKyIxpB6fiMSL44lEczdipyjwiUhmdFsqEYklnc4iInHigKvHJyKx4roRqYjEUK4vbpi3oGVpM1sOLGjudmRBKbCiuRshGcnX79lu7t59Zwows38RfX3SscLdx+xMfdnQogJfvjKzGe5+YHO3Q9Kn71l+K2juBoiI7GoKfCISOwp8u8Ztzd0AyZi+Z3lMc3wiEjvq8YlI7CjwZZGZjTGzD8xsrpld0tztkcaZ2R1mtszM3mnutkj2KPBliZkVAjcCxwF7Aaea2V7N2ypJw2SgxZ13Jk1LgS97RgBz3X2eu1cBU4ATm7lN0gh3fx6oaO52SHYp8GVPX2BR0n55SBORZqbAlz1WT5qW0EVaAAW+7CkH+ift9wMWN1NbRCSJAl/2vAYMNbNBZtYaGAc80sxtEhEU+LLG3WuA84FpwHvAVHef3bytksaY2X3AS8AeZlZuZuc0d5uk6enKDRGJHfX4RCR2FPhEJHYU+EQkdhT4RCR2FPhEJHYU+HKImSXM7A0ze8fM/mZmxTtR1pFm9lh4/eVUd48xs85m9p0dqOMXZnZxuunb5JlsZidnUNdA3VFF0qXAl1s2uvtwd98HqAK+nXzQIhl/T939EXe/KkWWzkDGgU+kpVLgy10vAENCT+c9M7sJmAX0N7NRZvaSmc0KPcMOsOX+gO+b2YvAV+sKMrOzzOyG8LqnmT1oZm+G7RDgKmD30Nv8Xcj3QzN7zczeMrNfJpX103APwieBPRr7EGb2zVDOm2b2j216scea2QtmNsfMTgj5C83sd0l1f2tnv5ASPwp8OcjMioju8/d2SNoDuMvd9wM2AJcBx7r7/sAM4CIzawv8GfgScBjQq4Hirweec/d9gf2B2cAlwIeht/lDMxsFDCW69dZw4AAzO9zMDiC6NG8/osD6uTQ+zgPu/rlQ33tA8pUSA4EjgC8Ct4TPcA6wxt0/F8r/ppkNSqMekS2KmrsBkpF2ZvZGeP0CMAnoAyxw95dD+kiiG5/+x8wAWhNdgvU/wEfuXgZgZvcAE+qp42jgTAB3TwBrzKzLNnlGhe31sN+BKBCWAA+6e2WoI51rk/cxsyuIhtMdiC7xqzPV3WuBMjObFz7DKOCzSfN/nULdc9KoSwRQ4Ms1G919eHJCCG4bkpOA6e5+6jb5htN0t8Uy4Dfufus2dXxvB+qYDJzk7m+a2VnAkUnHti3LQ90XuHtygMTMBmZYr8SYhrr552XgUDMbAmBmxWY2DHgfGGRmu4d8pzbw/qeAc8N7C82sI7COqDdXZxrwv0lzh33NrAfwPPAVM2tnZiVEw+rGlABLzKwVcPo2x8aaWUFo82Dgg1D3uSE/ZjbMzNqnUY/IFurx5Rl3Xx56TveZWZuQfJm7zzGzCcDjZrYCeBHYp54iLgRuC3clSQDnuvtLZvafcLrIP8M8357AS6HHuR74hrvPMrP7gTeABUTD8cb8DHgl5H+brQPsB8BzQE/g2+6+ycxuJ5r7m2VR5cuBk9L76ohEdHcWEYkdDXVFJHYU+EQkdhT4RCR2FPhEJHYU+EQkdhT4RCR2FPhEJHYU+EQkdv4fAPO8ewsoMC8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#report accuracy and confusion matrix\n",
    "from sklearn.metrics import classification_report, plot_confusion_matrix\n",
    "\n",
    "predicted = random_search.predict(X_test)\n",
    "print('The accuracy:', accuracy_score(y_test, predicted))\n",
    "print(\"Classification report for classifier %s:\\n%s\\n\"\n",
    "      % (random_search.best_estimator_, classification_report(y_test, predicted)))\n",
    "\n",
    "disp = plot_confusion_matrix(random_search, X_test, y_test)\n",
    "disp.figure_.suptitle(\"Confusion Matrix\")\n",
    "print(\"Confusion matrix:\\n%s\" % disp.confusion_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
